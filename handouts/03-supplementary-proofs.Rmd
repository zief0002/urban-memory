---
title: "Some OLS Theory: Proofs"
date: "`r Sys.Date()`"
header-includes:
   - \usepackage{amsthm}
   - \usepackage{xcolor}
   - \usepackage{xfrac}
   - \usepackage[framemethod=tikz]{mdframed}
   - \usepackage{graphicx}
   - \usepackage{rotating}
   - \usepackage{booktabs}
   - \usepackage{caption}
   - \definecolor{umn}{RGB}{153, 0, 85}
   - \definecolor{umn2}{rgb}{0.1843137, 0.4509804, 0.5372549}
   - \definecolor{myorange}{HTML}{EA6153}
output: 
  pdf_document:
    includes:
      before_body: notes.tex
    highlight: tango
    latex_engine: xelatex
    fig_width: 6
    fig_height: 6
mainfont: "Minion Pro"
sansfont: "ITC Slimbach Std Book"
monofont: "Source Code Pro"
urlcolor: "umn2"
always_allow_html: yes
bibliography: epsy8264.bib
csl: apa-single-spaced.csl
---

\frenchspacing
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## $B_1$ is a Linear Function of the Observations

We need to show that we can express the slope estimator $B_1$ as a linear function of the observations, namely,

$$
B_1 = \sum w_iY_i \qquad \mathrm{for~some~} w_i 
$$

To do this, we will use the definition of the slope estimator, that 

$$
B_1 = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2}
$$

Now,

$$
\begin{split}
B_1 &= \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2} \\[1em]
&= \frac{\sum (x_i - \bar{x})y_i}{\sum (x_i - \bar{x})^2} - \frac{\sum (x_i - \bar{x})\bar{y}}{\sum (x_i - \bar{x})^2} \\[1em]
&= \frac{\sum (x_i - \bar{x})y_i}{\sum (x_i - \bar{x})^2} - \frac{\bar{y}\sum (x_i - \bar{x})}{\sum (x_i - \bar{x})^2} \\[1em]
\end{split}
$$

Then we make use of the fact that the sum of mean deviations of a random variable is always zero; $\sum (x_i - \bar{x}) = 0$.

$$
\begin{split}
B_1 &= \frac{\sum (x_i - \bar{x})y_i}{\sum (x_i - \bar{x})^2} - \frac{\bar{y}(0)}{\sum (x_i - \bar{x})^2} \\[1em]
&= \frac{\sum (x_i - \bar{x})y_i}{\sum (x_i - \bar{x})^2} - 0\\[1em]
&= \frac{\sum (x_i - \bar{x})y_i}{\sum (x_i - \bar{x})^2}
\end{split}
$$


This means that $B_1$ can be written as a linear function of the observations $y_i$:

$$
B_1 = \sum w_iY_i \qquad \mathrm{where~} w_i = \frac{x_i-\bar{x}}{\sum(x_i-\bar{x})^2}
$$


## $B_0$ is a Linear Function of the Observations

We can carry out a similar process for $B_0$, but it is far easier. Recall that,

$$
B_0 = \bar{y} - B_1\bar{x}
$$

Since we just showed that $B_1 = \sum w_iy_i$, we can substitute that in for $B_1$.

$$
B_0 = \bar{y} - \sum w_iy_i\bar{x}
$$

Since $\bar{x}$ and $\bar{y}$ are constants, $B_0$ has been shown to be a linear function of the observations, $y_i$.

## $B_1$ is Unbiased

To show that the slope estimator is unbiased, we need to show that $\mathbb{E}(B_1) = \beta$. To do this, we will make use of the fact that,


$$
\begin{split}
\sum(x_i-\bar{x})^2 &= \sum(x_i-\bar{x})(x_i-\bar{x})\\
&= \sum(x_i^2 -2x_i\bar{x} + \bar{x}^2) \\
&= \sum x_i^2 -2\bar{x} \sum x_i + \sum \bar{x}^2 \\
&= \sum x_i^2 -2\bar{x}(n\bar{x}) + n \bar{x}^2 \\
&= \sum x_i^2 -2n\bar{x}^2 + n \bar{x}^2 \\
&= \sum x_i^2 - n \bar{x}^2
\end{split}
$$

and the assumption of linearity in the population (Assumption \#2), namely that,

$$
\mathbb{E}(Y_i) = \beta_0 + \beta_1(x_i)
$$

Since,

$$
B_1 = \sum \frac{(x_i-\bar{x})y_i}{\sum x_i^2 - n \bar{x}^2},
$$

then the proof is as follows:

$$
\begin{split}
\mathbb{E}(B_1) &= \mathbb{E}\bigg(\sum \frac{(x_i-\bar{x})y_i}{\sum x_i^2 - n \bar{x}^2}\bigg) \\
&= \mathbb{E}\bigg(\frac{1}{\sum x_i^2 - n \bar{x}^2} \times \sum \bigg[(x_i-\bar{x})y_i\bigg] \bigg) \\
&= \frac{1}{\sum x_i^2 - n \bar{x}^2} \times \mathbb{E}\bigg(\sum \bigg[(x_i-\bar{x})y_i\bigg] \bigg) \\
&= \frac{1}{\sum x_i^2 - n \bar{x}^2} \times \sum\bigg(\mathbb{E} \bigg[(x_i-\bar{x})y_i\bigg] \bigg) \\
&= \frac{1}{\sum x_i^2 - n \bar{x}^2} \times \sum\bigg((x_i-\bar{x})\mathbb{E} \big[y_i\big] \bigg) \\
&= \frac{1}{\sum x_i^2 - n \bar{x}^2} \times \sum\bigg((x_i-\bar{x})\big[\beta_0 + \beta_1(x_i)\big] \bigg) \\
&= \frac{1}{\sum x_i^2 - n \bar{x}^2} \times \sum\bigg(x_i\beta_0-\bar{x}\beta_0 + \beta_1x_i^2 - \beta_1x_i\bar{x} \bigg) \\
&= \frac{1}{\sum x_i^2 - n \bar{x}^2} \times \beta_0\sum x_i - n\bar{x}\beta_0 + \beta_1 \sum x_i^2 - \beta_1\bar{x}\sum x_i \bigg) \\
&= \frac{1}{\sum x_i^2 - n \bar{x}^2} \times \beta_0n\bar{x} - n\bar{x}\beta_0 + \beta_1 \bigg( \sum x_i^2 - \bar{x}n\bar{x} \bigg) \\
&= \frac{1}{\sum x_i^2 - n \bar{x}^2} \times \beta_1 \bigg( \sum x_i^2 - n\bar{x}^2 \bigg) \\
&= \beta_1
\end{split}
$$


## $B_0$ is Unbiased

We can also prove the unbiasedness of the $B_0$ estimator. To do this, we need to show that $\mathbb{E}(B_0) = \beta_0$. Again, we first start with the definition of the intercept estimator:

$$
B_0 = \bar{y} - B_1(\bar{x})
$$


We start with the regression equation from Assumption 1, $y_i = \beta_0 + \beta_1(x_i)+ \epsilon_i$, and manipulate it to take the sum of both sides of the equation, and then divide by $n$.

$$
\begin{split}
\sum y_i &= \sum \bigg(\beta_0 + \beta_1(x_i)+ \epsilon_i\bigg) \\[1em]
&= \sum \beta_0 + \sum \beta_1(x_i) + \sum \epsilon_i \\[1em]
&= n \beta_0 + \beta_1 \sum (x_i) + \sum \epsilon_i \\[1em]
\frac{\sum y_i}{n} &= \frac{n \beta_0}{n} + \frac{\beta_1 \sum (x_i)}{n} + \frac{\sum \epsilon_i}{n}
\end{split}
$$

This last expression we can re-write as,

$$
\bar{y} = \beta_0 + \beta_1(\bar{x}) + \bar{\epsilon}
$$

Now we will substitute this into $\bar{y}$ in the definition of $B_0$.

$$
\begin{split}
B_0 &= \bar{y} - B_1(\bar{x}) \\[1em]
&= \beta_0 + \beta_1(\bar{x}) + \bar{\epsilon} - B_1(\bar{x}) \\[1em]
&= \beta_0 + \bar{x}\bigg(\beta_1 - B_1 \bigg) + \bar{\epsilon}
\end{split}
$$

Lastly, we take the expectation (conditional on $X$).

$$
\begin{split}
\mathbb{E}(B_0) &= \mathbb{E}\bigg(\beta_0 + \bar{x}(\beta_1 - B_1) + \bar{\epsilon}\bigg) \\[1em]
&= \mathbb{E}(\beta_0) + \mathbb{E}\bigg(\bar{x}(\beta_1 - B_1)\bigg) + \mathbb{E}(\bar{\epsilon}) \\[1em]
&= \beta_0 + \bar{x}\mathbb{E}(\beta_1 - B_1) + \mathbb{E}(\bar{\epsilon}) \\[1em]
&= \beta_0 + \bar{x}\bigg(\mathbb{E}(\beta_1) - \mathbb{E}(B_1)\bigg) + \mathbb{E}(\bar{\epsilon}) \\[1em]
&= \beta_0 + \bar{x}\bigg(\beta_1 - \mathbb{E}(B_1)\bigg) + \mathbb{E}(\bar{\epsilon}) 
\end{split}
$$

Using Assumption \#2, we find that $\mathbb{E}(\bar{\epsilon}) = 0$. We also just proved that $\mathbb{E}(B_1) = \beta_1$. So,

$$
\begin{split}
\mathbb{E}(B_0) &= \beta_0 + \bar{x}\bigg(\beta_1 - \beta_1\bigg) +0 \\[1em]
&= \beta_0 + \bar{x}(0) \\[1em]
&= \beta_0
\end{split}
$$




