---
title: "Assignment 08"
author: "Cross-Validation"
date: "Answer Key"
header-includes:
   - \usepackage{xcolor}
   - \definecolor{umn}{HTML}{FF2D21}
   - \usepackage{caption}
   - \captionsetup[table]{textfont={it}, labelfont={bf}, singlelinecheck=false, labelsep=period}
   - \captionsetup[figure]{textfont={it}, labelfont={bf}, singlelinecheck=false, labelsep=period}
   - \usepackage{floatrow}
   - \floatsetup[figure]{capposition=top}
   - \usepackage{xfrac}
   - \usepackage{booktabs}
   - \usepackage{float}
   - \floatstyle{plaintop}
   - \restylefloat{table}
   - \usepackage{siunitx}
   - \newcolumntype{d}{S[table-format=3.2]}
output: 
  pdf_document:
    highlight: tango
    latex_engine: xelatex
    fig_width: 6
    fig_height: 6
mainfont: "Sabon"
sansfont: "Helvetica Neue UltraLight"
monofont: "Inconsolata"
urlcolor: "umn"
always_allow_html: yes
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE)

library(knitr)
library(kableExtra)

library(broom)
library(glmnet)
library(MASS)
library(modelr)
library(patchwork)
library(tidyverse)

mpls = readr::read_csv("../data/mpls-violent-crime.csv") %>%
  mutate(year2 = year - 2000)
#mpls
```

This assignment is worth 20 points. 


## Part I: Minneapolis Violent Crime

**1. Create a scatterplot showing the violent crime rate as a function of time.** 

```{r echo=FALSE, fig.width=6, fig.height=6, out.width='3in', fig.align='center', fig.pos='H', message=FALSE, fig.cap='Plot of violent crime rate versus years since 2000. The loess smoother is also displayed.'}
ggplot(data = mpls, aes(x = year2, y = crime_rate)) +
  geom_point() +
  geom_smooth(se = FALSE) +
  theme_bw() +
  xlab("Years since 2000") +
  ylab("Violent crime rate per 100,000 people")
```

\vspace{3em}

**2. Based on the plot, describe the trend in violent crime rate over time.**

The plot of the data suggests that crime rate is non-linear and both increases and decreases at various points since 2000.

\vspace{3em}

**3. If you were going to fit a polynomial model to these data, what degree polynomial would you fit to? Explain.**

The data suggest a cubic polynomial may fit the data. The loess line suggests two changes in the direction of the trend.

\newpage

**4. Fit a series of polynomial models starting with a linear model, and then models that also include higher order polynomials that allow you to evaluate your response to Question #3. Be sure to fit models up to degree ð‘˜ + 1, where ð‘˜ is the degree you hypothesized in Question #3. Analyze each of the polynomial terms (including the linear term) by using a series of nested Fâ€tests. Report these results in an ANOVA table. (Note: If you need a refresher on fitting polynomial models and carrying out a nested Fâ€test, see the Polynomial Regression notes from EPsy 8252.)**

```{r echo=FALSE}
lm.1 = lm(crime_rate ~ 1 + year2, data = mpls)
lm.2 = lm(crime_rate ~ 1 + year2 + I(year2^2), data = mpls)
lm.3 = lm(crime_rate ~ 1 + year2 + I(year2^2) + I(year2^3), data = mpls)
lm.4 = lm(crime_rate ~ 1 + year2 + I(year2^2) + I(year2^3) + I(year2^4), data = mpls)

knitr::kable(anova(lm.1, lm.2, lm.3, lm.4))
```

\vspace{3em}

**5. Based on these results, which polynomial model would you adopt? Explain.**

Based on these results we would adopt the cubic polynomial model. Adding the cubic term in Model 3 results in a statistically significant increase in variance explained from the quadratic model, $F(1, 14)=8.89$, $p = 0.011$. The subsequent increase in variance when we add the 4th-degree polynomial term is not statistically significant, $F(1, 13)=0.83$, $p = 0.380$.

\vspace{3em}

**6. Write and include syntax that will carry out the LOOCV.**

```{r echo=TRUE}
# Set up empty vector to store results
mse_1 = rep(NA, 18)
mse_2 = rep(NA, 18)
mse_3 = rep(NA, 18)
mse_4 = rep(NA, 18)


# Loop through the cross-validation
for(i in 1:18){
  train = mpls %>% filter(row_number() != i)
  validate = mpls %>% filter(row_number() == i)
  
  lm.1 = lm(crime_rate ~ 1 + year2, data = train)
  lm.2 = lm(crime_rate ~ 1 + year2 + I(year2^2), data = train)
  lm.3 = lm(crime_rate ~ 1 + year2 + I(year2^2) + I(year2^3), data = train)
  lm.4 = lm(crime_rate ~ 1 + year2 + I(year2^2) + I(year2^3) + I(year2^4), data = train)
  
  yhat_1 = predict(lm.1, newdata = validate)
  yhat_2 = predict(lm.2, newdata = validate)
  yhat_3 = predict(lm.3, newdata = validate)
  yhat_4 = predict(lm.4, newdata = validate)
  
  mse_1[i] = (validate$crime_rate - yhat_1) ^ 2
  mse_2[i] = (validate$crime_rate - yhat_2) ^ 2
  mse_3[i] = (validate$crime_rate - yhat_3) ^ 2
  mse_4[i] = (validate$crime_rate - yhat_4) ^ 2
}

```

\vspace{3em}

**7. Report the cross-validated MSE for each of the models in your set of polynomial models.**

```{r}
x = # Compute CV-MSE
data.frame(
  Model = c("Linear", "Quadratic", "Cubic", "Quartic"),
  MSE = c(mean(mse_1), mean(mse_2), mean(mse_3), mean(mse_4))
)

knitr::kable(x)
```


\vspace{3em}

**8. Based on these results, which degree polynomial model should be adopted? Explain.**

Based on these results, we will adopt the cubic model. It has the lowest cross-validated MSE of the four candidate models.
 

## Part II: Course Evaluations

```{r message=FALSE}
evals = readr::read_csv("../data/evaluations.csv")
lm.full = lm(avg_eval ~ 1 + beauty + female + num_courses + native_english, data = evals)
```


**9. Using average course evaluation scores ($y$), compute the total sum of squares (SST). Show your work.**

```{r echo=TRUE}
sum((evals$avg_eval - mean(evals$avg_eval)) ^ 2)
```

\vspace{3em}

**10. Using average course evaluation scores ($y$) and the predicted values from the model ($\hat{y}$), compute the sum of squared errors (SSE). Show your work.**

```{r echo=TRUE}
sum((evals$avg_eval - fitted(lm.full)) ^ 2)
```

\vspace{3em}

**11. Compute the model $R^2$ value using the formula: $1 - \frac{\mathrm{SSE}}{\mathrm{SST}}$.**

```{r}
1 - 13.80276/19.7745
```

\newpage

**12. Write and include syntax that will carry out the 5-fold cross-validation. In this syntax use `set.seed(1000)` so that you and the answer key will get the same results.**

```{r echo=TRUE}
# Create 5 folds
set.seed(1000)
my_cv = evals %>% crossv_kfold(k = 5)

# Fit model to training set and get augmented data based on validation set
cv_1 = my_cv %>%
  mutate(
    model = map(train, ~lm(avg_eval ~ 1 + beauty + female + num_courses + native_english, data = .)),
    out = map2(model, test, ~ augment(.x, newdata = .y))
    ) %>%
  tidyr::unnest(out)
```


\vspace{3em}

**13. Report the five $R^2$ values from your analysis and the cross-validated $R^2$ value.**

```{r echo=TRUE}
# Group by validation set; compute SST, SSE, and R2
all_r2s = cv_1 %>%
  group_by(.id) %>%
  summarize(
    SSE = sum((avg_eval - .fitted) ^ 2),
    SST = sum((avg_eval - mean(avg_eval))^2)
  ) %>%
  mutate(
    R2 = 1 - SSE/SST
  ) 

all_r2s

# Compute cross-validated R2
all_r2s %>%
  summarize(
    Mean_R2 = mean(R2)
  )
```

\vspace{3em}

**14. How does this value compare to the $R^2$ value you computed in Question \#11, based on the data.**

The $R^2$ value based on the cross-validation (0.219) is much lower than the data-based $R^2$ value (0.302). 

\vspace{3em}

**15. Explain why the cross-validated estimate of $R^2$ is a better estimate than the data-based $R^2$.**

We expect the cross-validated $R^2$ value to be a better estimate since the errors (and thus the SSE) are minimized based on the data. Thus the data-based $R^2$ value is optimistic. The errors from the cross-validated data will be larger and thus the $R^2$ value will be smaller, and a better estimate of both how the model will perform on new data sets.


## Part III: Credit Balance

```{r message=FALSE}
credit = readr::read_csv("../data/credit.csv")
X = scale(credit)[ , -1] 
Y = scale(credit)[ , 1] 
```

**16. Use the `lm.ridge()` function to fit the same sequence of *d* values you used in the FOR loop from Assignment 6, Question 7. Running `MASS::select()` on this output, provides *d* values based on different criteria. Report the *d* value associated with the generalized cross-validation (GCV) metric.**

```{r}
out = lm.ridge(balance ~ . -1, data = data.frame(scale(credit)), lambda = seq(from = 0, to = 10, by = 0.001))
MASS::select(out)
```

The modified *d* value that has the lowest CVâ€MSE based on fitting a ridge regression is 1.849.

\vspace{3em}


**17. Re-run the FOR loop from Assignment 6, Question 7. Except this time compute the AICc and select the *d* value based on using the AICc. How does this compare to the *d* value you found using the GCV metric from the previous question? (Show your syntax.)**

```{r}
# Set up d sequence
d = seq(from = 0, to = 10, by = 0.001)

# FOR loop to cycle through the different values of d
aicc = c()

for(i in 1:length(d)){
  b = solve(t(X) %*% X + d[i]*diag(6)) %*% t(X) %*% Y 
  e = Y - (X %*% b)
  H = X %*% solve(t(X) %*% X + d[i]*diag(6)) %*% t(X) 
  df = sum(diag(H))
  
  # Create and store the AICc value
  aicc[i] = 400 * log(t(e) %*% e) + 2 * df + (2 * (df + 2) * (df + 3)) / (400 - df - 3)
  }


# Find d associated with smallest AIC
data.frame(d, aicc) %>% 
  filter(aicc == min(aicc))
```


The *d* value associated with the lowest AICc was $d=1.87$. This is quite comparable to the *d* value chosen with the GCV metric ($d=1.85$). 

\vspace{3em}

**18. Use 10-fold cross-validation to find the modified *d* value that has the lowest CV-MSE based on fitting an elastic net. (Prior to carrying out this analysis, set the seed for the random number generation to 100.) Then use this modified *d* value to re-fit the elastic net. Report the modified *d* value and the fitted equation from the elastic net.**

```{r}
set.seed(100)

en_models = cv.glmnet(
  x = X,
  y = Y,
  alpha = 0.5,
  standardize = FALSE,
  intercept = FALSE
)

# Choose modified d
en_models$lambda.min

tidy(glmnet(
  x = X,
  y = Y,
  alpha = 0.5,
  lambda = en_models$lambda.min,
  standardize = FALSE,
  intercept = FALSE
))
```

The modified *d* value chosen by the 10-fold cross-validation was 0.003. The fitted equation is:

$$
\begin{split}
\hat{\mathrm{Balance}}_i &= -0.57(\mathrm{Income}_i) + 0.68(\mathrm{Credit~Limit}_i) + 0.64(\mathrm{Credit~Rating}_i) + \\
&~~~~~~~~  0.04(\mathrm{Credit~Cards}_i) - 0.03(\mathrm{Age}_i) + 0.01(\mathrm{Education}_i)
\end{split}
$$

\vspace{3em}


**19. Compare the coefficients from the elastic net (from Question 18) to those from the ridge regression analyses fitted using the *d* value you found in Question 16. How are they different? Explain based on the penalty terms in the two models.**

```{r}
data.frame(
  Ridge =  coef(lm.ridge(balance ~ . -1, data = data.frame(scale(credit)), lambda = 1.85)),
  elas_net = tidy(glmnet(x = X, y = Y, alpha = 0.5, lambda = en_models$lambda.min, standardize = FALSE, intercept = FALSE))$estimate
) %>% 
  mutate(Predictor = c("Income", "Credit Limit", "Credit Rating", "Credit Cards", "Age", "Education")) %>%
  dplyr::select(Predictor, Ridge, elas_net) %>%
  knitr::kable(
    format = "latex",
    booktabs = TRUE,
    digits = 3, 
    col.names = c("Predictor", "Ridge Model", "Elastic Net"),
    align = c("l", "c"),
    caption = "Coefficients for the ridge regression (d=1.833) and elastic net (modified d = 0.0026)."
    ) %>%
  kable_styling(latex_options = "HOLD_position")


```

The coefficients from the two models are quite similar. The elastic net has shrunk them slightly more than the ridge model, but the difference is negligible. This is expected as the elastic net produces a slightly larger penalty than the ridge model.

\vspace{3em}

**20. Use the elastic net to compute a multiple $R^2$ value. Remember that multiple $R^2$ is the squared correlation between the observed and predicted values. Report this value. (Show your work.)**

```{r}
B = tidy(glmnet(x = X, y = Y, alpha = 0.5, lambda = en_models$lambda.min, standardize = FALSE, intercept = FALSE))$estimate

y_hat = X %*% B

cor(Y, y_hat)^2
```

The model explains 87.8\% of the variation in credit balance.
