---
title: "Collinearity and Biased Estimation"
author: "EPsy 8264"
date: "`r Sys.Date()`"
header-includes:
   - \usepackage{amsthm}
   - \usepackage{xcolor}
   - \usepackage{xfrac}
   - \usepackage{mdframed}
   - \usepackage{graphicx}
   - \usepackage{rotating}
   - \usepackage{booktabs}
   - \usepackage{caption}
   - \definecolor{umn}{RGB}{153, 0, 85}
   - \definecolor{umn2}{rgb}{0.1843137, 0.4509804, 0.5372549}
   - \definecolor{myorange}{HTML}{EA6153}
output: 
  pdf_document:
    highlight: tango
    latex_engine: xelatex
    fig_width: 6
    fig_height: 6
    includes:
      before_body: notes.tex
mainfont: "Minion Pro"
sansfont: "ITC Slimbach Std Book"
monofont: "Source Code Pro"
urlcolor: "umn2"
always_allow_html: yes
bibliography: epsy8264.bib
csl: apa-single-spaced.csl
---

\frenchspacing

```{r setup, include=FALSE, message=FALSE}
library(knitr)
library(kableExtra)
```

Collinearity is defined mathematically as when any of the predictors (*X*) in a regression model is a perfect linear combination of the other predictors:

$$
X_{k+1} = c_0(1) + c_1X_1 + c_2X_2 + c_3X_3 + \ldots + c_kX_k
$$

When this happens, the matrix $\mathbf{X}^{\intercal}\mathbf{X}$ is singular, and the OLS normal equations do not have a unique solution. Recall that the sampling variance for a predictor, $B_j$ is

$$
\mathrm{Var}(B_j) = \frac{1}{1 - R^2_j} \times \frac{\sigma^2_\epsilon}{(n-1)S^2_j}
$$

The first term in this product is referred to as the *variance inflation factor* (VIF). Recall that $R^2_j$ in this term is the squared multiple correlation between $X_j$ on the other $X$s. 
- When the correlation between $X_j$ and the other $X$s is 0 (they are independent; *orthogonal*) then the VIF becomes 1.
- When the correlation between $X_j$ and the other $X$s is high then the VIF becomes larger than 1; it becomes a multiplier of the variance.
- When there is perfect correlation between $X_j$ and the other $X$s then the VIF approaches $\infty$; the sampling variances (and SEs) are infinitely large.

### Collinearity in Practice

In practice it is rare to have perfect collinearity. When it does happen it is often the result of mis-formulating the model (e.g., including dummy variables in the model for all levels of a categorical variable, as well as the intercept).

It is, however, common to have strong less-than-perfect collinearity in practice. In thes cases the VIF will be less than 1, but can still have an adverse effect on the sampling variances; making them quite large.



### Detecting Collinearity

We can empirically diagnose problematic collinearity in the data using several methods. Before we do, however, it is important to first determine the functional form of the model (model specification). Collinearity produces unstable estimates of the coefficients and sampling variances which result from strong linear relationships between the predictors. In applied work, the model needs to be specified before we can estimate coefficients or their sampling variances; hence, collinearity should only be investigated after the model has been satisfactorily specified. 

## Data Set




The school that has a fitted value of $-1.38$ and studentized residual of $-2.6$ may be problematic.

```{r}
# Model-level information
glance(lm.1)

# Coefficient-level information
tidy(lm.1)
```

Examining this information we find:

- 20\% of the variation in student achievement is explained by the model; which is statistically significant $F(3, 66)=5.72$; $p=0.002$.
- However, none of the individual coefficients are statistically significant!

These results are typical when there is collinearity in the model. In this example, the collinearity could have been anticipated by examining the pairwise correlations between the predictors.

```{r}
eeo %>%
  correlate()
```

 

## Signs of Potential Collinearity

In our example we were alerted to the possible collinearity by finding that the predictors jointly were statistically significant, but that each of the individual predictors were not. Other signs that you may have collinearity problems are:

- Large changes in the size of the estimated coefficients when variables are added to the model;
- Large changes in the size of the estimated coefficients when an observation is added or deleted;
- The signs of the estimated coefficients do not conform to their prior substantively hypothesized directions;
- Large SEs on variables that are expected to be important predictors.

## Detecting Collinearity

Unfortunately the source of collinearity may be due to more than just the simple relationships among the predictors. As such, just examining the pairwise correlations is not enough to detect collinearity (although it is a good first step). There are two common methods statisticians use to detect collinearity, (1) computing variance inflation factors for the coefficients, and (2) examining the eigenvalues of the correlation matrix. 

### Variance Inflation Factor

The variance inflation factor (VIF) is an indicator of the degree of collinearity, where VIF is:

$$
\mathrm{VIF} = \frac{1}{1 - R^2_j}
$$

The VIF impacts the size of the variance estimates for the regression coefficients, and as such, can be used as a diagnostic of collinearity. In practice, since it is more conventional to use the SE to measure uncertainty, it is typical to use the square root of the VIF as a diagnostic of collinearity in practice. The square root of the VIF expresses the proportional change in the CI for the coefficients. 

```{r echo=FALSE, eval=FALSE}
tab_01 = data.frame(
  R = seq(from = 0.5, to = 0.9, by = 0.1)
) %>%
  mutate(
    R2 = R^2,
    VIF = 1 / (1 - R2),
    sqrt_vif = sqrt(VIF)
  )

knitr::kable(tab_01, digits = 2, col.names = c("$R$", "$R^2$", "$\\mathrm{VIF}$", "$\\sqrt{\\mathrm{VIF}}$"), format = "latex")
```

\begin{tabular}{cccc}
\hline
$R_j$ & $R^2_j$ & $\mathrm{VIF}$ & $\sqrt{\mathrm{VIF}}$\\
\hline
0.5 & 0.25 & 1.33 & 1.15\\
\hline
0.6 & 0.36 & 1.56 & 1.25\\
\hline
0.7 & 0.49 & 1.96 & 1.40\\
\hline
0.8 & 0.64 & 2.78 & 1.67\\
\hline
0.9 & 0.81 & 5.26 & 2.29\\
\hline
\end{tabular}

For example, if the correlation between $X_j$ and the other $X$s is 0.9, then the CIs for the coefficients would increase by a factor of of 2.29. The uncertainty in the estimates would more than double!

In our example, we can use the `vif()` function from the **car** package to compute the variance inflation factors for each coefficient.

```{r}
# VIF
vif(lm.1)

# Square root of VIF
sqrt(vif(lm.1))
```

All three coefficients are impacted by VIF. The SEs for these coefficients are all more than five times as large as they would be if the predictors were independent.

### Eigenvalues of the Correlation Matrix

Each $k \times k$ matrix has a set of $k$ scalars, called eigenvalues (denoted $\lambda$) associated with it. These eigenvalues can be arranged in descending order such that,

$$
\lambda_1 \geq \lambda_2 \geq \lambda_3 \geq \ldots \geq \lambda_k
$$

Because the correlation matrix of the predictors is a square matrix, we can find a corresponding set of eigenvalues for this correlation matrix. It turns out that if any of these eigenvalues is exactly equal to zero, there would be a linear dependence among the predictors. In practice, if one of the eigenvalues is quite a bit smaller than the others (and near zero), there is collinearity.

Empirically, we compute the sum of the reciprocals of the eigenvalues 

$$
\sum_{i=1}^k \frac{1}{\lambda_i}
$$

If the sum is greater than a given criterion, say, five times the number of predictors, it is a sign of collinearity.

```{r}
# Correlation matrix of predictors
x = cor(eeo[c("faculty", "peer", "school")])

# Compute eigenvalues and eigenvectors
eigen(x)

# Sum of reciprocal of eigenvalues
sum(1 / eigen(x)$values)
```

Since this sum is greater than 15 (five times the number of predictors) then we would conclude that there is a collinearity problem. 

### Condition Indices

One related diagnostic measure of collinearity are the *condition indices* of the correlation matrix; see @Belsley:1991 and @Belsley:1980. The *j*th condition index is defined as

$$
\kappa_j = \sqrt{\frac{\lambda_1}{\lambda_j}}
$$

for $j=1,2,3,\ldots,k$, where $\lambda_1$ is the first (largest) eigenvalue and $\lambda_j$ is the *j*th eigenvalue. 

The first condition index, $\kappa_1$, will always be equal to 1, and the other condition indices will be larger than one. The largest condition index, which will be,

$$
\kappa_k = \sqrt{\frac{\lambda_1}{\lambda_k}}
$$

where $\lambda_k$ is the smallest eigenvalue, is known as the *condition number* of the correlation matrix. If the condition number is small, it indicates that the predictors are not collinear, whereas large condition numbers are evidence supporting collinearity. 

From empirical work, condition numbers that exceed 15 are typically problematic (this indicates that the maximum eigenvalue is more than 225 times greater than the maximum eigenvalue). When the condition number exceeds 30, corrective action will almost surely need to be taken.

```{r}
# Compute condition indices
sqrt(max(eigen(x)$values) / eigen(x)$values)
```

The condition number of 19.26, suggests strong collinearity among the predictors.



## Fixing Collinearity in Practice

Although there are several solutions in practice, none are a magic bullet. Here are three potential fixes:

- Re-specify the model
    - Combine collinear predictors, 
    - Drop one (or more) of the collinear predictors---This changes what you are controlling for.
- Biased estimation
    - Trade small amount of bias for a reduction in coefficient variability
- Introduce prior information about the coefficients
    - This can be done formally in the analysis (e.g., Bayesian analysis)
    - It can be used to give a different model specification

Note that although collinearity is a data problem, the most common fixes in practice are to change the model. For example, we could alleviate the collinearity by dropping any two of the predictors and re-fitting the model with only one predictor. 

This would fix the problem, but would be unsatisfactory because the resulting model would not allow us to answer the research question. The highly correlated relationships between the predictors is an inherent characteristic of the data generating process we are studying. This makes it difficult to estimate the individual effects of the predictors. Instead, we could look for underlying causes that would explain the relationships we found among the predictors and perhaps re-formulate the model using these underlying causes.



# References
