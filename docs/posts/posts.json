[
  {
    "path": "posts/2021-10-25-assignment-06/",
    "title": "üêê Assignment 06",
    "description": "This goal of this assignment is to give you experience using ridge regression for alleviating interpretability problems that arise because of collinearity.",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-10-25",
    "categories": [
      "Assignments"
    ],
    "contents": "\nThe goal of the analysis you are going to undertake in this assignment is to build a model that predicts customers‚Äô credit card balance. To do this you will use the data provided in the file credit.csv. The six predictors included in the dataset have all been previously shown to predict credit card balance.\n[CSV]\n[Data Codebook]\n\nInstructions\nSubmit a printed document of your responses to the following questions. Please adhere to the following guidelines for further formatting your assignment:\nAll plots should be resized so that they do not take up more room than necessary.\nAll figures and tables should have a name (e.g., Figure 1) and an appropriate caption.\nIn questions that ask you to ‚Äúuse matrix algebra‚Äù to solve the problem, you can either show your syntax and output from carrying out the matrix operations, or you can use Equation Editor to input the matrices involved in your calculations.\nThis assignment is worth 20 points.\n\nExploratory Analysis\nCreate and report a correlation matrix of the outcome (balance) and the six predictors.\nBased on the correlation matrix, comment on whether there may be any potential collinearity problems. Explain.\nFit the OLS model that regresses customers‚Äô credit card balance on the six predictors. (Don‚Äôt forget to standardize any numeric variables prior to fitting the model.) Report the coefficient-level output, including the estimated coefficients, standard errors, t-values, and p-values.\nCompute and report the condition number for the \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) matrix. Show your work.\nBased on the condition number of the \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) matrix, is there evidence of collinearity? Explain.\nCompute and report the VIF values for the standardized regression. Based on the VIF values, which estimates from the coefficient-level output you reported in Question 3 are likely affected by the collinearity? Explain.\n\nFinding an Optimal \\(\\lambda\\) Value\nIn this section, you will find a \\(\\lambda\\) value to use in a ridge regression analysis to alleviate estimation problems associated with the near perfect collinearity. Consider all \\(\\lambda\\) values between 0 and 10 (inclusive). Make sure that you can identify \\(\\lambda\\) to the nearest 1000th.\nUse the AIC to help you select the optimal \\(\\lambda\\) value to use in the ridge regression. What is the value of \\(\\lambda\\) you will use in the ridge regression? Show your work.\nCreate a ridge trace plot that includes the values of \\(\\lambda\\) you examined in Question 7. Also include a guideline indicating the value of \\(\\lambda\\) chosen by the AIC.\nUse the ridge trace plot you created to indicate the direction of bias for each of the coefficients. Explain.\n\nFitting the Ridge Regression Model\nIn this section, you will fit a ridge regression using your identified \\(\\lambda\\) value.\nUse matrix algebra to compute the ridge regression coefficient estimates using the \\(\\lambda\\) value you identified in Question #7. Show your work.\nFit the ridge regression model to the standardized credit data using the \\(\\lambda\\) value you identified in Question #7. Report the fitted equation based on the ridge regression.\n\nCoefficient-Level Summaries\nAlthough they are not meaningful in practice, as an exercise, I still want you to compare the SEs from the standardized OLS and ridge regression models. Create and report a table that allows a comparison of the standard error estimates for the coefficients estimated in each of the two models.\nWhich coefficients saw the biggest reduction in their SEs? How could you predict this? Explain. (Hint: Revisit your response to Question #6.)\nCreate a coefficient-level regression table that reports the estimates, SEs, t-values, p-values, and confidence intervals for each of the predictors from the ridge regression model.\nCompute and report the amount of bias in each of the coefficients. Show your work.\nCompute the VIF values for each of the coefficients from the ridge regression model. The VIF value for the ith coefficient is computed as:\n\\[\n\\mathrm{VIF}(\\hat\\beta_i) = \\mathrm{R}_{i,i} \\times \\frac{\\mathrm{det}(\\mathrm{R}_{-i,-i})}{\\mathrm{det}(\\mathrm{R})} \n\\]\nwhere R is the standardized (correlation) matrix of the sampling variances and covariances of the ridge coefficients, \\(\\mathrm{R}_{i,i}\\) is the element in the ith row and ith column of R, and \\(\\mathrm{R}_{-i,-i}\\) is the matrix composed of all rows and columns of R except the ith. (Hint: You computed the unstandardized matrix of the sampling variances and covariances of the ridge coefficients in Question #12. To turn a variance‚Äìcovariance matrix into a correlation matrix use the cov2cor() function.)\nBased on the VIF values, have we eliminated the collinearity problems? Explain.\n\nModel-Level Summaries\nCompute and report the model-level \\(R^2\\) for the ridge regression model. (Hint: Remember that the model-level \\(R^2\\) is the squared correlation between the observed and predicted values of the outcome.) Show your work. How does this compare to the \\(R^2\\) from the OLS model?\nCompute and report the F-value associated with the \\(R^2\\) value you computed in Question #18.\nCompute and report the p-value associated with the test of whether \\(\\rho^2=0\\).\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-10-25T11:56:49-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-25-assignment-07/",
    "title": "üêù Assignment 07",
    "description": "This goal of this assignment is to give you experience using cross-validation methods in regression analyses.",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-10-25",
    "categories": [
      "Assignments"
    ],
    "contents": "\nInstructions\nSubmit a printed document of your responses to the following questions. Please adhere to the following guidelines for further formatting your assignment:\nAll plots should be resized so that they do not take up more room than necessary.\nAll figures and tables should have a name (e.g., Figure 1) and an appropriate caption.\nIn questions that ask you to ‚Äúuse matrix algebra‚Äù to solve the problem, you can either show your syntax and output from carrying out the matrix operations, or you can use Equation Editor to input the matrices involved in your calculations.\nThis assignment is worth 20 points.\n\nPart I: Minneapolis Violent Crime\nFor the first part of this assignment, you will use the data provided in the file mpls-violent-crime.csv to build a model that examines the trend in violent crime rate over time.\n[CSV]\n[Data Codebook]\n\nPreparation\nCreate a variable that indicates the number of years since 2000. Use this variable in all analyses for Part I rather than the year variable.\n\nDescription\nCreate a scatterplot showing the violent crime rate as a function of time.\nBased on the plot, describe the trend in violent crime rate over time.\nIf you were going to fit a polynomial model to these data, what degree polynomial would you fit? Explain.\n\nUse p-Value methods for Model Selection\nFit a series of polynomial models starting with a linear model, and then models that also include higher order polynomials that allow you to evaluate your response to Question #3. Be sure to fit models up to degree \\(k+1\\), where \\(k\\) is the degree you hypothesized in Question #3. Analyze each of the polynomial terms (including the linear term) by using a series of nested F-tests. Report these results in an ANOVA table. (Note: If you need a refresher on fitting polynomial models and carrying out a nested F-test, see the Polynomial Regression notes from EPsy 8252.)\nBased on these results, which polynomial model would you adopt? Explain.\n\nUsing LOOCV for Model Selection\nIn this section of the assignment, you are going to use LOOCV to evaluate the MSE for the same set of polynomial models you evaluated in Question #4.\nWrite and include syntax that will carry out the LOOCV.\nReport the cross-validated MSE for each of the models in your set of polynomial models.\nBased on these results, which degree polynomial model should be adopted? Explain.\n\nPart II: Course Evaluations\nFor the second part of this assignment, you will use the data provided in the file evaluations.csv to build a model that predicts variation in course evaluation scores.\n[CSV]\n[Data Codebook]\n\nPreparation\nBegin by fitting a model that predicts average course evaluation score using the following predictors: beauty, number of courses for which the professor has evaluations, whether the professor is a native English speaker, and whether the professor is female.\n\nDescription\nUsing average course evaluation scores (y), compute the total sum of squares (SST). Show your work.\nUsing average course evaluation scores (y) and the predicted values from the model (\\(\\hat{y}\\)), compute the sum of squared errors (SSE). Show your work.\nCompute the model \\(R^2\\) value using the formula: \\(1 - \\frac{\\mathrm{SSE}}{\\mathrm{SST}}\\).\n\nUsing k-Fold Cross-Validation to Estimate \\(R^2\\)\nAs we know, the estimate for \\(R^2\\) is biased. We can obtain a better estimate of \\(R^2\\) by using cross-validation. You will use 5-fold cross-validation to estimate the \\(R^2\\) value. The algorithm for this will be:\nRandomly divide the beauty data into 5 folds.\nHold out 1 fold as your validation data and use the remaining 4 folds as your training data.\nFit the model to the training data.\nUse the estimated coefficients from those fits to compute \\(\\hat{y}\\) values using the validation data.\nCompute the SST and SSE values for the validation data, and use those to compute \\(R^2\\) based on the formula \\(1 - \\frac{\\mathrm{SSE}}{\\mathrm{SST}}\\). (Note that sometimes the \\(R^2\\) may be negative when we compte it in this manner.)\n\nRepeat for each fold.\nCompute the cross-validated \\(R^2\\) by finding the mean of the five \\(R^2\\) from the cross-validations.\nWrite and include syntax that will carry out the 5-fold cross-validation. In this syntax use set.seed(1000) so that you and the answer key will get the same results. (This website may be useful in using the purrr package to obtain the y- and \\(\\hat{y}\\)-values in order to compute the SST and SSE values: https://drsimonj.svbtle.com/k-fold-cross-validation-with-modelr-and-broom)\nReport the five \\(R^2\\) values from your analysis and the cross-validated \\(R^2\\) value.\nHow does this value compare to the \\(R^2\\) value you computed in Question #11, based on the data.\nExplain why the cross-validated estimate of \\(R^2\\) is a better estimate than the data-based \\(R^2\\).\n\nPart III: Credit Balance\nFor the third part of this assignment, you will again use the file the data provided in the file credit.csv to build a model that predicts customers‚Äô credit card balance. Do not forget to standardize all the variables.\n[CSV]\n[Data Codebook]\n\nUse the lm.ridge() function to fit the same sequence of \\(\\lambda\\) values you used in Question #7 from Assignment 6. Running select() on this output, provides \\(\\lambda\\) values based on different criteria. Report the \\(\\lambda\\) value associated with the generalized cross-validation (GCV) metric.\nRe-do Question #7 from Assignment 6, except this time, select the optimal \\(\\lambda\\) value based on using the AICc. How does this compare to the \\(\\lambda\\) value you found using the GCV metric from the previous question? (Show your syntax.)\nCompute the coefficients, standard errors based on the ridge regression model based on the \\(\\lambda\\) value you identified in Question #17. Also compute the t-values, and p-values for each coefficient. Report all of these in a coefficient table.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-10-26T14:11:58-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-25-reading-biased-estimation-ridge-regression/",
    "title": "üìñ Biased Estimation: Ridge Regression",
    "description": "Reading",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-10-25",
    "categories": [
      "Reading"
    ],
    "contents": "\nRead the following:\nFortmann-Roe, S. (2012). Understanding the bias-variance tradeoff.\n\nAdditional Resources\nCross-Validated. (2014). Why is ridge regression called ‚Äúridge‚Äù, why is it needed, and what happens when \\(\\lambda\\) goes to infinity?.\nJames, G., Witten, D., Hastie, T., & Tibshirani, R. (2013). An introduction to statistical learning: with applications in R. New York: Springer.\nStatistical Learning MOOC taught by Hastie and Tibshirani\n\n\n\n",
    "preview": {},
    "last_modified": "2021-10-25T10:13:04-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-20-notes-biased-estimation-ridge-regression/",
    "title": "üìù Biased Estimation: Ridge Regression",
    "description": "A brief introduction to ridge regression for dealing with collinearity. Data from @Chatterjee:2012.",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-10-20",
    "categories": [
      "Notes"
    ],
    "contents": "\n\n\n\nIn 1964, the US Congress passed the Civil Rights Act and also ordered a survey of school districts to evaluate the availability of equal educational opportunity in public education. The results of this survey were reported on in Coleman et al. (1966) and Mosteller & Moynihan (1972). We will use a subset of the data collected (in equal-educational-opportunity.csv) to mimic one of the original regression analyses performed; examining whether the level of school facilities was an important predictor of student achievement after accounting for the variation in faculty credentials and peer influence.\n[CSV]\n[R Script File]\nThe data come from a random sample of 70 schools in 1965. The variables, which have all been mean-centered and standardized, include:\nachievement: Measurement indicating the student achievement level\nfaculty: Measurement indicating the faculty‚Äôs credentials\npeer: Measurement indicating the influence of peer groups in the school\nschool: Measurement indicating the school facilities (e.g., building, teaching materials)\n\n\n# Load libraries\nlibrary(broom)\nlibrary(MASS)\nlibrary(tidyverse)\n\n# Read in data\neeo = read_csv(\"~/Documents/github/epsy-8264/data/equal-education-opportunity.csv\")\nhead(eeo)\n\n\n# A tibble: 6 √ó 4\n  achievement faculty    peer school\n        <dbl>   <dbl>   <dbl>  <dbl>\n1      -0.431   0.608  0.0351  0.166\n2       0.800   0.794  0.479   0.534\n3      -0.925  -0.826 -0.620  -0.786\n4      -2.19   -1.25  -1.22   -1.04 \n5      -2.85    0.174 -0.185   0.142\n6      -0.662   0.202  0.128   0.273\n\n\nBiased Estimation\nThe Gauss-Markov Theorem posits many attractive features of the OLS regression model. Two of these properties are that the OLS estimators will be unbiased and that the sampling variances of the coefficients are as small as possible1. In our example, the fitted model using faculty credentials, peer influence, and school facilities to predict variation in achievement showed strong evidence of collinearity; which makes the SEs super large even if they are the minimum of all possible linear, unbiased estimates.\nOne method of dealing with collinearity is to use a biased estimation method. These methods forfeit unbiasedness to decrease the size of the sampling variances; it is bias‚Äìvariance tradeoff. The goal with these methods is to trade a small amount of bias in the estimate for a large reduction in the sampling variances for the coefficients.\n\nThe bias‚Äìvariance tradeoff is a commonplace, especially in prediction models. You can explore it in more detail at http://scott.fortmann-roe.com/docs/BiasVariance.html.\n\nRidge Regression\nTo date, the most commonly used biased estimation method in the social sciences is ridge regression. Instead of finding the coefficients that minimize the sum of squared errors, ridge regression finds the coefficients that minimize a penalized sum of squares, namely:\n\\[\n\\mathrm{SSE}_{\\mathrm{Penalized}} = \\sum_{i=1}^n \\bigg(y_i - \\hat y_i\\bigg)^2 + \\lambda \\sum_{j=1}^p \\beta_j^2\n\\]\nwhere, \\(\\lambda\\geq0\\) is a scalar value, \\(\\beta_j\\) is the jth regression coefficient, and \\(y_i\\) and \\(\\hat{y}_i\\) are the observed and model fitted values, respectively. One way to think about this formula is:\n\nNote that \\(\\lambda\\) here is NOT an eigenvalue.\n\\[\n\\mathrm{SSE}_{\\mathrm{Penalized}} = \\mathrm{SSE} + \\mathrm{Penalty}\n\\]\nMinimizing over this penalized sum of squared error has the effect of ‚Äúshrinking‚Äù the mean square error and coefficient estimates toward zero. As such, ridge regression is part of a larger class of methods known as shrinkage methods.\nThe \\(\\lambda\\) value in the penalty term controls the amount of shrinkage. When \\(\\lambda = 0\\), the entire penalty term is 0, and the penalized sums of squared error reduces to the non-penalized sum of squared errors:\n\\[\n\\mathrm{SSE}_{\\mathrm{Penalized}} = \\mathrm{SSE}\n\\]\nMinimizing this will, of course, produce the OLS estimates. The bigger \\(\\lambda\\) is, the more the model‚Äôs residual variance estimate and coefficients shrink toward zero. At the extreme end, \\(\\lambda = \\infty\\) will shrink every coefficient to zero.\n\nMatrix Formulation of Ridge Regression\nRecall that the OLS estimates are given by:\n\\[\n\\mathbf{b} = (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}\\mathbf{y}\n\\]\nUnder collinearity, the \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) matrix is ill-conditioned. (A matrix is said to be ill-conditioned if it has a high condition number which is computed as \\(\\mathrm{Condition~Number} = \\frac{\\vert\\lambda_{\\mathrm{Max}}\\vert}{\\vert\\lambda_{\\mathrm{Min}}\\vert}\\).) This ill-conditioning results in inaccuracy when we compute the inverse of the \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) matrix, which translates into bad estimates of the coefficients and standard errors.\nTo see this, consider our EEO example data. We first standardize the variables (so we can omit the ones column in the design matrix) using the scale() function. Note that the output of the scale() function is a matrix. Then, we will select the predictors using indexing and compute the condition number for the \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) matrix.\n\n\n# Standardize all variables in the eeo data frame\nz_eeo = eeo %>% \n  scale()\n\n# Create and view the design matrix\nX = z_eeo[ , c(\"faculty\", \"peer\", \"school\")]\nhead(X)\n\n\n        faculty        peer     school\n[1,]  0.5158617 -0.01213684  0.1310782\n[2,]  0.6871673  0.46812962  0.4901170\n[3,] -0.8084583 -0.71996624 -0.7994390\n[4,] -1.2024935 -1.36577136 -1.0479983\n[5,]  0.1150408 -0.25030749  0.1078451\n[6,]  0.1413252  0.08793894  0.2356566\n\n# Get eigenvalues\neig_val = eigen(t(X) %*% X)$values\n\n# Compute condition number\nabs(max(eig_val)) / abs(min(eig_val))\n\n\n[1] 370.8844\n\nWe can inflate the diagonal elements of \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) to better condition the matrix. This, hopefully, leads to more stability in the inverse matrix and produces better (albeit biased) coefficient estimates. To do this, we can add some constant amount (\\(\\lambda\\)) to each of the diagonal elements of \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\), prior to finding the inverse. This can be expressed as:\n\\[\n\\widetilde{\\mathbf{b}} = (\\mathbf{X}^{\\intercal}\\mathbf{X} + \\lambda \\mathbf{I})^{-1}\\mathbf{X}^{\\intercal}\\mathbf{Y}\n\\]\nwhere the tilde over b indicates that the coefficients are biased. To see how increasing the diagonal values leads to better conditioning, we will add some value (here \\(\\lambda=10\\), but it could be any value between 0 and positive infinity) to each diagonal element of the \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) matrix in our example.\n\nTechnically this equation is for standardized variables; it assumes that there is no ones column in the X matrix. This is because we only want to add the \\(\\lambda\\) value to the parts of the \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) matrix associated with the predictors.\n\n\n# Add 50 to each of the diagonal elements of X^T(X)\ninflated = t(X) %*% X + 10*diag(3)\n\n# Get eigenvalues\neig_val_inflated = eigen(inflated)$values\n\n# Compute condition number\nabs(max(eig_val_inflated)) / abs(min(eig_val_inflated))\n\n\n[1] 20.25629\n\nThe condition number has decreased from 359 to 20.72. Adding 10 to each diagonal element of the \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) matrix resulted in a better conditioned matrix!\n\nRidge Regression in Practice: An Example\nLet‚Äôs fit a ridge regression model to our EEO data. For now, we will choose the value for \\(\\lambda\\). Let‚Äôs use \\(\\lambda=0.1\\). It is recommended that you always standardize all variables prior to fitting a ridge regression because large coefficients will impact the penalty in the SSE more than small coefficients. Here we will again use the design matrix based on the standardized predictors we created earlier, and also the vector of y, also based on standardized values. Here we use the diagonal inflation value of \\(\\lambda=0.1\\), just to illustrate the concept of ridge regression:\n\n\n# Create y vector\ny = z_eeo[ , \"achievement\"]\n\n# Compute and view lambda(I)\nlambda_I = 0.1 * diag(3)\nlambda_I\n\n\n     [,1] [,2] [,3]\n[1,]  0.1  0.0  0.0\n[2,]  0.0  0.1  0.0\n[3,]  0.0  0.0  0.1\n\n# Compute ridge regression coefficients\nb = solve(t(X) %*% X + lambda_I) %*% t(X) %*% y\nb\n\n\n              [,1]\nfaculty  0.4347370\npeer     0.8552354\nschool  -0.8491600\n\nThe fitted ridge regression model using \\(\\lambda=0.1\\) is:\n\\[\n\\hat{\\mathrm{Achievement}}^{\\star}_i = 0.435(\\mathrm{Faculty}^{\\star}_i) + 0.855(\\mathrm{Peer}^{\\star}_i) - 0.849(\\mathrm{School}^{\\star}_i)\n\\]\nwhere the star-superscript denotes a standardized (z-scored) variable.\n\nUsing an Built-In R Function\nWe can also use the lm.ridge() function from the {MASS} package to fit a ridge regression. This function uses a formula based on variables from a data frame in the same fashion as the lm() function. It also takes the argument lambda= which specifies the values of \\(\\lambda\\) to use in the penalty term. Because the data= argument has to be a data frame (or tibble), we convert the standardized data (which is a matrix) to a data frame using the data.frame() function.\n\n\n# Create data frame for use in lm.ridge()\nz_data = z_eeo %>%\n  data.frame()\n\n# Fit ridge regression (lambda = 0.1)\nridge_1 = lm.ridge(achievement ~ -1 + faculty + peer + school, data = z_data, lambda = 0.1)\n\n# View coefficients\ntidy(ridge_1)\n\n\n# A tibble: 3 √ó 5\n  lambda    GCV term    estimate scale\n   <dbl>  <dbl> <chr>      <dbl> <dbl>\n1    0.1 0.0121 faculty    0.433 0.993\n2    0.1 0.0121 peer       0.850 0.993\n3    0.1 0.0121 school    -0.845 0.993\n\n\nComparison to the OLS Coefficients\nHow do these biased coefficients from the ridge regression compare to the unbiased coefficients from the OLS estimation? Below, we fit a standardized OLS model to the data, and compare the coefficients to those from the ridge regression with \\(\\lambda=0.1\\).\n\n\n# Fit standardized OLS model\nlm.1 = lm(achievement ~ faculty + peer + school - 1, data = z_data)\ncoef(lm.1) #View coefficients\n\n\n   faculty       peer     school \n 0.5248647  0.9449056 -1.0272986 \n\nComparing these coefficients from the different models:\n\n\nTable 1: Comparison of the coefficients from the OLS (\\(\\lambda=0\\)) and the ridge regression using \\(\\lambda=0.1\\) based on the standardized data.\n\n\nPredictor\n\n\n\\(\\lambda=0\\)\n\n\n\\(\\lambda=0.1\\)\n\n\nFaculty\n\n\n0.525\n\n\n0.436\n\n\nPeer\n\n\n0.945\n\n\n0.856\n\n\nSchool\n\n\n-1.027\n\n\n-0.851\n\n\nBased on this comparison, the ridge regression has ‚Äúshrunk‚Äù the estimate of each coefficient toward zero. Remember, the larger the value of \\(\\lambda\\), the more the coefficient estimates will shrink toward 0. The table below shows the coefficient estimates for four different values of \\(\\lambda\\).\n\n\nTable 2: Comparison of the coefficients from the OLS (\\(\\lambda=0\\)) and three ridge regressions (using \\(\\lambda=0.1\\), \\(\\lambda=1\\), and \\(\\lambda=10\\)) based on the standardized data. The condition numbers for the \\(\\mathbf{X}^{\\intercal}\\mathbf{X} + \\lambda\\mathbf{I}\\) matrix are also provided.\n\n\nPredictor\n\n\n\\(\\lambda=0\\)\n\n\n\\(\\lambda=0.1\\)\n\n\n\\(\\lambda=1\\)\n\n\n\\(\\lambda=10\\)\n\n\nFaculty\n\n\n0.525\n\n\n0.436\n\n\n0.180\n\n\n0.114\n\n\nPeer\n\n\n0.945\n\n\n0.856\n\n\n0.537\n\n\n0.227\n\n\nSchool\n\n\n-1.027\n\n\n-0.851\n\n\n-0.283\n\n\n0.073\n\n\nCondition Number\n\n\n370.884\n\n\n313.908\n\n\n132.125\n\n\n20.256\n\n\nExamining these results we see that increasing the penalty (i.e., higher \\(\\lambda\\) values) better conditions the \\(\\mathbf{X}^{\\intercal}\\mathbf{X} + \\lambda\\mathbf{I}\\) matrix, but a higher penalty also shrinks the estimates toward zero more (increased bias). This implies that we want a \\(\\lambda\\) that is large enough so that it conditions the \\(\\mathbf{X}^{\\intercal}\\mathbf{X} + \\lambda\\mathbf{I}\\) matrix, but not too large because we want to introduce the least amount of bias as possible.\n\nChoosing \\(\\lambda\\)\nIn practice you need to specify the \\(\\lambda\\) value to use in the ridge regression. Ideally you want to choose a value for \\(\\lambda\\) that:\nIntroduces the least amount of bias possible, while also\nObtaining better sampling variances.\nThis is an impossible task without knowing the true values of the coefficients (i.e., the \\(\\beta\\) values). There are, however, some empirical methods to help us in this endeavor. One of these methods is to create and evaluate a plot of the ridge trace.\n\nRidge Trace\nA ridge trace computes the ridge regression coefficients for many different values of \\(\\lambda\\). A plot of this trace, can be examined to select the \\(\\lambda\\) value. To do this, we pick the smallest value for \\(\\lambda\\) that produces stable regression coefficients. Here we examine the values of \\(\\lambda\\) where \\(\\lambda = \\{ 0,0.001,0.002,0.003,\\ldots,100\\}\\).\nTo create this plot, we fit a ridge regression that includes a sequence of values in the lambda= argument of the lm.ridge() function. Then we use the tidy() function to summarize the output from this model. This output includes the coefficient estimates for each of the \\(\\lambda\\) values in our sequence. We can then create a line plot of the coefficient values versus the \\(\\lambda\\) values for each predictor.\n\n\n# Fit ridge model across several lambda values\nridge_models = lm.ridge(achievement ~ -1 + faculty + peer + school, data = z_data, \n                        lambda = seq(from = 0, to = 100, by = 0.01))\n\n# Get tidy() output\nridge_trace = tidy(ridge_models)\nridge_trace\n\n\n# A tibble: 30,003 √ó 5\n   lambda    GCV term    estimate scale\n    <dbl>  <dbl> <chr>      <dbl> <dbl>\n 1   0    0.0122 faculty    0.521 0.993\n 2   0    0.0122 peer       0.938 0.993\n 3   0    0.0122 school    -1.02  0.993\n 4   0.01 0.0122 faculty    0.511 0.993\n 5   0.01 0.0122 peer       0.928 0.993\n 6   0.01 0.0122 school    -1.00  0.993\n 7   0.02 0.0122 faculty    0.501 0.993\n 8   0.02 0.0122 peer       0.918 0.993\n 9   0.02 0.0122 school    -0.980 0.993\n10   0.03 0.0122 faculty    0.491 0.993\n# ‚Ä¶ with 29,993 more rows\n\n# Ridge trace\nggplot(data = ridge_trace, aes(x = lambda, y = estimate)) +\n  geom_line(aes(group = term, color = term)) +\n  theme_bw() +\n  xlab(\"d value\") +\n  ylab(\"Coefficient estimate\") +\n  ggsci::scale_color_d3(name = \"Predictor\")\n\n\n\n\nFigure 1: Ridge plot showing the size of the standardized regression coefficients for \\(\\lambda\\) values between 0 (OLS) and 100.\n\n\n\nWe want to find the \\(\\lambda\\) value where the lines begin to flatten out; where the coefficients are no longer changing value. This is difficult to ascertain, but somewhere around \\(\\lambda=50\\), there doesn‚Äôt seem to be a lot of change in the coefficients. This suggests that a \\(\\lambda\\) value around 50 would produce stable coefficient estimates.\n\nAIC Value\nIt turns out that not only is it difficult to make a subjective call about where the trace lines begin to flatten, but even when people do make this determination, they often select a \\(\\lambda\\) value that is too high. A better method for obtaining \\(\\lambda\\) is to compute a model-level metric that we can then evaluate across the models produced by the different values of \\(\\lambda\\) . One such metric is the Akiake Information Criteria (AIC). We can compute the AIC for a ridge regression as\n\\[\n\\mathrm{AIC} = n \\times \\ln\\big(\\mathbf{e}^{\\intercal}\\mathbf{e}\\big) + 2(\\mathit{df})\n\\]\nwhere \\(n\\) is the sample size, \\(\\mathbf{e}\\) is the vector of residuals from the ridge model, and df is the degrees of freedom associated with the ridge regression model, which we compute by finding the trace of the H matrix, namely,\n\\[\ntr(\\mathbf{H}_{\\mathrm{Ridge}}) =  tr(\\mathbf{X}(\\mathbf{X}^{\\intercal}\\mathbf{X} + \\lambda\\mathbf{I})^{-1}\\mathbf{X}^{\\intercal})\n\\]\nFor example, to compute the AIC value associated with the ridge regression estimated using a \\(\\lambda\\) value of 0.1 we can use the following syntax.\n\n\n# Compute coefficients for ridge model\nb = solve(t(X) %*% X + 0.1*diag(3)) %*% t(X) %*% y\n\n# Compute residual vector\ne = y - (X %*% b)\n\n# Compute H matrix\nH = X %*% solve(t(X) %*% X + 0.1*diag(3)) %*% t(X)\n\n# Compute df\ndf = sum(diag(H))\n\n# Compute AIC\naic = 70 * log(t(e) %*% e) + 2 * df\naic\n\n\n         [,1]\n[1,] 285.8729\n\nWe want to compute the AIC value for every single one of the models associated with the \\(\\lambda\\) values from our sequence we used to produce the ridge trace plot. To do this, we will create a function that will compute the AIC from a given \\(\\lambda\\) value.\n\n\n# Function to compute AIC based on inputted lambda value\nridge_aic = function(lambda){\n  b = solve(t(X) %*% X + lambda*diag(3)) %*% t(X) %*% y\n  e = y - (X %*% b)\n  H = X %*% solve(t(X) %*% X + lambda*diag(3)) %*% t(X)\n  df = sum(diag(H))\n  n = length(y)\n  aic = n * log(t(e) %*% e) + 2 * df\n  return(aic)\n}\n\n# Try function\nridge_aic(lambda = 0.1)\n\n\n         [,1]\n[1,] 285.8729\n\ncycle through each of the \\(\\lambda\\) values, and . We need to store these AIC values, so we initially will create an empty vector to store them in.\nWe now create a data frame that has a column that includes the \\(\\lambda\\) values. Then we use the rowwise() and mutate() functions to apply the ridge_aic() function to each of the lambda values. Finally, we can use filter() to find the \\(\\lambda\\) value associated with the smallest AIC value.\n\n\n# Create data frame with column of lambda values\n# Create a new column by usingthe ridge_aic() function for each row\nmy_models = data.frame(\n  Lambda = seq(from = 0, to = 100, by = 0.01)\n  ) %>%\n  rowwise() %>%\n   mutate(\n    AIC = ridge_aic(Lambda)\n  ) %>%\n  ungroup() #Turn off the rowwise() operation\n\n# Find lambda associated with smallest AIC\nmy_models %>% \n  filter(AIC == min(AIC))\n\n\n# A tibble: 1 √ó 2\n  Lambda AIC[,1]\n   <dbl>   <dbl>\n1   21.8    284.\n\nA \\(\\lambda\\) value of 22.36 produces the smallest AIC value, so this is the \\(\\lambda\\) value we will adopt.\n\n\n# Re-fit ridge regression using lambda = 22.36\nridge_smallest_aic = lm.ridge(achievement ~ -1 + faculty + peer + school, data = z_data, \n                              lambda = 22.36)\n\n# View coefficients\ntidy(ridge_smallest_aic)\n\n\n# A tibble: 3 √ó 5\n  lambda    GCV term    estimate scale\n   <dbl>  <dbl> <chr>      <dbl> <dbl>\n1   22.4 0.0118 faculty   0.115  0.993\n2   22.4 0.0118 peer      0.173  0.993\n3   22.4 0.0118 school    0.0999 0.993\n\nBased on using \\(\\lambda=22.36\\), the fitted ridge regression model is:\n\\[\n\\hat{\\mathrm{Achievement}}^{\\star}_i = 0.115(\\mathrm{Faculty}^{\\star}_i) + 0.173(\\mathrm{Peer}^{\\star}_i) + 0.099(\\mathrm{School}^{\\star}_i)\n\\]\n\nEstimating Bias\nRecall that the ridge regression produces biased estimates which means that:\n\\[\n\\mathbb{E}(\\hat{\\beta})\\neq\\beta\n\\]\nThe amount of bias in the coefficient estimates is defined as:\n\\[\n\\mathrm{Bias}(\\mathbf{b}_{\\mathrm{Ridge}}) = -\\lambda(\\mathbf{X}^{\\intercal}\\mathbf{X}+\\lambda\\mathbf{I})^{-1}\\boldsymbol\\beta\n\\]\nwhere \\(\\boldsymbol\\beta\\) are the population coefficients from the standardized regression model.\nRemember, for the OLS model that \\(\\lambda=0\\). In that case,\n\\[\n\\begin{split}\n\\mathrm{Bias}(\\mathbf{b}_{\\mathrm{OLS}}) &= 0(\\mathbf{X}^{\\intercal}\\mathbf{X}+0\\mathbf{I})^{-1}\\boldsymbol\\beta \\\\[0.5em]\n&= 0\n\\end{split}\n\\]\nThere is no bias in any of the OLS coefficients. Of course, as \\(\\lambda\\) increases, the bias in the coefficient estimates will also increase. We can use our sample data to estimate the bias, it is likely a poor estimate, as to obtain the bias we really need to know the true \\(\\beta\\)-parameters (which of course we do not know).\n\n\n# OLS estimates\nb_ols = solve(t(X) %*% X) %*% t(X) %*% y\n\n# Compute lambda(I)\nlambda_I = 22.36*diag(3)\n\n# Estimate bias in ridge regression coefficients\n-22.36 * solve(t(X) %*% X + lambda_I) %*% b_ols\n\n\n              [,1]\nfaculty -0.4087001\npeer    -0.7717514\nschool   1.1281505\n\nIt looks like the faculty and peer coefficients are biased downward and the school coefficient is biased upward in our ridge regression model. We could also see this in the ridge trace plot we created earlier.\n\n\n# Ridge trace\nggplot(data = ridge_trace, aes(x = lambda, y = estimate)) +\n  geom_line(aes(group = term, color = term)) +\n  geom_vline(xintercept = 22.36, linetype = \"dotted\") +\n  theme_bw() +\n  xlab(\"d value\") +\n  ylab(\"Coefficient estimate\") +\n  ggsci::scale_color_d3(name = \"Predictor\")\n\n\n\n\nFigure 2: Ridge plot showing the size of the standardized regression coefficients for \\(\\lambda\\) values between 0 (OLS) and 100. The vertical dotted line is shown at \\(\\lambda=22.36\\), the value of \\(\\lambda\\) that has the minimum GCV value.\n\n\n\nIn this plot, the estimates for the peer and faculty coefficients are being shrunken downward from the OLS estimates (at \\(\\lambda=0\\)), and the school coefficient is being ‚Äúshrunken‚Äù upward (closer to 0). This implies that the bias is simply the difference between the OLS estimates and the ridge coefficient estimates.\n\n\n# Difference b/w OLS and ridge coefficients\ntidy(ridge_3)$estimate - b_ols\n\n\n              [,1]\nfaculty -0.4120673\npeer    -0.7199641\nschool   1.0999168\n\n\nSampling Variances, Standard Errors, and Confidence Intervals\nRecall that the sampling variation for the coefficients is measured via the variance. Mathematically, the variance estimate of a coefficient is defined as the expectation of the squared difference between the parameter value and the estimate:\n\\[\n\\sigma^2_{b} = \\mathbb{E}\\bigg[(b - \\beta)^2\\bigg]\n\\]\nUsing rules of expectations, we can re-write this as:\n\\[\n\\begin{split}\n\\sigma^2_{b} &= \\mathbb{E}\\bigg[\\bigg(b - \\mathbb{E}\\big[\\beta\\big]\\bigg)^2\\bigg] + \\bigg(\\mathbb{E}\\big[b-\\beta\\big]\\bigg)^2\n\\end{split}\n\\]\nThe first term in this sum represents the variance in b and the second term is the squared amount of bias in b. As a sum,\n\\[\n\\sigma^2_{b} = \\mathrm{Var}(b) + \\mathrm{Bias}(b)^2\n\\]\nIn the OLS model, the bias of all the estimates is 0, and \\(\\sigma^2_{b} = \\mathrm{Var}(b)\\). In ridge regression, the bias term is not zero. The bias‚Äìvariance tradeoff implies that by increasing bias, we will decrease the variance. So while the second term will get bigger, the first will get smaller. The hope is that overall we can reduce the amount of sampling variance in the coefficients. However, since the sampling variance includes the square of the bias, we have to be careful that we don‚Äôt increase bias too much, or it will be counterproductive.\nThe fact that the sampling variance for the coefficients is dependent on both the variance and the amount of bias is a major issue in estimating the sampling variation for a coefficient in ridge regression. This means we need to know how much bias there is to get a true accounting of the sampling variation. As Goeman et al. (2018) notes,\n\nUnfortunately, in most applications of penalized regression it is impossible to obtain a sufficiently precise estimate of the bias‚Ä¶calculations can only give an assessment of the variance of the estimates. Reliable estimates of the bias are only available if reliable unbiased estimates are available, which is typically not the case in situations in which penalized estimates are used.\n\nHe goes on to make it clear why most programs do not report SEs for the coefficients:\n\nReporting a standard error of a penalized estimate therefore tells only part of the story. It can give a mistaken impression of great precision, completely ignoring the inaccuracy caused by the bias. It is certainly a mistake to make confidence statements that are only based on an assessment of the variance of the estimates.\n\n\nEstimating Sampling Variance\nIn theory it is possible to obtain the sampling variances for the ridge regression coefficients using matrix algebra:\n\\[\n\\sigma^2_{\\mathbf{b}} = \\sigma^2_{e}(\\mathbf{X}^\\intercal \\mathbf{X} + \\lambda\\mathbf{I})^{-1}\\mathbf{X}^\\intercal \\mathbf{X} (\\mathbf{X}^\\intercal \\mathbf{X} + \\lambda\\mathbf{I})^{-1}\n\\]\nwhere \\(\\sigma^2_e\\) is the the error variance estimated from the standardized OLS model.\n\n\n# Fit standardized model to obtain sigma^2_e\nglance(lm(achievement ~ -1 + faculty + peer + school, data = z_data))\n\n\n# A tibble: 1 √ó 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC\n      <dbl>         <dbl> <dbl>     <dbl>   <dbl> <dbl>  <dbl> <dbl>\n1     0.206         0.171 0.904      5.80 0.00138     3  -90.7  189.\n# ‚Ä¶ with 4 more variables: BIC <dbl>, deviance <dbl>,\n#   df.residual <int>, nobs <int>\n\n# Compute sigma^2_epsilon\nresid_var = 0.9041214 ^ 2\n\n# Compute variance-covariance matrix of ridge estimates\nW = solve(t(X) %*% X + 22.36*diag(3))\nvar_b = resid_var * W %*% t(X) %*% X %*% W\n\n# Compute SEs\nsqrt(diag(var_b))\n\n\n   faculty       peer     school \n0.05394529 0.05576954 0.04089317 \n\nComparing these SEs to the SEs from the OLS regression:\n\n\nTable 3: Comparison of the standard errors from the OLS (\\(\\lambda=0\\)) and ridge regression (\\(\\lambda=22.36\\)) based on the standardized data.\n\n\nPredictor\n\n\n\\(\\lambda=0\\)\n\n\n\\(\\lambda=22.36\\)\n\n\nFaculty\n\n\n0.667\n\n\n0.054\n\n\nPeer\n\n\n0.598\n\n\n0.056\n\n\nSchool\n\n\n0.993\n\n\n0.041\n\n\nBased on this comparison, we can see that the standard errors from the ridge regression are quite a bit smaller than those from the OLS. We can then use the estimates and SEs to compute t- and p-values, and confidence intervals. Here we only do it for the school facilities predictor (since it is of primary interest based on the RQ) but one could do it for all the predictors.\n\n\n# Compute t-value for school predictor\nt = 0.0998967 / 0.04089317  \nt\n\n\n[1] 2.44287\n\n# Compute df residual\nH = X %*% solve(t(X) %*% X + 22.36*diag(3)) %*% t(X)\ndf_model = sum(diag(H))\ndf_residual = 69 - df_model\n\n# Compute p-value\np = pt(-abs(t), df = df_residual) * 2\np\n\n\n[1] 0.01717516\n\n# Compute CI\n0.0998967 - qt(p = 0.975, df = df_residual) * 0.04089317  \n\n\n[1] 0.01829489\n\n0.0998967 + qt(p = 0.975, df = df_residual) * 0.04089317  \n\n\n[1] 0.1814985\n\nThis suggests that after controlling for peer influence and faculty credential, there is evidence of an effect of school facilities on student achievement (\\(p=.017\\)). The uncertainty in the 95% CI suggests that the true partial effect of school facilities is between 0.018 and 0.181. The empirical evidence is pointing toward a slight positive effect of school facilities.\n\nBias‚ÄìVariance Tradeoff: Revisited\nNow that we have seen how the bias and the sampling variance are calculated, we can study these formulas to understand why there is a bias‚Äìvariance tradeoff.\n\\[\n\\begin{split}\n\\mathrm{Bias}(\\hat{\\boldsymbol{\\beta}}_{\\mathrm{Ridge}}) &= -\\lambda(\\mathbf{X}^{\\intercal}\\mathbf{X}+\\lambda\\mathbf{I})^{-1}\\boldsymbol{\\beta} \\\\[0.5em]\n\\mathrm{Var}(\\hat{\\boldsymbol{\\beta}}_{\\mathrm{Ridge}}) &= \\sigma^2_{\\epsilon}(\\mathbf{X}^\\intercal \\mathbf{X} + \\lambda\\mathbf{I})^{-1}\\mathbf{X}^\\intercal \\mathbf{X} (\\mathbf{X}^\\intercal \\mathbf{X} + \\lambda\\mathbf{I})^{-1}\n\\end{split}\n\\]\nExamining these two formulas, we can see that in the formula for bias, larger \\(\\lambda\\) values are associated with increased bias. Whereas in the formula for sampling variance, increasing \\(\\lambda\\) decreases the amount of sampling variation. This. in a nutshell, is the bias‚Äìvariance tradeoff. Decreasing one of these properties tends to increase the other. The key is to find a value of \\(\\lambda\\) that minimally increases bias to maximally decrease the sampling variances.\n\n\n\n\nChatterjee, S., & Hadi, A. S. (2012). Regression analysis by example. Wiley.\n\n\nColeman, J. S., Cambell, E. Q., Hobson, C. J., McPartland, J., Mood, A. M., Weinfield, F. D., & York, R. L. (1966). Equality of educational opportunity. U.S. Government Printing Office.\n\n\nGoeman, J., Meijer, R., & Chaturvedi, N. (2018). L1 and L2 penalized regression models. R Vignette. https://cran.r-project.org/web/packages/penalized/vignettes/penalized.pdf\n\n\nMosteller, F., & Moynihan, D. F. (1972). On equality of educational opportunity. Random House.\n\n\nAt least within the class of linear, unbiased estimators.‚Ü©Ô∏é\n",
    "preview": "posts/2021-10-20-notes-biased-estimation-ridge-regression/distill-preview.png",
    "last_modified": "2021-11-03T08:22:40-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-05-notes-pca-via-svd/",
    "title": "üìù Principal Components Analysis via Singular Value Decomposition",
    "description": "A brief introduction to principal components analysis via eigendecomposition. Example taken from @Chatterjee:2012.",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-10-05",
    "categories": [
      "Notes"
    ],
    "contents": "\n\n\n\nIn 1964, the US Congress passed the Civil Rights Act and also ordered a survey of school districts to evaluate the availability of equal educational opportunity in public education. The results of this survey were reported on in Coleman et al. (1966) and Mosteller & Moynihan (1972). We will use a subset of the data collected (in equal-educational-opportunity.csv) to mimic one of the original regression analyses performed; examining whether the level of school facilities was an important predictor of student achievement after accounting for the variation in faculty credentials and peer influence.\n[CSV]\n[R Script File]\nThe data come from a random sample of 70 schools in 1965. The variables, which have all been mean-centered and standardized, include:\nachievement: Measurement indicating the student achievement level\nfaculty: Measurement indicating the faculty‚Äôs credentials\npeer: Measurement indicating the influence of peer groups in the school\nschool: Measurement indicating the school facilities (e.g., building, teaching materials)\n\n\n# Load libraries\nlibrary(broom)\nlibrary(corrr)\nlibrary(tidyverse)\nlibrary(tidymodels)\nlibrary(patchwork)\n\n# Read in data\neeo = read_csv(\"~/Documents/github/epsy-8264/data/equal-education-opportunity.csv\")\nhead(eeo)\n\n\n# A tibble: 6 √ó 4\n  achievement faculty    peer school\n        <dbl>   <dbl>   <dbl>  <dbl>\n1      -0.431   0.608  0.0351  0.166\n2       0.800   0.794  0.479   0.534\n3      -0.925  -0.826 -0.620  -0.786\n4      -2.19   -1.25  -1.22   -1.04 \n5      -2.85    0.174 -0.185   0.142\n6      -0.662   0.202  0.128   0.273\n\n# Source the residual_plots() function from the script file\nsource(\"../../scripts/residual_plots.R\")\n\n\n\nThe problem we faced from the last set of notes, was that the predictors in the model were collinear, so we encountered computational issues when trying to estimate the effects and standard errors. One method to deal with collinearity among a set of predictors is to combine the predictors into a smaller subset of orthogonal measures (called principal components) that can be used instead of the original predictors. This subset of measures will not have the collinearity problems (they are orthogonal to one another), but constitute a slightly smaller amount of ‚Äúvariance accounted for‚Äù than the original set of predictors.\n\nSingular Value Decomposition\nAnother decomposition method that creates orthogonal unit vectors (i.e., basis) is singular value decomposition (SVD). This decomposition method decomposes a matrix A into the product of three matrices, namely:\n\\[\n\\mathbf{A} = \\mathbf{U}\\mathbf{D}\\mathbf{V}^{\\intercal}\n\\]\nwhere, U and V are an orthogonal matrices and D is a diagonal matrix.\nPCA using SVD: Matrix Algebra\nTo carry out a principal components analysis, we need to use SVD to decompose the matrix \\(\\mathbf{X}^{\\intercal}_{\\mathrm{Predictor}}\\mathbf{X}_{\\mathrm{Predictor}}\\), where \\(\\mathbf{X}_{\\mathrm{Predictor}}\\) is a matrix of the predictors being used in the PCA. Below, we create the \\(\\mathbf{X}_{\\mathrm{Predictor}}\\) matrix we use the svd() function to decompose the \\(\\mathbf{X}^{\\intercal}_{\\mathrm{Predictor}}\\mathbf{X}_{\\mathrm{Predictor}}\\) matrix using singular value decomposition.\n\n\n# Create matrix of predictors\nX_p = as.matrix(eeo[ , c(\"faculty\", \"peer\", \"school\")])\n\n# SVD decomposition\nsv_decomp = svd(t(X_p) %*% X_p)\n\n# View results\nsv_decomp\n\n\n$d\n[1] 209.3295610   2.7303093   0.5833274\n\n$u\n           [,1]       [,2]       [,3]\n[1,] -0.6174223  0.6698093 -0.4124865\n[2,] -0.5243779 -0.7413256 -0.4188844\n[3,] -0.5863595 -0.0423298  0.8089442\n\n$v\n           [,1]       [,2]       [,3]\n[1,] -0.6174223  0.6698093 -0.4124865\n[2,] -0.5243779 -0.7413256 -0.4188844\n[3,] -0.5863595 -0.0423298  0.8089442\n\n\\[\n\\begin{split}\n\\mathbf{X}^{\\intercal}_{\\mathrm{Predictor}}\\mathbf{X}_{\\mathrm{Predictor}} &= \\mathbf{U}\\mathbf{D}\\mathbf{V}^{\\intercal} \\\\[1em]\n\\begin{bmatrix}81.123 & 66.518 & 75.512 \\\\ 66.518 & 59.163 & 64.251 \\\\ 75.512 & 64.251 & 72.358\\end{bmatrix} &= \\begin{bmatrix}0.617 & 0.670 & -0.412 \\\\ -0.524 & -0.741 & -0.419 \\\\ -0.586 & -0.042 & 0.809\\end{bmatrix} \\begin{bmatrix}209.330 & 0 & 0 \\\\ 0 & 2.730 & 0 \\\\ 0 & 0 & 0.583 \\end{bmatrix} \\begin{bmatrix}-0.617 & -0.524 & -0.586 \\\\ 0.670 & -0.741 & -0.042 \\\\ -0.412 & -0.419 &  0.809 \\end{bmatrix}\n\\end{split}\n\\]\n\nUnderstanding What the Matrix Algebra is Doing\nMathematically, since any matrix can be decomposed using SVD, we can also decompose \\(\\mathbf{X}_{\\mathrm{Predictor}} = \\mathbf{U}\\mathbf{D}\\mathbf{V}^{\\intercal}\\). Then we can write the \\(\\mathbf{X}^{\\intercal}_{\\mathrm{Predictor}}\\mathbf{X}_{\\mathrm{Predictor}}\\) matrix, \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\), as:\n\\[\n\\mathbf{X}^{\\intercal}_{\\mathrm{Predictor}}\\mathbf{X}_{\\mathrm{Predictor}} = (\\mathbf{U}\\mathbf{D}\\mathbf{V}^{\\intercal})^{\\intercal} (\\mathbf{U}\\mathbf{D}\\mathbf{V}^{\\intercal})\n\\]\nRe-expressing this we get:\n\\[\n\\begin{split}\n\\mathbf{X}^{\\intercal}_{\\mathrm{Predictor}}\\mathbf{X}_{\\mathrm{Predictor}} &= \\mathbf{V}\\mathbf{D}^{\\intercal}\\mathbf{U}^{\\intercal}\\mathbf{U}\\mathbf{D}\\mathbf{V}^{\\intercal} \\\\[0.5em]\n\\end{split}\n\\]\nSince D is a diagonal matrix, \\(\\mathbf{D}^{\\intercal}\\mathbf{D} = \\mathbf{D}^2\\), so reducing this expression gives:\n\\[\n\\mathbf{X}^{\\intercal}_{\\mathrm{Predictor}}\\mathbf{X}_{\\mathrm{Predictor}} = \\mathbf{V}\\mathbf{D}^2\\mathbf{V}^{\\intercal}\n\\]\nThe matrices V and \\(\\mathbf{V}^{\\intercal}\\) are both orthogonal basis matrices that ultimately act to change the coordinate system by rotating the original basis vectors used in the predictor space. The \\(\\mathbf{D}^2\\) matrix is diagonalizing the \\(\\mathbf{X}^{\\intercal}_{\\mathrm{Predictor}}\\mathbf{X}_{\\mathrm{Predictor}}\\) matrix which amounts to finding the the major axes in the data ellipse along which our data varies.\n\nUsing the SVD Decomposition as PCA\nAs was pointed out in the previous section, the important matrices from the SVD are the V matrix (rotation matrix) and the D matrix (diagonalization matrix). The V matrix are the principal components:\n\\[\n\\mathbf{V} = \\begin{bmatrix}-0.617 & 0.670 & -0.412 \\\\ -0.524 &  -0.741 & -0.419 \\\\ -0.586 & -0.042  & 0.809 \\end{bmatrix}\n\\]\nNote these are the same principal components we obtained using the eigendecomposition. The values in the D matrix are mathematically related to the eigenvalues. Because we based the SVD on the \\(\\mathbf{X}^{\\intercal}_{\\mathrm{Predictor}}\\mathbf{X}_{\\mathrm{Predictor}}\\) matrix, the diagonal elements on D are the eigenvalues! That is,\n\\[\n\\lambda_i = d_{ii}\n\\]\nThe eigenvalues (which are the variances) can then be used to compute the proportion of variance for each of the principal components.\n\n\n# Compute proportion of variance\nsv_decomp$d / sum(sv_decomp$d)\n\n\n[1] 0.984416916 0.012839862 0.002743222\n\nThe principal component scores can be obtained by postmultiplying the mean centered predictor matrix by the V matrix.\n\n\n# Mean center each predictor\nX_p[, 1] = X_p[, 1] - mean(X_p[, 1])\nX_p[, 2] = X_p[, 2] - mean(X_p[, 2])\nX_p[, 3] = X_p[, 3] - mean(X_p[, 3])\n\n# Compute PC scores\npc_scores = X_p %*% sv_decomp$v\nhead(pc_scores)\n\n\n            [,1]        [,2]        [,3]\n[1,] -0.41777127  0.37690208 -0.11724716\n[2,] -0.98071768  0.15636966 -0.08255266\n[3,]  1.36960233 -0.05831171 -0.02181284\n[4,]  2.09547335  0.10933210  0.19860746\n[5,] -0.02027426  0.25039535  0.13486066\n[6,] -0.27859048  0.03203317  0.09791201\n\nUsing the prcomp() Function to Carry out PCA using SVD\nIn practice, we will use R functions (not the matrix algebra) to carry out a PCA using SVD. The prcomp() function carries out PCA using SVD decomposition. Different elements of the The prcomp() object can then be accessed to obtain output from the PCA.\n\n\n# Fit the PCA using SVD decomposition\nsvd_pca = eeo %>%\n  select(faculty, peer, school) %>%\n  scale(center = TRUE, scale = FALSE) %>% # center data\n  prcomp()\n\n# View standard deviations and rotation matrix (eigenvector matrix)\nsvd_pca \n\n\nStandard deviations (1, .., p=3):\n[1] 1.7401965 0.1989041 0.0908621\n\nRotation (n x k) = (3 x 3):\n               PC1         PC2        PC3\nfaculty -0.6173237  0.67025631 -0.4119077\npeer    -0.5241853 -0.74086406 -0.4199407\nschool  -0.5866355 -0.04332342  0.8086915\n\nAgain, to compute the variances we can square the standard deviations output by the function and then use those variances to compute the variance accounted for by the principal components.\n\n\n# Compute variances\nvar_pc = svd_pca[[1]] ^ 2\nvar_pc\n\n\n[1] 3.028283805 0.039562836 0.008255921\n\n# Compute variance accounted for\nvar_pc / sum(var_pc)\n\n\n[1] 0.98445476 0.01286135 0.00268389\n\nWe can also obtain the scores on each principal component for each observation in the sample using the augment() function from the {broom} package.\n\n\n# Obtain PC scores\naugment(svd_pca)\n\n\n# A tibble: 70 √ó 4\n   .rownames .fittedPC1 .fittedPC2 .fittedPC3\n   <chr>          <dbl>      <dbl>      <dbl>\n 1 1            -0.418      0.377    -0.117  \n 2 2            -0.981      0.156    -0.0827 \n 3 3             1.37      -0.0582   -0.0214 \n 4 4             2.10       0.109     0.199  \n 5 5            -0.0203     0.250     0.135  \n 6 6            -0.279      0.0319    0.0979 \n 7 7            -0.0577     0.229    -0.00757\n 8 8            -0.712      0.217     0.0974 \n 9 9             1.08      -0.0198   -0.0380 \n10 10           -1.41       0.167     0.0983 \n# ‚Ä¶ with 60 more rows\n\n\nIn general it is more efficient to use singular value decomposition than eigendecomposition when carrying out a PCA. As such the use of prcomp() rather than princomp() is recommended in practice.\n\nScaling the Predictors\nIn practice, it is important to scale all the predictors used in the PCA. This is especially true when the variables are measured in different metrics or have varying degrees of magnitude. In these cases, not scaling the predictors will often result in results in which variables with large magnitudes of scale dominate the PCA. It also is helpful when the predictors are measured using qualitatively different scales (e.g., one is measured in dollars and another in years of education).\n\nRecall, the \\(\\mathbf{X}^{\\intercal}_{\\mathrm{Predictor}}\\mathbf{X}_{\\mathrm{Predictor}}\\) matrix for a set of standardized predictors his the correlation matrix of the predictors. Thus, the SVD is actually being carried out on the correlation matrix rather than the raw \\(\\mathbf{X}^{\\intercal}_{\\mathrm{Predictor}}\\mathbf{X}_{\\mathrm{Predictor}}\\) matrix.\nWhile it isn‚Äôt necessary to also center the predictors, it is common to simply standardize the set of predictors being used in the PCA (i.e., convert them to z-scores); thus both centering and scaling them. To do this, we will pipe the selected predictors into the scale() function prior to piping into the prcomp() function.\n\n\n# Fit the PCA using SVD decomposition on standardized predictors\nsvd_pca_z = eeo %>%\n  select(faculty, peer, school) %>%\n  scale(center = TRUE, scale = TRUE) %>%\n  prcomp()\n\n# View standard deviations and rotation matrix (eigenvector matrix)\nsvd_pca_z\n\n\nStandard deviations (1, .., p=3):\n[1] 1.71813654 0.20011873 0.08921511\n\nRotation (n x k) = (3 x 3):\n               PC1         PC2        PC3\nfaculty -0.5761385  0.67939712 -0.4544052\npeer    -0.5754361 -0.73197527 -0.3648089\nschool  -0.5804634  0.05130072  0.8126687\n\n# tidy version of the rotation matrix (good for graphing)\nsvd_pca_z %>%\n  tidy(matrix = \"rotation\")\n\n\n# A tibble: 9 √ó 3\n  column     PC   value\n  <chr>   <dbl>   <dbl>\n1 faculty     1 -0.576 \n2 faculty     2  0.679 \n3 faculty     3 -0.454 \n4 peer        1 -0.575 \n5 peer        2 -0.732 \n6 peer        3 -0.365 \n7 school      1 -0.580 \n8 school      2  0.0513\n9 school      3  0.813 \n\n# View sds, variance accounted for\nsvd_pca_z %>%\n  tidy(matrix = \"eigenvalues\")\n\n\n# A tibble: 3 √ó 4\n     PC std.dev percent cumulative\n  <dbl>   <dbl>   <dbl>      <dbl>\n1     1  1.72   0.984        0.984\n2     2  0.200  0.0134       0.997\n3     3  0.0892 0.00265      1    \n\n# Obtain PC scores\npc_scores = augment(svd_pca_z)\npc_scores\n\n\n# A tibble: 70 √ó 4\n   .rownames .fittedPC1 .fittedPC2 .fittedPC3\n   <chr>          <dbl>      <dbl>      <dbl>\n 1 1            -0.366      0.366     -0.123 \n 2 2            -0.950      0.149     -0.0847\n 3 3             1.34      -0.0633    -0.0197\n 4 4             2.09       0.129      0.193 \n 5 5             0.0152     0.267      0.127 \n 6 6            -0.269      0.0437     0.0952\n 7 7            -0.0275     0.230     -0.0128\n 8 8            -0.672      0.231      0.0905\n 9 9             1.06      -0.0261    -0.0369\n10 10           -1.37       0.182      0.0925\n# ‚Ä¶ with 60 more rows\n\nHere the results from using the standardized predictors are not that different from the previous results since the variables were already reported as z-scores to begin with. The variance accounted for by the principal components is comparable (within rounding); the standardization of the predictors does not change this. The actual principal components in the V (rotation) matrix are different because of the centering and scaling, but the interpretations are the same as when we used the decomposition based on the \\(\\mathbf{X}^{\\intercal}_{\\mathrm{Predictor}}\\mathbf{X}_{\\mathrm{Predictor}}\\) matrix. Similarly, because of the centering and scaling, the PC scores are different.\n\nBehind the Scenes\nBy standardizing the predictors, we are carrying out the SVD on the correlation matrix of the predictors rather than on the \\(\\mathbf{X}^{\\intercal}_{\\mathrm{Predictor}}\\mathbf{X}_{\\mathrm{Predictor}}\\) matrix. To see this, recall that the correlation matrix of the predictors (\\(\\mathbf{R_X}\\)) is based on the standardized predictors, namely,\n\nThis implies that you can also carry out PCA on summaries of the predictors (rather than the raw data) by decomposing either the covariance matrix of the predictors or the correlation matrix of the predictors. This can be useful for example, when trying to reproduce the results of a PCA from a published paper, in which authors will often report summaries of the data used (e.g., correlation matrices) but not the raw data.\n\\[\n\\mathbf{R_X} = \\frac{1}{n-1} \\big(\\mathbf{X}^{\\intercal}_{\\mathrm{Standardized~Predictor}}\\mathbf{X}_{\\mathrm{Standardized~Predictor}}\\big)\n\\]\nThe \\(1/(n-1)\\) component is a scalar and is pulled out so that the decomposition is carried out on the \\(\\mathbf{X}^{\\intercal}_{\\mathrm{Standardized~Predictor}}\\mathbf{X}_{\\mathrm{Standardized~Predictor}}\\) matrix:\n\\[\n\\frac{1}{n-1} \\big(\\mathbf{X}^{\\intercal}_{\\mathrm{Standardized~Predictor}}\\mathbf{X}_{\\mathrm{Standardized~Predictor}}\\big) = \\frac{1}{n-1}\\mathbf{U}\\mathbf{D}\\mathbf{V}^{\\intercal}\n\\]\nSimilarly, we can also decompose the covariance matrix of the predictors (\\(\\boldsymbol\\Sigma_{\\mathbf{X}}\\)), which is based on the centered predictors,\n\\[\n\\boldsymbol\\Sigma_{\\mathbf{X}} = \\frac{1}{n-1} \\big( \\mathbf{X}^{\\intercal}_{\\mathrm{Centered~Predictor}}\\mathbf{X}_{\\mathrm{Centered~Predictor}}\\big)\n\\]\nIn this case, the decomposition is carried out on the \\(\\mathbf{X}^{\\intercal}_{\\mathrm{Centered~Predictor}}\\mathbf{X}_{\\mathrm{Centered~Predictor}}\\) matrix. To do this using prcomp() we would change the scale= argument to FALSE in the scale() function, while still leaving center=TRUE.\n\nUsing the Principal Components in a Regression Model\nRemember, we undertook the PCA because of the collinearity in the original predictors. Rather than using the original predictor values in our regression, we can use the scores from the PCA. Since we created the principal components to be orthogonal, this should alleviate any collinearity problems.\n\n\n# Add the PC scores to the original data\neeo2 = eeo %>%\n  bind_cols(pc_scores)\n\n# View data\neeo2\n\n\n# A tibble: 70 √ó 8\n   achievement faculty    peer  school .rownames .fittedPC1 .fittedPC2\n         <dbl>   <dbl>   <dbl>   <dbl> <chr>          <dbl>      <dbl>\n 1      -0.431   0.608  0.0351  0.166  1            -0.366      0.366 \n 2       0.800   0.794  0.479   0.534  2            -0.950      0.149 \n 3      -0.925  -0.826 -0.620  -0.786  3             1.34      -0.0633\n 4      -2.19   -1.25  -1.22   -1.04   4             2.09       0.129 \n 5      -2.85    0.174 -0.185   0.142  5             0.0152     0.267 \n 6      -0.662   0.202  0.128   0.273  6            -0.269      0.0437\n 7       2.64    0.242 -0.0902  0.0497 7            -0.0275     0.230 \n 8       2.36    0.594  0.218   0.519  8            -0.672      0.231 \n 9      -0.913  -0.616 -0.490  -0.632  9             1.06      -0.0261\n10       0.594   0.994  0.622   0.934  10           -1.37       0.182 \n# ‚Ä¶ with 60 more rows, and 1 more variable: .fittedPC3 <dbl>\n\n# Fit model using PC scores\nlm.pc = lm(achievement ~ 1 + .fittedPC1 + .fittedPC2 + .fittedPC3, data = eeo2)\n\n# Check for collinearity -- correlations\neeo2 %>%\n  select(starts_with(\".fitted\")) %>%\n  correlate()\n\n\n# A tibble: 3 √ó 4\n  term       .fittedPC1 .fittedPC2 .fittedPC3\n  <chr>           <dbl>      <dbl>      <dbl>\n1 .fittedPC1  NA         -4.02e-15   4.20e-16\n2 .fittedPC2  -4.02e-15  NA          6.68e-16\n3 .fittedPC3   4.20e-16   6.68e-16  NA       \n\n# Check for collinearity -- VIF\ncar::vif(lm.pc)\n\n\n.fittedPC1 .fittedPC2 .fittedPC3 \n         1          1          1 \n\nExamining some of the collinearity diagnostics we see that the predictors in this model are completely uncorrelated and the VIF values are 1; indicating that the SEs for these coefficients are exactly as large as they would be if the predictors were independent (which they are). Looking at the model- and coefficient-level output:\n\n\n# Model-level output\nglance(lm.pc)\n\n\n# A tibble: 1 √ó 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC\n      <dbl>         <dbl> <dbl>     <dbl>   <dbl> <dbl>  <dbl> <dbl>\n1     0.206         0.170  2.07      5.72 0.00153     3  -148.  306.\n# ‚Ä¶ with 4 more variables: BIC <dbl>, deviance <dbl>,\n#   df.residual <int>, nobs <int>\n\n# Coefficient-level output\ntidy(lm.pc, conf.int = 0.95)\n\n\n# A tibble: 4 √ó 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)   0.0192     0.247    0.0776 0.938      -0.475     0.513\n2 .fittedPC1   -0.568      0.145   -3.91   0.000217   -0.857    -0.278\n3 .fittedPC2   -0.881      1.25    -0.708  0.482      -3.37      1.61 \n4 .fittedPC3   -3.22       2.79    -1.15   0.253      -8.80      2.35 \n\nThe model-level output from this model is exactly the same as the model-level output from the model fitted with the original predictors. The coefficient-level output is where we start to see differences. Because there is no collinearity in this model, we now see a statistically significant result at the coefficient level (PC1). The other thing to remember here is that each principal component is a composite variable composed of all three predictors and perhaps had a more substantive interpretation. We need to use those substantive interpretations in the interpretations of coefficients:\nThe intercept is the predicted average achievement for cases where all the composite variables are 0.\nThe positive slope associated with the first composite indicates that higher values on this composite are associated with higher achievement, on average. In other words, higher values on all three predictors are associated with higher achievement, on average.\nThe negative slope associated with the second composite indicates that higher values on this composite are associated with lower achievement, on average. In other words, larger contrasts between faculty credentials and peer influence are associated with lower achievement, on average.\nThe positive slope associated with the third composite indicates that higher values on this composite are associated with higher achievement, on average. In other words, larger contrasts between school facilities and faculty credentials/peer influence are associated with higher achievement, on average.\nAgain, these interpretations may not be satisfactory‚Äîit all depends on whether the loadings offer a reasonable interpretation of the composite variable.\n\nDimension Reduction\nOne of the most useful qualities of a PCA, aside from fixing collinearity issues, can be to reduce the size of the predictor space in a regression model; thereby reducing the complexity of the model and improving statistical power. To do this, we consider the variance accounted for by each of the principal components.\nIn our example, the first principal component accounted for most of the variance in the original predictor space (approximately 98%). The other two principal components did not account for that much variation, which suggests that they may not be necessary. We can capture most of the variation in the original three predictors by just using the first principal component.\n\n\n# Fit reduced model using PC1\nlm.pc.2 = lm(achievement ~ 1 + .fittedPC1, data = eeo2)\n\n# Model-level output\nglance(lm.pc.2)\n\n\n# A tibble: 1 √ó 12\n  r.squared adj.r.squared sigma statistic  p.value    df logLik   AIC\n      <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>  <dbl> <dbl>\n1     0.184         0.172  2.07      15.4 0.000209     1  -149.  304.\n# ‚Ä¶ with 4 more variables: BIC <dbl>, deviance <dbl>,\n#   df.residual <int>, nobs <int>\n\n# Coefficient-level output\ntidy(lm.pc.2, conf.int = 0.95)\n\n\n# A tibble: 2 √ó 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)   0.0192     0.247    0.0777 0.938      -0.474     0.512\n2 .fittedPC1   -0.568      0.145   -3.92   0.000209   -0.857    -0.279\n\nIn this model, the model \\(R^2\\) value is slightly smaller (0.183 rather than 0.206) since the first principal component did not account for all of the variance in the original predictors. However, this difference is negligible, and the model is still explains a statistically relevant amount of variation in achievement scores, \\(F(1,68)=15.25\\), \\(p=0.0002\\).\nFrom the coefficient-level output, we see that the magnitude of the intercept and slope are comparable to those in the model where all three composites were used. The standard error and p-value for the composite in this model are slightly smaller than when we used all of the predictors. This represents the additional two degrees of freedom in the error term that we got from reducing the number of predictors. (In this example, the differences are negligible.)\n\n\n\n\nChatterjee, S., & Hadi, A. S. (2012). Regression analysis by example. Wiley.\n\n\nColeman, J. S., Cambell, E. Q., Hobson, C. J., McPartland, J., Mood, A. M., Weinfield, F. D., & York, R. L. (1966). Equality of educational opportunity. U.S. Government Printing Office.\n\n\nMosteller, F., & Moynihan, D. F. (1972). On equality of educational opportunity. Random House.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-10-11T10:08:56-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-04-reading-diagnosing-collinearity/",
    "title": "üìñ Diagnosing Collinearity",
    "description": "Reading",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-10-04",
    "categories": [
      "Reading"
    ],
    "contents": "\nRead the following:\nRodriguez, M., & Zieffler, A. (2021). Eigenvalues and Eigenvectors. In Matrix algebra for educational scientists.\n\nAdditional Resources\nCook, D. (2019). How to use a tour to check if your model suffers from multicollinearity. Personal blog.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-10-04T12:57:42-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-04-reading-pca-via-spectral-decomposition/",
    "title": "üìñ Principal Components Analysis via Spectral Decomposition",
    "description": "Reading",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-10-04",
    "categories": [
      "Reading"
    ],
    "contents": "\nRead the following:\nRodriguez, M., & Zieffler, A. (2021). Basis vectors and matrices. In Matrix algebra for educational scientists.\nRodriguez, M., & Zieffler, A. (2021). Eigenvalues and eigenvectors. In Matrix algebra for educational scientists.\nRodriguez, M., & Zieffler, A. (2021). Spectral decomposition. In Matrix algebra for educational scientists.\n\nAdditional Resources\nWilke, C. O. (2020). PCA tidyverse style. Personal blog.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-10-11T10:09:24-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-11-reading-pca-via-svd/",
    "title": "üìñ Principal Components Analysis via Singular Value Decomposition",
    "description": "Reading",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-10-04",
    "categories": [
      "Reading"
    ],
    "contents": "\nRead the following:\nRodriguez, M., & Zieffler, A. (2021). Singular value decomposition. In Matrix algebra for educational scientists.\n\nAdditional Resources\nGundersen, G. (2018). Singular value decomposition as simply as possible.\nWang, Z. (2019). PCA and SVD explained with numpy.\nWikipdeia. (2020). Singular value decomposition.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-10-11T09:49:21-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-30-notes-diagnosing-collinearity/",
    "title": "üìù Diagnosing Collinearity",
    "description": "A brief introduction to empirical diagnostics to detect collinearity. Example taken from @Chatterjee:2012.",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-09-30",
    "categories": [
      "Notes"
    ],
    "contents": "\n\n\n\nIn 1964, the US Congress passed the Civil Rights Act and also ordered a survey of school districts to evaluate the availability of equal educational opportunity in public education. The results of this survey were reported on in Coleman et al. (1966) and Mosteller & Moynihan (1972). We will use a subset of the data collected (in equal-educational-opportunity.csv) to mimic one of the original regression analyses performed; examining whether the level of school facilities was an important predictor of student achievement after accounting for the variation in faculty credentials and peer influence.\n[CSV]\n[R Script File]\nThe data come from a random sample of 70 schools in 1965. The variables, which have all been mean-centered and standardized, include:\nachievement: Measurement indicating the student achievement level\nfaculty: Measurement indicating the faculty‚Äôs credentials\npeer: Measurement indicating the influence of peer groups in the school\nschool: Measurement indicating the school facilities (e.g., building, teaching materials)\n\n\n# Load libraries\nlibrary(broom)\nlibrary(car)\nlibrary(corrr)\nlibrary(tidyverse)\nlibrary(patchwork)\n\n# Read in data\neeo = read_csv(\"~/Documents/github/epsy-8264/data/equal-education-opportunity.csv\")\nhead(eeo)\n\n\n# A tibble: 6 √ó 4\n  achievement faculty    peer school\n        <dbl>   <dbl>   <dbl>  <dbl>\n1      -0.431   0.608  0.0351  0.166\n2       0.800   0.794  0.479   0.534\n3      -0.925  -0.826 -0.620  -0.786\n4      -2.19   -1.25  -1.22   -1.04 \n5      -2.85    0.174 -0.185   0.142\n6      -0.662   0.202  0.128   0.273\n\n# Source the residual_plots() function from the script file\nsource(\"../../scripts/residual_plots.R\")\n\n\n\n\nRegression Analysis\nTo examine the RQ, the following model was posited:\n\\[\n\\mathrm{Achievement}_i = \\beta_0 + \\beta_1(\\mathrm{Faculty}_i) + \\beta_2(\\mathrm{Peer}_i) + \\beta_3(\\mathrm{School}_i) + \\epsilon_i\n\\]\n\n\n\nFigure 1: Residual plots for the model that includes the main effects of faculty credentials, influence of peer groups, and measure of school facilities to predict variation in student achievement.\n\n\n\n\n\n# Index plots of several regression diagnostics\ninfluenceIndexPlot(lm.1)\n\n\n\n\nFigure 2: Diagnostic plots for the model that includes the main effects of faculty credentials, influence of peer groups, and measure of school facilities to predict variation in student achievement.\n\n\n\nSchool 28 may be problematic, but removing this observation (work not shown) made little improvement in the residual plots. As such, School 28 was retained in the data. As the assumptions seem reasonably met, we next look to the model-level and coefficient-level output:\n\n\n# Model-level information\nprint(glance(lm.1), width = Inf)\n\n\n# A tibble: 1 √ó 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC\n      <dbl>         <dbl> <dbl>     <dbl>   <dbl> <dbl>  <dbl> <dbl>\n1     0.206         0.170  2.07      5.72 0.00153     3  -148.  306.\n    BIC deviance df.residual  nobs\n  <dbl>    <dbl>       <int> <int>\n1  318.     283.          66    70\n\n# Coefficient-level information\ntidy(lm.1, conf.int = 0.95)\n\n\n# A tibble: 4 √ó 7\n  term        estimate std.error statistic p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>    <dbl>     <dbl>\n1 (Intercept)  -0.0700     0.251    -0.279   0.781   -0.570     0.430\n2 faculty       1.10       1.41      0.781   0.438   -1.72      3.92 \n3 peer          2.32       1.48      1.57    0.122   -0.635     5.28 \n4 school       -2.28       2.22     -1.03    0.308   -6.71      2.15 \n\nExamining this information we find:\n20% of the variation in student achievement is explained by the model; which is statistically significant \\(F(3, 66)=5.72\\); \\(p=0.002\\).\nHowever, none of the individual coefficients are statistically significant!\nThis is a bit of a paradox since we rejected the model-level null hypothesis that \\(H_0:\\beta_{\\mathrm{Faculty~Credentials}}=\\beta_{\\mathrm{Peer~Influence}}=\\beta_{\\mathrm{School~Facilities}}=0\\), yet the coefficient-level results are consistent with \\(H_0:\\beta_{\\mathrm{Faculty~Credentials}}=0\\), \\(H_0:\\beta_{\\mathrm{Peer~Influence}}=0\\), and \\(H_0:\\beta_{\\mathrm{School~Facilities}}=0\\). These inconsistencies between the model- and coefficient-level results are typical when there is collinearity in the model.\n\nWhat is Collinearity?\nRecall from our introduction to matrix algebra that two vectors are collinear if they span the same subspace. In regression, collinearity occurs when any of the columns of the design matrix, X, is a perfect linear combination of the other columns:\n\\[\n\\mathbf{X_j} = c_0(\\mathbf{1}) + c_1\\mathbf{X_1} + c_2\\mathbf{X_2} + c_3\\mathbf{X_3} + \\ldots + c_k\\mathbf{X_k}\n\\]\nand the constants, \\(c_1, c_2, c_3,\\ldots, c_k\\) are not all 0. In this situation, X is not of full column rank, and the \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) matrix is singular.\n\nSometimes ‚Äòcollinearity‚Äô is referred to as ‚Äòmulticollinearity‚Äô. The two terms are synonomous.\nEffects of Collinearity\nIf the design matrix is not of full rank, and \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) is singular, then the OLS normal equations do not have a unique solution. Moreover, the sampling variances for the coefficient are all infinitely large. To understand why this is the case, we can examine one formula for the sampling variance of a slope in a multiple regression:\n\\[\n\\mathrm{Var}(B_j) = \\frac{1}{1 - R^2_j} \\times \\frac{\\sigma^2_\\epsilon}{(n-1)S^2_j}\n\\]\nwhere\n\\(R^2_j\\) is the squared multiple correlation for the regression of \\(X_j\\) on the the other predictors;\n\\(S^2_j\\) is the sample variance of predictor \\(X_j\\) defined by \\(S^2_j = \\dfrac{\\sum(X_{ij}-\\bar{X}_j)^2}{n-1}\\);\n\\(\\sigma^2_{\\epsilon}\\) is the variance of the residuals based on regressing \\(Y\\) on all the \\(X\\)‚Äôs\n\nRecall that the multiple correlation is the correlation between the outcome and the predicted values.\nThe first term in this product is referred to as the variance inflation factor (VIF). When one of the predictors is perfectly collinear with the others, the value of \\(R^2_j\\) is 1 and the VIF is infinity. Thus the sampling variance of \\(B_j=\\infty\\).\n\nPerfect Collinearity in Practice: Model Mis-specification\nIn practice, it is unlikely that you will have exact collinearity. When it does happen it is often the result of mis-formulating the model (e.g., including dummy variables in the model for all levels of a categorical variable, as well as the intercept). As an example of this, imagine that you were creating the design matrix for a regression model that included occupational status (employed/not employed) to predict some outcome for 5 cases.\n\n\n# Create design matrix\nX = data.frame(\n  b_0 = rep(1, 5),\n  employed = c(1, 1, 0, 0, 1),\n  not_employed = c(0, 0, 1, 1, 0)\n)\n\n# View design matrix\nX\n\n\n  b_0 employed not_employed\n1   1        1            0\n2   1        1            0\n3   1        0            1\n4   1        0            1\n5   1        1            0\n\nThe columns in this design matrix are collinear because we can express any one of the columns as a linear combination of the others. For example,\n\\[\nb_0 = 1(\\mathrm{employed}) + 1(\\mathrm{not~employed})\n\\]\nChecking the rank of this matrix, we find that this matrix has a rank of 2. Since there are three columns, X is not full column rank; it is rank deficient.\n\n\nMatrix::rankMatrix(X)\n\n\n[1] 2\nattr(,\"method\")\n[1] \"tolNorm2\"\nattr(,\"useGrad\")\n[1] FALSE\nattr(,\"tol\")\n[1] 1.110223e-15\n\nIncluding all three coefficients in the model results in overparameterization. The simple solution here is to drop one of the predictors from the model. This is why we only include a single dummy variable in a model that includes an intercept for a dichotomous categorical predictor.\n\nIncluding the intercept is not imperative, although it has a useful interpretation when using dummy coding. One could also include the two dummy-coded predictors and omit the intercept. This gives the means for the two groups, but does not provide a comparison of those means.\n\n\n# Create vector of outcomes\nY = c(15, 15, 10, 15, 30)\n\n# Create data frame of Y and X\nmy_data = cbind(Y, X)\nmy_data\n\n\n   Y b_0 employed not_employed\n1 15   1        1            0\n2 15   1        1            0\n3 10   1        0            1\n4 15   1        0            1\n5 30   1        1            0\n\n# Coefficients (including all three terms)\ncoef(lm(Y ~ 1 + employed + not_employed, data = my_data))\n\n\n (Intercept)     employed not_employed \n        12.5          7.5           NA \n\n# Coefficients (omitting intercept)\ncoef(lm(Y ~ -1 + employed + not_employed, data = my_data))\n\n\n    employed not_employed \n        20.0         12.5 \n\nIf you overparameterize a model with lm(), one or more of the coefficients will not be estimated (the last parameters entered in the model).\n\nConstraining some parameters is another way to produce a full rank design matrix. For example the ANOVA model has a constraint that the sum of the effect-coded variable is 0. This constraint ensures that the design matrix will be of full rank.\n\nNon-Exact Collinearity\nIt is more likely, in practice, that you will have less-than-perfect collinearity, and that this will have an adverse effect on the computational estimates of the coefficients‚Äô sampling variances. Again, we look toward how the sampling variances for the coefficent‚Äôs are computed:\n\\[\n\\mathrm{Var}(B_j) = \\frac{1}{1 - R^2_j} \\times \\frac{\\sigma^2_\\epsilon}{(n-1)S^2_j}\n\\]\nWhen the predictors are completely independent, all of the columns of the design matrix will be orthogonal and the correlation between \\(X_j\\) and the other \\(X\\)s will be 0. In this situation, the VIF is 1 and the second term in the product completely defines the sampling variance. This means that the sampling variance is a function of the model‚Äôs residual variance, sample size, and the predictor‚Äôs variance‚Äîthe factors we typically think of affecting the sampling variance of a coefficient.\nIn cases where the columns in ths design matrix are not perfectly orthogonal, the correlation between \\(X_j\\) and the other \\(X\\)s is larger than 0. (Perfect collinearity results in \\(R^2_j=1\\).) For these situations, the VIF has a value that is greater than 1. When this happens the VIF acts as a multiplier of the second term, inflating the the sampling variance and reducing the precision of the estimate (i.e., increasing the uncertainty).\nHow much the uncertainty in the estimate increases is a function of how correlated the predictors are. Here we can look at various multiple correlations (\\(R_j\\)) between \\(X_j\\) and the predicted values from using the other \\(X\\)‚Äôs to predict \\(X_j\\).\n\n\nTable 1: Impact of various \\(R_j\\) values on the VIF and size of the CI for \\(B_j\\).\n\n\n\\(R_j\\)\n\n\nVIF\n\n\nCI Factor\n\n\n0.0\n\n\n1.00\n\n\n1.00\n\n\n0.1\n\n\n1.01\n\n\n1.01\n\n\n0.2\n\n\n1.04\n\n\n1.02\n\n\n0.3\n\n\n1.10\n\n\n1.05\n\n\n0.4\n\n\n1.19\n\n\n1.09\n\n\n0.5\n\n\n1.33\n\n\n1.15\n\n\n0.6\n\n\n1.56\n\n\n1.25\n\n\n0.7\n\n\n1.96\n\n\n1.40\n\n\n0.8\n\n\n2.78\n\n\n1.67\n\n\n0.9\n\n\n5.26\n\n\n2.29\n\n\n1.0\n\n\nInf\n\n\nInf\n\n\nFor example, a multiple correlation of 0.7 results in a VIF of 1.96, which in turn means that the CI (which is based on the square root of the sampling variance) will increase by a factor of 1.4. This inflation increases the uncertainty of the estimate making it harder to make decisions or understand the effect of \\(B_j\\).\nTo sum things up, while perfect collinearity is rare in practice, less-than-perfect collinearity is common. In these cases the VIF will be less than 1, but can still have an adverse effect on the sampling variances; sometimes making them quite large.\n\nIdentifying Collinearity\nIn our case study example, we were alerted to the possible collinearity by finding that the predictors jointly were statistically significant, but that each of the individual predictors were not. Other signs that you may have collinearity problems are:\nLarge changes in the size of the estimated coefficients when variables are added to the model;\nLarge changes in the size of the estimated coefficients when an observation is added or deleted;\nThe signs of the estimated coefficients do not conform to their prior substantively hypothesized directions;\nLarge SEs on variables that are expected to be important predictors.\n\nCollinearity Diagnostics\nWe can also empirically diagnose problematic collinearity in the data (D. A. Belsley, 1991; D. Belsley et al., 1980). Before we do, however, it is important that the functional form of the model has been correctly specified. Since, a model needs to be specified before we can estimate coefficients or their sampling variances, and collinearity produces unstable estimates of these estimates, collinearity should only be investigated after the model has been satisfactorily specified.\nBelow we will explore some of the diagnostic tools available to an applied researcher.\n\nHigh Correlations among Predictors\nCollinearity can sometimes be anticipated by examining the pairwise correlations between the predictors. If the correlation between predictors is large, this might be indicative of collinearity problems.\n\n\neeo %>%\n  select(faculty, peer, school) %>%\n  correlate()\n\n\n# A tibble: 3 √ó 4\n  term    faculty   peer school\n  <chr>     <dbl>  <dbl>  <dbl>\n1 faculty  NA      0.960  0.986\n2 peer      0.960 NA      0.982\n3 school    0.986  0.982 NA    \n\nIn this example, all three of the predictors are highly correlated with one another. This is likely a good indicator that their may be problems in the estimation of coefficients, inflated standard errors, or both; especially given that the correlations are all very high. Unfortunately the source of collinearity may be due to more than just the simple relationships among the predictors. As such, just examining the pairwise correlations is not enough to detect collinearity (although it is a good first step).\n\nRegress each Predictor on the Other Predictors\nSince collinearity is defined as linear dependence within the set of predictors, a better way to diagnose collinearity than just examining the pairwise correlation coefficients is to regress each of the predictors on the remaining predictors and evaluate the \\(R^2\\) value. If all the \\(R^2\\) values are close to zero there is no collinearity problems. If one or more of the \\(R^2\\) values are close to 1, there is a collinearity problem.\n\n\n# Use faculty as outcome; obtain R2\nsummary(lm(faculty ~ 1 + peer + school, data = eeo))$r.squared\n\n\n[1] 0.9733906\n\n# Use faculty as outcome; obtain R2\nsummary(lm(peer ~ 1 + faculty + school, data = eeo))$r.squared\n\n\n[1] 0.9669002\n\n# Use faculty as outcome; obtain R2\nsummary(lm(school ~ 1 + faculty + peer, data = eeo))$r.squared\n\n\n[1] 0.9879743\n\nAll three \\(R^2\\) values are quite high, which is indicative of collinearity.\nOne shortcoming with this method of diagnosing collinearity is that when the predictor space is large, you would need to look at the \\(R^2\\) values from several models. And, while this could be automated in an R function, there are other common methods that allow us to diagnose collinearity.\nWe will examine three additional common methods statisticians use to empirically detect collinearity: (1) computing variance inflation factors for the coefficients; (2) examining the eigenvalues of the correlation matrix; and (3) examining the condition indices of the correlation matrix.\nVariance Inflation Factor (VIF)\nPerhaps the most common method applied statisticians use to diagnose collinaerity is to compute and examine variance inflation factors. Recall that the variance inflation factor (VIF) is an indicator of the degree of collinearity, where VIF is:\n\\[\n\\mathrm{VIF} = \\frac{1}{1 - R^2_j}\n\\]\nThe VIF impacts the size of the variance estimates for the regression coefficients, and as such, can be used as a diagnostic of collinearity. In practice, since it is more conventional to use the SE to measure uncertainty, it is typical to use the square root of the VIF as a diagnostic of collinearity in practice. The square root of the VIF expresses the proportional change in the CI for the coefficients. We can use the vif() function from the car package to compute the variance inflation factors for each coefficient.\n\n\n# VIF\nvif(lm.1)\n\n\n faculty     peer   school \n37.58064 30.21166 83.15544 \n\n# Square root of VIF\nsqrt(vif(lm.1))\n\n\n faculty     peer   school \n6.130305 5.496513 9.118960 \n\nAll three coefficients are impacted by VIF. The SEs for these coefficients are all more than five times as large as they would be if the predictors were independent.\nRemember, the VIF can range from 1 (independence among the predictors) to infinity (perfect collinearity). There is not consensus among statisticians about how high the VIF has to be to constitute a problem. Some references cite \\(\\mathrm{VIF}>10\\) as problematic (which increases the size of the CI for the coefficient by a factor of over three); while others cite \\(\\mathrm{VIF}>4\\) as problematic (which increases the size of the CI for the coefficient by a factor of two). As you consider what VIF value to use as an indicator of problematic inflation, it is more important to consider what introducing that much uncertainty would mean in your substantive problem. For example, would you be comfortable with tripling the uncertainty associated with the coefficient? What about doubling it? Once you make that decision, you can determine your VIF cutoff.\nThere are several situations in which high VIF values are expected and not problematic:\nThe variables with high VIFs are control variables, and the variables of interest do not have high VIFs. Since we would not be interested in inference around the control variables, high VIF values on those variables would not\nThe high VIFs are caused by the inclusion of powers or products of other variables. The p-value for a product term is not affected by the multicollinearity. Centering predictors prior to creating the powers or the products will reduce the correlations, but the p-value the products will be exactly the same whether or not you center. Moreover the results for the other effects will be the same in either case indicating that multicollinearity has no adverse consequences.\nThe variables with high VIFs are indicator (dummy) variables that represent a categorical variable with three or more categories. This is especially true when the reference category used has a small proportion of cases. In this case, p-values for the indicator variables may be high, but the overall test that all indicators have coefficients of zero is unaffected by the high VIFs. And nothing else in the regression is affected. To avoid the high VIF values in this situaton, just choose a reference category with a larger proportion of cases.\n\nEigenvalues of the Correlation Matrix\nA second common method of evaluating collinearity is to compute and evaluate the eigenvalues of the correlation matrix for the predictors. Recall that each square (\\(k \\times k\\)) matrix has a set of k scalars, called eigenvalues (denoted \\(\\lambda\\)) associated with it. These eigenvalues can be arranged in descending order such that,\n\\[\n\\lambda_1 \\geq \\lambda_2 \\geq \\lambda_3 \\geq \\ldots \\geq \\lambda_k\n\\]\nBecause any correlation matrix is a square matrix, we can find a corresponding set of eigenvalues for the correlation matrix. If any of these eigenvalues is exactly equal to zero, it indicates a linear dependence among the variables making up the correlation matrix.\nAs a diagnostic, rather than looking at the size of all the eigenvalues, we compute the sum of the reciprocals of the eigenvalues:\n\\[\n\\sum_{i=1}^k \\frac{1}{\\lambda_i}\n\\]\nIf the predictors are orthogonal to one another (independent) then \\(\\lambda_i = 1\\) and the sum of the reciprocal values will be equal to the number of predictors, \\(\\sum_{i=1}^k \\frac{1}{\\lambda_i} = k\\).\nIf the predictors are collinear with one another (dependent) then \\(\\lambda_i = 0\\) and the sum of the reciprocal values will be equal to infinity, \\(\\sum_{i=1}^k \\frac{1}{\\lambda_i} = \\infty\\).\nWhen there is nonperfect collinearity then \\(0 < \\lambda_i < 1\\), and the sum of the reciprocal values will be greater than the number of predictors, \\(\\sum_{i=1}^k \\frac{1}{\\lambda_i} > k\\).\n\nIn an orthogonal matrix, the eigenvalues are all \\(\\pm1\\), but since the correlation matrix is positive semidefinite, the eigenvalues are all \\(+1\\).\nLarger sums of the reciprocal values of the eigenvalues is indicative of higher degrees of collinearity. In practice, we might use some cutoff to indicate when the collinearity is problematic. One such cutoff used is, if the sum is greater than five times the number of predictors, it is a sign of collinearity.\n\\[\n\\mathrm{IF} \\quad \\sum_{i=1}^k \\frac{1}{\\lambda_i} > 5k \\quad \\mathrm{THEN} \\quad \\mathrm{collnearity~is~a~problem}\n\\]\n\nIn practice, perfect collinearity is rare, but near perfect collinearity can exist and is indicated when at least one of the eigenvalues is near zero, and is quite a bit smaller than the others.\nUsing R to Compute the Eigenvalues of the Correlation Matrix\nBecause collinearity indicates dependence among the predictors, we would want to compute the eigenvalues for the correlation matrix of the predictors (do not include the outcome when computing this matrix). We can then use the eigen() function to compute the eigenvalues of a square matrix.\nIn previous classes, I have been using the correlate() function from the {corrr} package to produce correlation matrices. This function produces a formatted output that is nice for displaying the correlation matrix, but, because of its formatting, is not truly a matrix object. Instead, we will use the cor() function, which produces a matrix object, to produce the correlation matrix.\n\n\n# Correlation matrix of predictors\nr_xx = cor(eeo[c(\"faculty\", \"peer\", \"school\")])\nr_xx\n\n\n          faculty      peer    school\nfaculty 1.0000000 0.9600806 0.9856837\npeer    0.9600806 1.0000000 0.9821601\nschool  0.9856837 0.9821601 1.0000000\n\nOnce we have the correlation matrix, we can use the eigen() function to compute the eigenvalues (and eigenvectors) of the inputted correlation matrix.\n\n\n# Compute eigenvalues and eigenvectors\neigen(r_xx)\n\n\neigen() decomposition\n$values\n[1] 2.951993158 0.040047507 0.007959335\n\n$vectors\n           [,1]        [,2]       [,3]\n[1,] -0.5761385  0.67939712 -0.4544052\n[2,] -0.5754361 -0.73197527 -0.3648089\n[3,] -0.5804634  0.05130072  0.8126687\n\n# Sum of reciprocal of eigenvalues\nsum(1 / eigen(r_xx)$values)\n\n\n[1] 150.9477\n\nWe compare the sum of the reciprocal of the eigenvalues to five times the number of predictors; \\(5 \\times 3 =15\\). Since this sum is greater than 15, we would conclude that there is a collinearity problem for this model.\n\nCondition Indices\nA third common diagnostic measure of collinearity, called condition indices, is also based on the eigenvalues of the model predictors‚Äô correlation matrix. Each eigenvalue has an associated value called its condition index and denoted \\(\\kappa\\). The jth condition index is defined as\n\\[\n\\kappa_j = \\sqrt{\\frac{\\lambda_1}{\\lambda_j}}\n\\]\nfor \\(j=1,2,3,\\ldots,k\\), where \\(\\lambda_1\\) is the first (largest) eigenvalue and \\(\\lambda_j\\) is the jth eigenvalue.\nThe first condition index, \\(\\kappa_1\\), will always be equal to 1, and the other condition indices will be larger than one. The largest condition index will be,\n\\[\n\\kappa_k = \\sqrt{\\frac{\\lambda_1}{\\lambda_k}}\n\\]\nwhere \\(\\lambda_k\\) is the smallest eigenvalue. This is referred to as the condition number of the correlation matrix. If the condition number is small, it indicates that the predictors are not collinear, whereas large condition numbers are evidence supporting collinearity.\nFrom empirical work, condition numbers that exceed 15 are typically problematic (this indicates that the maximum eigenvalue is more than 225 times greater than the maximum eigenvalue). When the condition number exceeds 30, corrective action will almost surely need to be taken. Here we compute the condition indices and the condition number for our empirical example.\n\n\n# Sort eigenvalues from largest to smallest\nlambda = sort(eigen(r_xx)$values, decreasing = TRUE)\n\n# View eigenvalues\nlambda\n\n\n[1] 2.951993158 0.040047507 0.007959335\n\n# Compute condition indices\nsqrt(max(lambda) / lambda)\n\n\n[1]  1.000000  8.585586 19.258359\n\nThe condition number of the correlation matrix, \\(\\kappa = 19.26\\), suggests strong collinearity among the predictors.\n\nFixing Collinearity in Practice\nAlthough there are several solutions to ‚Äúfix‚Äù collinearity in practice, none are a magic bullet. Here are three potential fixes:\nRe-specify the model\nDrop one (or more) of the collinear predictors‚ÄîThis changes what you are controlling for;\nCombine collinear predictors;\n\nBiased estimation\nTrade small amount of bias for a reduction in coefficient variability;\n\nIntroduce prior information about the coefficients\nThis can be done formally in the analysis (e.g., Bayesian analysis);\nIt can be used to give a different model specification.\n\nNote that although collinearity is a data problem, the most common fixes in practice are to change the model. In upcoming notes, we will look at methods for combining collinear predictors and performing biased estimation.\nFor example, we could alleviate the collinearity by dropping any two of the predictors and re-fitting the model with only one predictor. This would fix the problem, but would be unsatisfactory because the resulting model would not allow us to answer the research question.\nThe highly correlated relationships between the predictors is an inherent characteristic of the data generating process we are studying. This makes it difficult to estimate the individual effects of the predictors. Instead, we could look for underlying causes that would explain the relationships we found among the predictors and perhaps re-formulate the model using these underlying causes.\n\n\n\n\n\n\nBelsley, D. A. (1991). Conditioning diagnostics, collinearity and weak data in regression. John Wiley & Sons.\n\n\nBelsley, D., Kuh, E., & Welsch, R. (1980). Regression diagnostics. Wiley.\n\n\nChatterjee, S., & Hadi, A. S. (2012). Regression analysis by example. Wiley.\n\n\nColeman, J. S., Cambell, E. Q., Hobson, C. J., McPartland, J., Mood, A. M., Weinfield, F. D., & York, R. L. (1966). Equality of educational opportunity. U.S. Government Printing Office.\n\n\nMosteller, F., & Moynihan, D. F. (1972). On equality of educational opportunity. Random House.\n\n\n\n\n",
    "preview": "posts/2021-09-30-notes-diagnosing-collinearity/distill-preview.png",
    "last_modified": "2021-10-04T13:38:50-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-04-notes-pca-via-spectral-decomposition/",
    "title": "üìù Principal Components Analysis via Spectral Decomposition",
    "description": "A brief introduction to principal components analysis via spectral decomposition. Example taken from @Chatterjee:2012.",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-09-30",
    "categories": [
      "Notes"
    ],
    "contents": "\n\n\n\nIn 1964, the US Congress passed the Civil Rights Act and also ordered a survey of school districts to evaluate the availability of equal educational opportunity in public education. The results of this survey were reported on in Coleman et al. (1966) and Mosteller & Moynihan (1972). We will use a subset of the data collected (in equal-educational-opportunity.csv) to mimic one of the original regression analyses performed; examining whether the level of school facilities was an important predictor of student achievement after accounting for the variation in faculty credentials and peer influence.\n[CSV]\n[R Script File]\nThe data come from a random sample of 70 schools in 1965. The variables, which have all been mean-centered and standardized, include:\nachievement: Measurement indicating the student achievement level\nfaculty: Measurement indicating the faculty‚Äôs credentials\npeer: Measurement indicating the influence of peer groups in the school\nschool: Measurement indicating the school facilities (e.g., building, teaching materials)\n\n\n# Load libraries\nlibrary(broom)\nlibrary(car)\nlibrary(corrr)\nlibrary(tidyverse)\nlibrary(patchwork)\n\n# Read in data\neeo = read_csv(\"~/Documents/github/epsy-8264/data/equal-education-opportunity.csv\")\nhead(eeo)\n\n\n# A tibble: 6 √ó 4\n  achievement faculty    peer school\n        <dbl>   <dbl>   <dbl>  <dbl>\n1      -0.431   0.608  0.0351  0.166\n2       0.800   0.794  0.479   0.534\n3      -0.925  -0.826 -0.620  -0.786\n4      -2.19   -1.25  -1.22   -1.04 \n5      -2.85    0.174 -0.185   0.142\n6      -0.662   0.202  0.128   0.273\n\n# Source the residual_plots() function from the script file\nsource(\"../../scripts/residual_plots.R\")\n\n\n\nThe problem we faced from the last set of notes, was that the predictors in the model were collinear, so we encountered computational issues when trying to estimate the effects and standard errors. One method to deal with collinearity among a set of predictors is to combine the predictors into a smaller subset of orthogonal measures (called principal components) that can be used instead of the original predictors. This subset of measures will not have the collinearity problems (they are orthogonal to one another), but constitute a slightly smaller amount of ‚Äúvariance accounted for‚Äù than the original set of predictors.\n\nIdea of Principal Components\nIf the X-matrix of the predictors were orthogonal, there would be no collinearity issues and we could easily estimate the effects and standard errors. This is, of course, not the case in our example in which the predictors are highly correlated. A scatterplot representing the relationship between two of the three predictors from our EEO data is shown on the left-hand side of the figure below. We can see the highly correlated nature of these variables in the plot.\nThe idea of principal components analysis is to rotate the basis vectors (coordinate system) so that the axes of the rotated basis correspond to the primary axes of the data ellipse. (Such a rotation of the basis vectors is shown in the center panel of the figure below.) When the coordinates of the predictors are expressed in this new coordinate system, the corresponding ‚Äòvariables‚Äô are orthogonal to one another. The new coordinates of the observations (under the rotated basis vectors) is shown in the right-hand side of the figure below.\n\n\n\nFigure 1: LEFT: Faculty credential and peer influence measures shown in the coordinate system given by the \\((1,0)\\)-\\((0,1)\\) basis. CENTER: The coordinate system has been rotated based to a new basis. RIGHT: The new coordinates of the observations based on the rotated coordinate space.\n\n\n\nIf we had considered all three predictors, the plot would be in three-dimensional space and we would need to rotate the coordinate system formed by the basis vectors (1, 0, 0)-(0, 1, 0)-(0, 0, 1). With three dimensions, of course, we can now rotate in multiple directions. This idea can also be extended to k-dimensional space. (For now, to keep it simple, we will continue to work with the predictor space defined by the faculty and peer predictors.)\nRecall from the chapter Basis Vectors and Matrices, that transforming the coordinates for a point to a new basis is a simple matter of pre-multiplying the vector of original coordinates by the matrix composed of the basis vectors. For example, the first observation in the eeo data had a faculty value of 0.608, and a peer value of 0.0351. If we transform this using a new set of basis vectors, say\n\\[\n\\begin{bmatrix}0.7625172 & 0.6469680 \\\\ 0.6469680 & -0.7625172\\end{bmatrix}\n\\]\nThen the same point has the coordinates (0.486, 0.367) under the new basis.\n\n\n# Coordinates in original predictor space (row vector)\nold = t(c(0.608, 0.0351))\n\n# Matrix of new basis vectors\nbasis = matrix(c(0.7625172, 0.6469680, 0.6469680, -0.7625172), nrow = 2)\n\n# Coordinates in rotated predictor space\nold %*% basis \n\n\n         [,1]      [,2]\n[1,] 0.486319 0.3665922\n\nDetermining the Principal Components\nThe set of rotated basis vectors, referred to as the set of principal components, is chosen so that:\nthe basis vectors are orthogonal to one another, and\nto maximize variance in the predictor space. For example, the direction of the first basis vector (i.e., the first principal component) is chosen to maximize the variation in the predictor space.\nMaximizing the variation in the predictor space essentially boils down to rotating the basis so that one of the basis vectors points in the direction of the major axis in the data ellipse. The second principal component is then chosen to maximize variance in an orthogonal direction to the first principal component. (With only two predictors, there is only one possible direction for the second principal component.) In our data ellipse this would be the direction of the minor axis. If the predictor space has more than two dimensions, this process continues until we exhaust the number of principal components.\n\n\n\nFigure 2: Data ellipse showing the directions of the principal components. The first principal component is in the direction of the major axis and the second principal component is in the direction of the minor axis.\n\n\n\nFinding the rotated set of basis vectors essentially comes down to determining the eigenvalues and eigenvectors for the matrix \\(\\mathbf{X}_p^\\intercal\\mathbf{X}_p\\), where \\(\\mathbf{X}_p\\) is a matrix of the predictors. Since this matrix is square, we can use spectral decomposition (i.e., eigen decomposition) to decompose this matrix as follows:\n\\[\n\\mathbf{X}_p^\\intercal\\mathbf{X}_p = \\mathbf{PDP}^\\intercal\n\\]\nwhere the columns of P are the eigenvectors of the \\(\\mathbf{X}_p^\\intercal\\mathbf{X}_p = \\mathbf{PDP}^\\intercal\\) matrix, and D is a diagonal matrix whose diagonal elements are composed of the eigenvalues of the \\(\\mathbf{X}_p^\\intercal\\mathbf{X}_p = \\mathbf{PDP}^\\intercal\\) matrix.\nThe matrices P and \\(\\mathbf{P}^\\intercal\\) are both orthogonal basis matrices that ultimately act to change the coordinate system by rotating the original basis vectors used in the predictor space. The D matrix is diagonalizing the \\(\\mathbf{X}_p^\\intercal\\mathbf{X}_p\\) matrix which amounts to finding the the major axes in the data ellipse along which our data varies.\n\nMatrix Algebra to Carry Out the PCA using Spectral Decomposition\nTo carry out the spectral decomposition in R, we need to create the matrix of the predictors (\\(\\mathbf{X}_p\\)), compute the \\(\\mathbf{X}_p^\\intercal\\mathbf{X}_p = \\mathbf{PDP}^\\intercal\\) matrix, and then use the eigen() function to carry out the spectral decomposition.\n\n\n# Create predictor matrix\nX_p = eeo %>%\n  select(peer, faculty) %>%\n  data.matrix()\n\n# Spectral decomposition\nspec_decomp = eigen(t(X_p) %*% X_p)\nspec_decomp\n\n\neigen() decomposition\n$values\n[1] 137.561000   2.724415\n\n$vectors\n          [,1]       [,2]\n[1,] 0.6469680 -0.7625172\n[2,] 0.7625172  0.6469680\n\nThe matrix of eigenvectors, in the $vectors component, compose the P matrix and make up the set of basis vectors for the rotated predictor space. The elements in the $values component are the diagonal elements in D and are the eigenvalues. The decomposition is:\n\\[\n\\begin{split}\n\\mathbf{X}^{\\intercal}_p\\mathbf{X}_p &= \\mathbf{PDP}^\\intercal \\\\[1em]\n\\begin{bmatrix}59.16 & 66.52 \\\\ 66.62 & 81.12\\end{bmatrix} &= \\begin{bmatrix}0.647 & -0.763 \\\\ 0.762 & 0.647\\end{bmatrix} \\begin{bmatrix}137.561 & 0  \\\\ 0 & 2.724 \\end{bmatrix} \\begin{bmatrix}0.647 & -0.763 \\\\ 0.762 & 0.647\\end{bmatrix}^\\intercal\n\\end{split}\n\\]\nThe span of the basis vectors define the principal components, with the first eigenvector defining the first principal component and the second eigenvector defining the second principal component. (Note: There number of principal components will always be the same as the number of predictors in the \\(\\mathbf{X}_p\\) matrix.)\nWe can post-multiply the matrix of the original predictor values (in \\(\\mathbf{X}_p\\)) by this matrix of basis vectors to obtain the predictor values in the rotated space.\n\n\n# Matrix of basis vectors for rotated predictor space\nrot_basis = spec_decomp$vectors\n\n# Compute rotated values under the new basis\nrot_pred = X_p %*% rot_basis\nhead(rot_pred)\n\n\n            [,1]        [,2]\n[1,]  0.48641930  0.36669037\n[2,]  0.91525519  0.14806327\n[3,] -1.03087107 -0.06220262\n[4,] -1.74270855  0.11707722\n[5,]  0.01287131  0.25376126\n[6,]  0.23695822  0.03365744\n\nFor example, the first observation, has a value of 0.486 on the first principal component and a value of 0.367 on the second principal component. Similarly, the second observation has a value of 0.915 on the first principal component and a value of 0.148 on the second principal component. Each observation has a set of values on the principal components.\nThe eigenvalues are related to the variances of the principal components. Because we decomposed the \\(\\mathbf{X}_p^\\intercal\\mathbf{X}_p = \\mathbf{PDP}^\\intercal\\) matrix, the variance on each prinicpal component can be computed as:\n\\[\n\\mathrm{Var}(\\mathrm{PC}_i) = \\frac{\\lambda_i}{n-1}\n\\] In our example, the variances can be computed as:\n\n\n# Compute variances of PCs\nvar_pc = spec_decomp$values / (70 - 1)\nvar_pc\n\n\n[1] 1.99363769 0.03948427\n\nThe first principal component has the largest variance, which will always be the case. Remember, the principal components are selected so the first component maximizes the variation in the predictor space, the second component will maximize the remaining variance (and be orthogonal to the first), etc.\nWe can use these variances to determine the proportion of variation in the predictor space that each principal component accounts for. This is often more useful to the applied data analyst than the actual variance measure itself. Since the principal components are orthogonal, we can sum the variances to obtain a total measure of variation in the original set of predictors accounted for by the principal components. Below, we compute the proportion of variance in the predictor space that each principal component in our example accounts for:\n\n\n# Compute proportion of variation\nvar_pc / sum(var_pc)\n\n\n[1] 0.98057949 0.01942051\n\nHere the first principal component accounts for 98.1% of the variation in the predictor space, and the second principal component accounts for the remaining 1.9% of the variation.\n\nUsing princomp() to Obtain the Principal Components\nWe can also use the R function princomp() to obtain the principal components based on the spectral decomposition. We provide this function with a data frame of the predictors.\n\n\n# Select predictors\neeo_pred = eeo %>%\n  select(faculty, peer)\n\n# Create princomp object\nmy_pca = princomp(eeo_pred)\n\n# View output\nsummary(my_pca, loadings = TRUE)\n\n\nImportance of components:\n                          Comp.1     Comp.2\nStandard deviation     1.4002088 0.19725327\nProportion of Variance 0.9805406 0.01945935\nCumulative Proportion  0.9805406 1.00000000\n\nLoadings:\n        Comp.1 Comp.2\nfaculty  0.763  0.647\npeer     0.647 -0.763\n\nThe values of the principal components, the rotated set of basis vectors, are given in the loadings output. Note that the signs of the principal components are arbitrary. For example, the vector associated with the first principal component could also have been \\((-0.763, -0.647)\\) and that for the second principal component could have been \\((-0.647, 0.763)\\). The variances of each component can be computed by squaring the appropriate standard deviations in the output.\n\n\n# Compute variance of PC1\n1.4002088 ^ 2\n\n\n[1] 1.960585\n\n# Compute variance of PC2\n0.19725327 ^ 2\n\n\n[1] 0.03890885\n\nWe can use the varianc measures to obtain a total measure of variation in the original set of predictors accounted for by each of the principal components.\n\n\n# Compute total variation accounted for\ntotal_var = 1.4002088 ^ 2 + 0.19725327 ^ 2\ntotal_var\n\n\n[1] 1.999494\n\n# Compute variation accounted for by PC1\n(1.4002088 ^ 2) / total_var\n\n\n[1] 0.9805406\n\n# Compute variation accounted for by PC2\n(0.19725327 ^ 2) / total_var\n\n\n[1] 0.01945935\n\nThis suggests that the first principal component accounts for 98% of the variance in the original set of predictors and that the second principal component accounts for 2% of the variance. (Same as we computed in the matrix algebra.) Note that these values are also given in the summary() output.\nWe can also obtain the principal component scores (the values under the rotation) for each observation by accessing the scores element of the princomp object. (Below we only show the first six scores.)\n\n\n# Get PC scores\npc_scores = my_pca$scores\n\n# View PC scores\nhead(pc_scores)\n\n\n          Comp.1      Comp.2\n[1,]  0.41884376  0.37000669\n[2,]  0.84765375  0.15132881\n[3,] -1.09849740 -0.05870659\n[4,] -1.81031365  0.12065755\n[5,] -0.05471761  0.25713367\n[6,]  0.16934323  0.03700331\n\nThese are slightly different than the scores we obtained by multiplying the original predictor values by the new basis matrix. For example, the PC scores for the first observation were \\(-0.486\\) and \\(0.367\\). The princomp() function mean centers each variable prior to multiplying by the basis matrix.\n\n\n# Mimic scores from princomp()\nold = t(c(0.608 - mean(eeo$faculty), 0.0351 - mean(eeo$peer)))\n\n# Compute PC scores\nold %*% basis\n\n\n          [,1]      [,2]\n[1,] 0.4186997 0.3699581\n\n\nBecause we want the principal components to be solely functions of the predictors, we mean center them. Otherwise we would have to include a column of ones (intercept) in the \\(\\mathbf{X}_p\\) matrix. Mean centering the predictors makes each predictor orthogonal to the intercept, in which case it can be ignored. (This is similar to how mean centering predictors removes the intercept from the fitted equation.)\nSince we only have two principal components, we can visualize the scores using a scatterplot.\n\n\n# Create data frame of scores\npc_scores = pc_scores %>%\n  data.frame()\n\n# Plot the scores\nggplot(data = pc_scores, aes(x = Comp.1, y = Comp.2)) +\n  geom_point() +\n  geom_hline(yintercept = 0, color = \"lightgrey\") +\n  geom_vline(xintercept = 0, color = \"lightgrey\") +\n  scale_x_continuous(name = \"Principal Component 1\", limits = c(-4, 4)) +\n  scale_y_continuous(name = \"Principal Component 2\", limits = c(-4, 4)) +\n  theme_bw() +\n  theme(\n    panel.grid.major = element_blank(),\n    panel.grid.minor = element_blank()\n  )\n\n\n\n\nFigure 3: Rotated predictor space using the principal components as the new basis.\n\n\n\nConceptually this visualization shows the rotated predictor space after re-orienting the rotated coordinate system. From this visualization, it is also clear that there is much more variation in the values of the first principal component than in the second principal component.\n\nUsing All Three Predictors in the PCA\nIn the example we were focused on only two predictors, as it is pedagogically easier to conceptualize since the plotting is easier. However, PCA is directly extensible to more than two variables. With three variables, the data ellipse is a data ellipsoid and there are three principal components corresponding to the three orthogonal semi-axes of the ellipsoid. With four or more variables the ideas extend although the visual doesn‚Äôt.\nWe will again use the princomp() function to compute the principal components and rotated scores.\n\nThe cutoff=0 argument in the summary() function prints all the loadings. By default, only loadings above 0.1 are displayed.\n\n\n# Select predictors\neeo_pred = eeo %>%\n  select(faculty, peer, school)\n\n# Create princomp object\nmy_pca = princomp(eeo_pred)\n\n# View output\nsummary(my_pca, loadings = TRUE, cutoff = 0)\n\n\nImportance of components:\n                          Comp.1     Comp.2     Comp.3\nStandard deviation     1.7277218 0.19747823 0.09021075\nProportion of Variance 0.9844548 0.01286135 0.00268389\nCumulative Proportion  0.9844548 0.99731611 1.00000000\n\nLoadings:\n        Comp.1 Comp.2 Comp.3\nfaculty  0.617  0.670  0.412\npeer     0.524 -0.741  0.420\nschool   0.587 -0.043 -0.809\n\nThe rotation matrix, or loading matrix, is:\n\\[\n\\mathbf{P} =\\begin{bmatrix}0.617 & 0.670 & 0.412 \\\\ 0.524 & -0.741 & 0.420 \\\\ 0.587 & -0.043 & -0.809\\end{bmatrix}\n\\]\nThe first principal component again accounts for most of the variance in the three predictors (98.4%). The other two principal components account for an additional 1.3% and 0.3% of the variance in the predictor space.\n\nInterpreting the Principal Components\nRemember that each principal component is defining a composite variable composed of all three predictors. The loadings are the weights in computing the composite variables. However, they can also be interpreted as the correlations between each particular variable and the composite. And, although the signs are arbitrary, we can try to interpret the differences in direction. So, for example,\nThe composite variable formed by the first principal component is moderately and positively correlated with all three predictors.\nThe second composite variable is highly positively correlated with the faculty variable, highly negatively correlated with the peer variable, and not correlated with the school variable.\nThe third composite variable is positively and moderately correlated with the faculty and peer variables, and highly negatively correlated with the school variable.\nSometimes these patterns of correlations can point toward an underlying latent factor, but this is a subjective call by the researcher based on their substantive knowledge. Other times the patterns make no sense; the results, after all, are just a mathematical result based on the variances and covariances of the predictors used.\nHere, the first composite might be interpreted as an overall measurement of the three predictors since all the loadings are in the same direction and at least of moderate size. The second composite seems to represent a contrast between faculty credentials and peer influence due to the opposite signs on these loadings. While the third composite points toward a more complex contrast between school facilities and the combined faculty credentials/peer group influence.\n\n\n\nChatterjee, S., & Hadi, A. S. (2012). Regression analysis by example. Wiley.\n\n\nColeman, J. S., Cambell, E. Q., Hobson, C. J., McPartland, J., Mood, A. M., Weinfield, F. D., & York, R. L. (1966). Equality of educational opportunity. U.S. Government Printing Office.\n\n\nMosteller, F., & Moynihan, D. F. (1972). On equality of educational opportunity. Random House.\n\n\n\n\n",
    "preview": "posts/2021-10-04-notes-pca-via-spectral-decomposition/distill-preview.png",
    "last_modified": "2021-10-11T10:12:20-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-29-assignment-04/",
    "title": "üêâ Assignment 04",
    "description": "Using WLS to Model Data with Outliers",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-09-29",
    "categories": [
      "Assignments"
    ],
    "contents": "\nThe goal of this assignment is to give you experience using methods for estimating regression results under violation of homoskedasticity. You will again use the data from the file stack-1979.csv to evaluate the hypothesis from political science that suggests that countries that have a stronger Socialist party have less income inequality.\n[CSV]\n[Data Codebook]\n\nInstructions\nSubmit a printed document of your responses to the following questions. Please adhere to the following guidelines for further formatting your assignment:\nAll plots should be resized so that they do not take up more room than necessary.\nAll figures and tables should have a name (e.g., Figure 1) and an appropriate caption.\nIn questions that ask you to ‚Äúuse matrix algebra‚Äù to solve the problem, you can either show your syntax and output from carrying out the matrix operations, or you can use Equation Editor to input the matrices involved in your calculations.\nThis assignment is worth 20 points.\n\nExploratory Analysis\nStart by creating a scatterplot to examine the relationship between socialist party strength and income inequality (outcome).\nAre there observations that look problematic in this plot? If so, identify the country(ies).\nFit a linear model regressing income inequality on socialist party strength. Examine and report a set of regression diagnostics that allow you to identify any observations that are regression outliers.\n\nWeighted Least Squares Estimation\nRather than removing regression outliers from the data, we can instead fit a model that accounts for these observations. For example, fitting a model that allows for higher variance at \\(x\\)-values that have outliers. With higher variances, we would expect more extreme observations because of the increased variance. The WLS model allows for heteroskedasticity and can be used to model data that have extreme observations.\nCompute the empirical weights that you will use in the WLS estimation. Report the weight for the United States. (Hint: We do not know the true variances in the population.)\nFit the WLS model. Report the fitted equation.\nBased on the model results, what is suggested about the research hypothesis that countries with more socialist tendencies have less income inequality?\nCreate a scatterplot that shows the relationship between socialist party strength and income inequality. Include the country names as labels (or instead of the points). Include both the OLS and WLS regression lines on this plot.\nBased on the plot, comment on how the residuals from the WLS model compare to the residuals from the OLS model.\nBased on your response to Question #8, how will the model-level \\(R^2\\) value from the WLS model compare to the model-level \\(R^2\\) from the OLS model. Explain.\nThe mathematical formulaa for computing the studentized residuals for both the OLS and WLS models is given below. Compute and report the studentized residuals, using this formula, from both the OLS and WLS models for any regression outliers you identified in Question #2. (Hint: Remember that in an OLS regression the weight is 1 for each observation.)\n\\[\ne^{\\prime}_i = \\frac{e_i}{s_{e(-i)}\\sqrt{1-h_{ii}}} \\times \\sqrt{w_i}\n\\]\nBased on the values of the studentized residuals in the WLS model, are the observations you identified as regression outliers from the OLS model still regression outliers in the WLS model? Why or why not?\nExplain why the is the case by referring to the formula.\nCreate and report residual plots of the studentized residuals versus the fitted values for the OLS and WLS models. Comment on which model better fits the assumptions.\n\nIncluding Covariates\nNow include the energy covariate into the model to examine the effect of socilist strength after controlling for economic development. Since the model has changed, we need to re-compute the weights and re-carry out the WLS analysis.\nUse matrix algebra to compute the empirical weights based on the two-predictor model and report the weight for the United States.\nFit the two-predictor WLS model using matrix algebra. Report the fitted equation.\nCompute and report the standard errors of the two-predictor WLS model using matrix algebra.\nUsing your results from Questions #14 and #15, compute and report the t-values and p-values. While you can use the output of the tidy(), summary(), or other functions that automatically compute p-values to check your work, use the pt() function to answer this question. (Show your work or syntax for full credit.)\nBased on the two-predictor WLS model results, what is suggested about the research hypothesis that countries with more socialist tendencies have less income inequality?\nBased on the two-predictor OLS model results, what is suggested about the research hypothesis that countries with more socialist tendencies have less income inequality?\nWhich set of the model results should we trust. Explain by referring to the tenability of the assumptions.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-29T11:30:00-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-29-reading-variance-stabilizing-transformations/",
    "title": "üìñ Variance Stabilizing Transformations",
    "description": "Reading",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-09-29",
    "categories": [
      "Reading"
    ],
    "contents": "\nRead the following article.\nOsborne, J. W. (2009). Notes on the use of data transformations. Practical Assessment, Research & Evaluation, 8(6).\n\nAdditional Resources\nKaufman, R. L. (2013). Heteroskedasticity in regression: Detection and correction. Sage. https://dx-doi-org.ezp1.lib.umn.edu/10.4135/9781452270128.n4\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-29T11:56:55-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-29-reading-wls-and-sandwich-estimation/",
    "title": "üìñ Weighted Least Squares (WLS) and Sandwich Estimation",
    "description": "Reading",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-09-29",
    "categories": [
      "Reading"
    ],
    "contents": "\nRead the following chapter. It should be accessible via the link after logging in with your x500 and password.\nRead Kaufman, R. L. (2013). Heteroskedasticity-consistent (robust) standard errors. In Heteroskedasticity in regression: Detection and correction (pp.¬†43‚Äì50). Sage. https://dx-doi-org.ezp1.lib.umn.edu/10.4135/9781452270128.n4\n\nAdditional Resources\nShin, H.-C. (1998). Weighted least squares estimation with sampling weights. Journal of Econometrics, 8(2), 251‚Äì271.\nSolon, G., Haider, S. J., & Woolridge, J. (2013). What are we weighting for? (Working Paper No.¬†18859; NBER Working Paper Series). National Bureau of Economic Research.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-29T12:00:56-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-10-08-assignment-05/",
    "title": "üßõ Assignment 05",
    "description": "This goal of this assignment is to give you experience diagnosing collinearity and using principal components analysis to create orthogonal composites that can be used in a regression model.",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-09-29",
    "categories": [
      "Assignments"
    ],
    "contents": "\nIn 2018, Iowa attorneys rated the 64 judges who were up for election on 12 different attributes in a Judicial Performance Review. They also indicated whether or not the judge should be retained. In this assignment, you are going to examine whether those ratings we can explain variation in the percentage of attorneys who endorsed retention using the data provided in the file iowa-judges.csv.\n[CSV]\n[Data Codebook]\n\nInstructions\nSubmit a printed document of your responses to the following questions. Please adhere to the following guidelines for further formatting your assignment:\nAll plots should be resized so that they do not take up more room than necessary.\nAll figures and tables should have a name (e.g., Figure 1) and an appropriate caption.\nIn questions that ask you to ‚Äúuse matrix algebra‚Äù to solve the problem, you can either show your syntax and output from carrying out the matrix operations, or you can use Equation Editor to input the matrices involved in your calculations.\nThis assignment is worth 22 points.\n\nExploratory Analysis\nCompute and report the correlation matrix of the 12 predictors.\nBased on the correlations, comment on whether there may be any potential collinearity problems. Explain.\nCompute and report the eigenvalues for the correlation matrix you created in Question 1.\nBased on the eigenvalues, comment on whether there may be any potential collinearity problems. Explain.\n\nInitial Model\nSince the ratings are assigned based on different numbers of attorneys, use a weight equal to the number of respondents to fit a WLS model that regresses the standardized retention percentage on the 12 standardized predictors.\nReport the coefficient-level output, including the estimated coefficients (beta weights), standard errors, t-values, and p-values.\nBased on the VIF values, comment on whether there may be any potential collinearity problems. Explain.\nUsing the predictor with the largest VIF value, use the VIF value to indicate how the standard error for this predictor will be impacted by the collinearity.\n\nPrincipal Components Analysis\nIn this section you are going to carry out the principal components analysis by using singular value decomposition on the correlation matrix of the predictors.\nCompute the composite score based on the first principal component for the first observation (Judge John J. Bauercamper). Show your work.\nRead the section on scree plots (Section 4) in this web article.\nCreate a scree plot showing the eigenvalues for the 12 principal components from the previous analysis.\nUsing the ‚Äúelbow criterion‚Äù, how many principal components are sufficient to describe the data? Explain by referring to your scree plot.\nUsing the ‚ÄúKaiser criterion‚Äù, how many principal components are sufficient to describe the data? Explain.\nUsing the ‚Äú80% proportion of variance criterion‚Äù, how many principal components are sufficient to describe the data? Explain.\n\nRevisit the Regression Analysis\nThe evidence from the previous section suggests that the first two principal components are sufficient to explain the variation in the predictor space.\nBy examining the pattern of correlations (size and directions) in the first two principal components, identify the construct defined by the composites of these two components. Explain.\nFit the regression analysis using the first two principal components as predictors of retention percentage. (Don‚Äôt forget your weights.) Create and report the plot of the residuals vs.¬†fitted values. What does this suggest about the validity of the linearity assumption?\nAgain, fit the same regression model using the first two principal components as predictors of retention percentage, but this time also include a quadratic effect of the first principal component. Create and report the plot of the residuals vs.¬†fitted values. What does this suggest about the validity of the linearity assumption?\nInterpret the effect of the first principal component from this model.\n\nInfluential Values\nBased on Cook‚Äôs D, identify the name of any judges (and their Cook‚Äôs D value) that are influential observations.\nRemove any influential observations identified in Question 17. Re-fit the same model. Based on comparing the model- and coefficient-level output for this model and the model which included all the observations, comment on how these observations were influencing the \\(R^2\\) value, the estimate of the quadratic effect of PC1, and the effect of PC2.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-10-11T09:42:44-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-27-notes-tools-for-dealing-with-heteroskedasticity/",
    "title": "üìù Tools for Dealing with Heteroskedasticity",
    "description": "Notes",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-09-27",
    "categories": [
      "Notes"
    ],
    "contents": "\n\n\n\nIn this set of notes, we will use data from Statistics Canada‚Äôs Survey of Labour and Income Dynamics (SLID) to explain variation in the hourly wage rate of employed citizens in Ontario. The file slid.csv includes data collected in 1994 from employed citizens living in Ontario between the ages of 16 and 65.\n[CSV]\n[R Script File]\nThe variables in the dataset are:\nwages: Composite hourly wage rate based on all the participant‚Äôs jobs\nage: Age of the participant (in years)\neducation: Number of years of schooling\nmale: A dummy-coded predictor for sex (0=Non-male; 1=Male)\n\nData Exploration\nAs with any potential regression analysis, we will begin by importing the data and examining the scatterplot of each predictor with the outcome. These plots suggest that each of the predictors is related to the outcome.\n\n\n# Load libraries\nlibrary(broom)\nlibrary(car)\nlibrary(corrr)\nlibrary(ggExtra)\nlibrary(patchwork)\nlibrary(tidyverse)\n\n# Import data\nslid = read_csv(\"https://raw.githubusercontent.com/zief0002/epsy-8264/master/data/slid.csv\")\nhead(slid)\n\n\n# A tibble: 6 √ó 4\n  wages   age education  male\n  <dbl> <dbl>     <dbl> <dbl>\n1  10.6    40        15     1\n2  11      19        13     1\n3  17.8    46        14     1\n4  14      50        16     0\n5   8.2    31        15     1\n6  17.0    30        13     0\n\n\nNote that the read_csv() function can take a URL for a dataset stored on the web.\nHere we examine the marginal density plots for the outcome and each continuous predictor, along with the scatterplots showing the relationship between each predictor and the outcome.\n\n\n\nFigure 1: Scatterplot of hourly wage versus each predictor. The fitted regression line is also displayed in each plot.\n\n\n\nBased on what we see in these plots, the outcome (hourly wage) looks to be right-skewed. A skewed outcome may or may not be problematic. (It often leads to violations of the conditional normality or homoskedasticity assumption, although we cannot confirm until after we fit the model and examine the residuals.) The relationships between hourly wage and each of the three potential predictors seem linear. The plot with the age predictor, however, foreshadows that we might violate the homoskedasticity assumption (the variance of hourly wages seems to grow for higher ages), but we will withhold judgment until after we fit our multi-predictor model.\n\nFitting a Multi-Predictor Model\nNext, we fit a model regressing wages on the three predictors simultaneously and examine the residual plots. Because we will be looking at residual plots for many different fitted models, we will write and then use a function that creates these plots.\n\n\n# Function to create residual plots\nresidual_plots = function(object){\n  # Get residuals and fitted values\n  aug_lm = broom::augment(object)\n  \n  # Create residual plot\n  p1 = ggplot(data = aug_lm, aes(x =.resid)) +\n    educate::stat_density_confidence(model = \"normal\") +\n    geom_density() +\n    theme_light() +\n    xlab(\"Residuals\") +\n    ylab(\"Probability Density\")\n  \n  # Create residual plot\n  p2 = ggplot(data = aug_lm, aes(x =.fitted, y = .resid)) +\n    geom_hline(yintercept = 0, linetype = \"dashed\") +\n    geom_point() +\n    geom_smooth(method = \"loess\", se = TRUE, n = 50, span = 0.67) +\n    theme_light() +\n    xlab(\"FItted values\") +\n    ylab(\"Residuals\")\n  \n  \n  return(p1 | p2)\n}\n\n\n\n\nNote that the {educate} package is not available from CRAN, and only available via GitHub. To install this package use the install_github() function from the {remotes} package to install it. The full syntax is: remotes::install_github(\"zief0002/educate\").\nNow we can use our new function to examine the residual plots from the main-effects model.\n\n\n# Fit model\nlm.1 = lm(wages ~ 1 + age + education + male, data = slid)\n\n# Examine residual plots\nresidual_plots(lm.1)\n\n\n\n\nFigure 2: Residual plots for the model that includes the main effects of age, education level, and sex.\n\n\n\nExamining the residual plots:\nThe linearity assumption may be violated; the loess line suggests some nonlinearity (maybe due to omitted interaction/polynomial terms)\nThe normality assumption may be violated; the upper end of the distribution deviates from what would be expected from a normal distribution in the QQ-plot.\nThe homoskedasticity assumption is likely violated; the plot of studentized residuals versus the fitted values shows severe fanning; the variation in residuals seems to increase for higher fitted values.\nBecause of the nonlinearity, we might consider including interaction terms. The most obvious interaction is that between age and education level, as it seems like the effect of age on hourly wage might be moderated by education level. (Remember, do NOT include interactions unless they make theoretical sense!) Below we fit this model, still controlling for sex, and examine the residuals.\n\n\n\nFigure 3: Residual plots for the model that includes an interaction effect between age and education level.\n\n\n\nIncluding the age by education interaction term (age:education) seems to alleviate the nonlinearity issue, but the residual plots indicate there still may be violations of the normality and homoskedasticity assumptions. Violating normality is less problematic here since, given our sample size, the Central Limit Theorem will ensure that the inferences are still approximately valid. Violating homoskedasticity, on the other hand, is more problematic.\n\nViolating Homoskedasticity\nViolating the distributional assumption of homoskedasticity results in:\nIncorrect computation of the sampling variances and covariances; and because of this\nThe OLS estimates are no longer BLUE (Best Linear Unbiased Estimator).\nThis means that the SEs (and resulting t- and p-values) for the coefficients are incorrect. In addition, the OLS estimators are no longer the most efficient estimators. How bad this is depends on several factors (e.g., how much the variances differ, sample sizes).\n\nHeteroskedasticity: What is it and How do we Deal with It?\nRecall that the variance‚Äìcovariance matrix for the residuals under the asssumption of homoskedasticity was:\n\\[\n\\boldsymbol{\\sigma^2}(\\boldsymbol{\\epsilon}) =  \\begin{bmatrix}\\sigma^2_{\\epsilon} & 0 & 0 & \\ldots & 0 \\\\ 0 & \\sigma^2_{\\epsilon} & 0 & \\ldots & 0\\\\ 0 & 0 & \\sigma^2_{\\epsilon} & \\ldots & 0\\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & 0 & \\ldots & \\sigma^2_{\\epsilon}\\end{bmatrix}\n\\]\nHomoskedasticity implies that the variance for each residual was identical, namely \\(\\sigma^2_{\\epsilon}\\). Since the variance estimate for each residual was the same, we could estimate a single value for these variances, the residual variance, and use that to obtain the sampling variances and covariances for the coefficients:\n\\[\n\\boldsymbol{\\sigma^2_B} = \\sigma^2_{\\epsilon} (\\mathbf{X}^{\\prime}\\mathbf{X})^{-1}\n\\]\nHeteroskedasticy implies that the residual variances are not constant. We can represent the variance‚Äìcovariance matrix of the residuals under heteroskedasticity as:\n\\[\n\\boldsymbol{\\sigma^2}(\\boldsymbol{\\epsilon}) =  \\begin{bmatrix}\\sigma^2_{1} & 0 & 0 & \\ldots & 0 \\\\ 0 & \\sigma^2_{2} & 0 & \\ldots & 0\\\\ 0 & 0 & \\sigma^2_{3} & \\ldots & 0\\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & 0 & \\ldots & \\sigma^2_{n}\\end{bmatrix}\n\\]\nIn this matrix, each residual has a potentially different variance. Now, there is more than one residual variance, and estimating these variance becomes more complicated, as does estimating the sampling variances and covariances of the regression coefficients.\nThere are at least three primary methods for dealing with heteroskedasticity: (1) transform the y-values using a variance stablizing transformation; (2) fit the model using weighted least squares rather than OLS; or (3) adjust the SEs and covariances to account for the non-constant variances. We will examine each of these in turn.\nVariance Stabilizing Transformations\nThe idea behind using a variance stabilizing transformation on the outcome (y) is that the transformed y-values will be homoskedastic. If so, we can fit the OLS regression model using the transformed y-values; the inferences will be valid; and, if necessary, we can back-transform for better interpretations. There are several transformations that can be applied to y that might stabilize the variances. Two common transformations are:\nLog-transformation; \\(\\ln(Y)_i\\)\nSquare-root transformation; \\(\\sqrt{Y_i}\\)\n\nPrior to applying these transformations, you may need to add a constant value to each y-value so that all \\(y_i>0\\) (log-transformation) or all \\(y_i \\geq 0\\) (square-root transformation).\nBoth of these transformations are power transformations. Power transformations have the mathematical form:\n\\[\ny^*_i = y_i^{p}\n\\]\nwhere \\(y^*_i\\) is the transformed y-value, \\(y_i\\) is the original y-value, and p is an integer. The following are all power transformations of y:\n\\[\n\\begin{split}\n& ~\\vdots \\\\[0.5em]\n& Y^4 \\\\[0.5em]\n& Y^3 \\\\[0.5em]\n& Y^2 \\\\[1.5em]\n& Y^1 = Y \\\\[1.5em]\n& Y^{0.5} = \\sqrt{Y} \\\\[0.5em]\n& Y^0 \\equiv \\ln(Y) \\\\[0.5em]\n& Y^{-1} = \\frac{1}{Y} \\\\[0.5em]\n& Y^{-2} = \\frac{1}{Y^2} \\\\[0.5em]\n& ~\\vdots\n\\end{split}\n\\]\nPowers such that \\(p<1\\) are referred to as downward transformations, and those with \\(p>1\\) are referred to as upward transformations. Both the log-transformation and square-root transformation are downward transformations of y. Here we will fit the main effects model using the square-root trnsformation and the log-transformation of the hourly wage values.\n\n\n# Create transformed y-values\nslid = slid %>%\n  mutate(\n    sqrt_wages = sqrt(wages),\n    ln_wages = log(wages)\n  )\n\n# Fit models\nlm_sqrt = lm(sqrt_wages ~ 1 + age + education + male, data = slid)\nlm_ln = lm(ln_wages ~ 1 + age + education + male, data = slid)\n\n\n\nThe plots below show the residuals based on fitting a model with each of these transformations applied to the wages data.\n\n\n\nFigure 4: TOP: Residual plots for the main effects model that used a square root transformation on y. BOTTOM: Residual plots for the main effects model that used a logarithmic transformation on y.\n\n\n\nBoth of these residual plots seem to show less heterogeneity than the residuals from the model with untransformed wages. However, neither transformation seems to have ‚Äúfixed‚Äù the problem completely.\n\nBox-Cox Transformation\nIs there a power transformation that would better ‚Äúfix‚Äù the heteroskedasticity? In their seminal paper, Box & Cox (1964) proposed a series of power transformations that could be applied to data in order to better meet assumptions such as linearity, normality, and homoskedasticity. The general form of the Box-Cox model is:\n\\[\nY^{(\\lambda)}_i = \\beta_0 + \\beta_1(X1_{i}) + \\beta_2(X2_{i}) + \\ldots + \\beta_k(Xk_{i}) + \\epsilon_i\n\\]\nwhere the errors are independent and \\(\\mathcal{N}(0,\\sigma^2_{\\epsilon})\\), and\n\\[\nY^{(\\lambda)}_i = \\begin{cases}\n   \\frac{Y_i^{\\lambda}-1}{\\lambda} & \\text{for } \\lambda \\neq 0 \\\\[1em]\n   \\ln(Y_i)       & \\text{for } \\lambda = 0\n  \\end{cases}\n\\]\nThis is only defined for positive values of Y.\nThe powerTransform() function from the {car} library can be used to determine the optimal value of \\(\\lambda\\). The boxCox() function from the same library gives a profile plot showing the log-likelohoods for a sequence of \\(\\lambda\\) values. If you do not specify the sequence of \\(\\lambda\\) values it will use lambda = seq(from = -2, to = 2, by = 1/10) by default.\n\n\n# Find optimal power transformation using Box-Cox\npowerTransform(lm.1)\n\n\nEstimated transformation parameter \n        Y1 \n0.08598786 \n\nThe output from the powerTransform() function gives the optimal power for the transformation of y, namely \\(\\lambda = 0.086\\). To actually implement the power transformation we use the transform Y based on the Box-Cox algorithm presented earlier.\n\n\nslid = slid %>%\n  mutate(\n    bc_wages = (wages ^ 0.086 - 1) / 0.086\n  )\n\n# Fit models\nlm_bc = lm(bc_wages ~ 1 + age + education + male, data = slid)\n\n\n\nThe residual plots (shown below) indicate better behaved residuals for the main-effects model, although even this optimal transformation still shows some evidence of heteroskedasticity.\n\n\n\nFigure 5: Residual plots for the main effects model that used a Box-Cox transformation on Y with \\(\\lambda=0.086\\).\n\n\n\nOne problem with using this transformation is that the regression coefficients do not have a direct interpretation. For example, looking at the coefficent-level output:\n\n\ntidy(lm_bc, conf.int = TRUE)\n\n\n# A tibble: 4 √ó 7\n  term        estimate std.error statistic   p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)   1.04    0.0475        21.9 1.80e-100   0.947     1.13  \n2 age           0.0227  0.000687      33.0 2.85e-211   0.0213    0.0240\n3 education     0.0707  0.00272       26.0 5.14e-138   0.0654    0.0760\n4 male          0.282   0.0164        17.2 4.99e- 64   0.250     0.315 \n\nThe age coefficient would be interpreted as: each one-year difference in age is associated with a 0.0227-unit difference in the transformed Y controlling for differences in education and sex. But what does a 0.227-unit difference in transformed Y mean when we translate that back to wages?\n\nProfile Plot for Different Transformations\nMost of the power transformations under Box-Cox would produce coefficients that are difficult to interpret. The exception is when \\(\\lambda=0\\). This is the log-transformation which is directly interpretable. Since the optimal \\(\\lambda\\) value of 0.086 is quite close to 0, we might wonder whether we could just use the log-transformation (\\(\\lambda=0\\)). The Box-Cox algorithm optimizes the log-likelihood of a given model, so the statistical question is whether there is a difference in the log-likelihood produced by the optimal transformation and that for the log-transformation.\nTo evaluate this, we can plot of the log-likelihood for a given model using a set of lambda values. This is called a profile plot of the log-likelihood. The boxCox() function creates a profile plot of the log-likelihood for a defined sequence of \\(\\lambda\\) values. Here we will plot the profile of the log-likelihood for \\(-2 \\leq \\lambda \\leq 2\\).\n\n\n# Plot of the log-likelihood for a given model versus a sequence of lambda values\nboxCox(lm.1, lambda = seq(from = -2, to = 2, by = 0.1))\n\n\n\n\nFigure 6: Plot of the log-likelihood profile for a given model versus a sequence of lambda values. The lambda that produces the highest log-likelihood is 0.086, the optimal lambda value.\n\n\n\nThe profile plot shows that the optimal lambda value, 0.86, produces the maximum log-likelihood value for the given model. We also are shown the 95% confidence limits for lambda based on a test of the curvature of the log-likelihood function. This interval offers a range of \\(\\lambda\\) values that will give comparable transformations. Since the values associated with the confidence limits are not outputted by the boxCox() function, we may need to zoom in to determine these limits by tweaking the sequence of \\(\\lambda\\) values in the boxCox() function.\n\n\nboxCox(lm.1, lambda = seq(from = 0.03, to = 0.2, by = .001))\n\n\n\n\nFigure 7: Plot of the log-likelihood profile for a given model versus a narrower sequence of lambda values.\n\n\n\nIt looks as though \\(.03 \\leq \\lambda \\leq 0.14\\) all give comparable transformations. Unfortunately, 0 is not included in those limits. This means that the \\(\\lambda\\) value of 0.086 will produce a higher log-likelihood than the log-transformation. It is important to remember that even though the log-likelihood will be optimized, the compatibility with the assumptions may or may not be improved when we use \\(\\lambda=0.086\\) versus \\(\\lambda=0\\). The only way to evaluate this is to fit the models and check the residuals.\n\nWeighted Least Squares Estimation\nAnother method for dealing with heteroskedasticity is to change the method we use for estimating the coefficients and standard errors. The most common method for doing this is to use weighted least squares (WLS) estimation rather than ordinary least squares (OLS).\nUnder heteroskedasticity recall that the residual variance of the ith residual is \\(\\sigma^2_i\\), and the variance‚Äìcovariance matrix of the residuals is defined as,\n\\[\n\\boldsymbol{\\Sigma} =  \\begin{bmatrix}\\sigma^2_{1} & 0 & 0 & \\ldots & 0 \\\\ 0 & \\sigma^2_{2} & 0 & \\ldots & 0\\\\ 0 & 0 & \\sigma^2_{3} & \\ldots & 0\\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & 0 & \\ldots & \\sigma^2_{n}\\end{bmatrix},\n\\]\nThis implies that the n observations no longer have the same reliability (i.e., precision of estimation). Observations with small variances have more reliability than observations with large variances. The idea behind WLS estimation is that those observations that are less reliable are down-weighted in the estimation of the overall error variance.\nAssume Error Variances are Known\nLet‚Äôs assume that each of the error variances, \\(\\sigma^2_i\\), are known. This is generally not a valid assumption, but it gives us a point to start from. If we know these values, we can modify the likelihood function from OLS by substituting these values in for the OLS error variance, \\(\\sigma^2_{\\epsilon}\\).\n\\[\n\\begin{split}\n\\mathrm{OLS:} \\qquad \\mathcal{L}(\\boldsymbol{\\beta}) &= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2_{\\epsilon}}}\\exp\\left[-\\frac{1}{2\\sigma^2_{\\epsilon}} \\big(Y_i-\\beta_0 - \\beta_1X_{1i} - \\beta_2X_{2i} - \\ldots - \\beta_kX_{ki}\\big)^2\\right] \\\\[1em]\n\\mathrm{WLS:} \\qquad \\mathcal{L}(\\boldsymbol{\\beta}) &= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2_{i}}}\\exp\\left[-\\frac{1}{2\\sigma^2_{i}} \\big(Y_i-\\beta_0 - \\beta_1X_{1i} - \\beta_2X_{2i} - \\ldots - \\beta_kX_{ki}\\big)^2\\right]\n\\end{split}\n\\]\nNext, we define the reciprocal of the error variances as \\(w_i\\), or weight:\n\\[\nw_i = \\frac{1}{\\sigma^2_i}\n\\]\nThis can be used to simplify the likelihood function for WLS:\n\\[\n\\begin{split}\n\\mathcal{L}(\\boldsymbol{\\beta}) &= \\bigg[\\prod_{i=1}^n \\sqrt{\\frac{w_i}{2\\pi}}\\bigg]\\exp\\left[-\\frac{1}{2} \\sum_{i=1}^n w_i\\big(Y_i-\\beta_0 - \\beta_1X_{1i} - \\beta_2X_{2i} - \\ldots - \\beta_kX_{ki}\\big)^2\\right]\n\\end{split}\n\\]\nWe can then find the coefficient estimates by maximizing \\(\\mathcal{L}(\\boldsymbol{\\beta})\\) with respect to each of the coefficients; these derivatives will result in k normal equations. Solving this system of normal equations we find that:\n\\[\n\\mathbf{b}_{\\mathrm{WLS}} = (\\mathbf{X}^{\\intercal}\\mathbf{W}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}\\mathbf{W}\\mathbf{y}\n\\]\nwhere W is a diagonal matrix of the weights,\n\\[\n\\mathbf{W} =  \\begin{bmatrix}w_{1} & 0 & 0 & \\ldots & 0 \\\\ 0 & w_{2} & 0 & \\ldots & 0\\\\ 0 & 0 & w_{3} & \\ldots & 0\\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & 0 & \\ldots & w_{n}\\end{bmatrix}\n\\]\nThe variance‚Äìcovariance matrix for the regression coefficients can then be computed using:\n\\[\n\\boldsymbol{\\sigma^2}(\\mathbf{B}) = \\sigma^2_{\\epsilon}(\\mathbf{X}^{\\intercal}\\mathbf{W}\\mathbf{X})^{-1}\n\\]\nwhere the estimate for \\(\\sigma^2_{\\epsilon}\\) is based on a weighted sum of squares:\n\\[\n\\hat\\sigma^2_{\\epsilon} = \\frac{\\sum_{i=1}^n w_i \\times \\epsilon_i^2}{n - k - 1}\n\\]\nWhich can be expressed in matrix algebra as a function of the weight matrix and residual vector as:\n\\[\n\\hat\\sigma^2_{\\epsilon} = \\frac{(\\mathbf{We})^{\\intercal}\\mathbf{e}}{n - k - 1}\n\\]\n\nAn Example of WLS Estimation\nTo illustrate WLS, consider the following data which includes average ACT scores for a classroom of students, ACT score for the teacher, and the standard deviation of the class ACT scores.\n\n\nClass Average ACT\n\n\nTeacher ACT\n\n\nClass SD\n\n\n17.3\n\n\n21\n\n\n5.99\n\n\n17.1\n\n\n20\n\n\n3.94\n\n\n16.4\n\n\n19\n\n\n1.90\n\n\n16.4\n\n\n18\n\n\n0.40\n\n\n16.1\n\n\n17\n\n\n5.65\n\n\n16.2\n\n\n16\n\n\n2.59\n\n\nSuppose we want to use the teacher‚Äôs ACT score to predict variation in the class average ACT score. Fitting this model using OLS, we can compute the coefficient estimates and the standard errors for each coefficient.\n\n\n# Enter y vector\ny = c(17.3, 17.1, 16.4, 16.4, 16.1, 16.2)\n\n# Create design matrix\nX = matrix(\n  data = c(rep(1, 6), 21, 20 , 19, 18, 17, 16),\n  ncol = 2\n)\n\n# Compute coefficients\nb = solve(t(X) %*% X) %*% t(X) %*% y\n\n# Compute SEs for coefficients\ne = y - X %*% b\nsigma2_e = t(e) %*% e / (6 - 1 - 1) \nV_b = as.numeric(sigma2_e) * solve(t(X) %*% X)\nsqrt(diag(V_b))\n\n\n[1] 0.98356794 0.05294073\n\nWe could also have used built-in R functions to obtain these values:\n\n\nlm.ols = lm(y ~ 1 + X[ , 2])\ntidy(lm.ols, conf.int = TRUE)\n\n\n# A tibble: 2 √ó 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)   12.1      0.984      12.3  0.000252   9.36      14.8  \n2 X[, 2]         0.243    0.0529      4.59 0.0101     0.0959     0.390\n\nThe problem, of course, is that the variation in the residuals is not constant as the reliability for the 10 class average ACT values is not the same for each class; the standard deviations are different. Because of this, we may want to fit a WLS regression model rather than an OLS model.\n\n\n# Set up weight matrix, W\nclass_sd = c(5.99, 3.94, 1.90, 0.40, 5.65, 2.59)\nw_i = 1 / (class_sd ^ 2)\nW = diag(w_i)\nW\n\n\n          [,1]       [,2]      [,3] [,4]       [,5]      [,6]\n[1,] 0.0278706 0.00000000 0.0000000 0.00 0.00000000 0.0000000\n[2,] 0.0000000 0.06441805 0.0000000 0.00 0.00000000 0.0000000\n[3,] 0.0000000 0.00000000 0.2770083 0.00 0.00000000 0.0000000\n[4,] 0.0000000 0.00000000 0.0000000 6.25 0.00000000 0.0000000\n[5,] 0.0000000 0.00000000 0.0000000 0.00 0.03132587 0.0000000\n[6,] 0.0000000 0.00000000 0.0000000 0.00 0.00000000 0.1490735\n\n# Compute coefficients\nb_wls = solve(t(X) %*% W %*% X) %*% t(X) %*% W %*% y\nb_wls\n\n\n           [,1]\n[1,] 13.4154764\n[2,]  0.1658431\n\n# Compute standard errors for coefficients\ne_wls = y - X %*% b_wls                                 # Compute errors from WLS\nmse_wls = (t(W %*% e_wls) %*% e_wls) / (6 - 1 - 1)      # Compute MSE estimate\nv_b_wls = as.numeric(mse_wls) * solve(t(X) %*% W %*% X) # Compute variance-covariance matrix for B\nsqrt(diag(v_b_wls))\n\n\n[1] 1.17680463 0.06527187\n\nThe results of fitting both the OLS and WLS models appear below. Comparing the two sets of results, there is a difference in the coefficient values and in the estimated SEs when using WLS estimation rather than OLS estimation. This would also impact any statistical inference as well.\n\n\n\n\n\nOLS\n\n\n\n\nWLS\n\n\n\nCoefficient\n\n\nB\n\n\nSE\n\n\nB\n\n\nSE\n\n\nIntercept\n\n\n12.0905\n\n\n0.9836\n\n\n13.4155\n\n\n1.1768\n\n\nEffect of Teacher ACT Score\n\n\n0.2429\n\n\n0.0529\n\n\n0.1658\n\n\n0.0653\n\n\n\nFitting the WLS estimation in the lm() Function\nThe lm() function can also be used to fit a model using WLS estimation. To do this we include the weights= argument in lm(). This takes a vector of weights representing the \\(w_i\\) values for each of the n observations.\n\n\n# Create weights vector\nw_i = 1 / (class_sd ^ 2)\n\n# Fit WLS model\nlm_wls = lm(y ~ 1 + X[ , 2], weights = w_i)\ntidy(lm_wls, conf.int = TRUE)\n\n\n# A tibble: 2 √ó 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)   13.4      1.18       11.4  0.000338  10.1       16.7  \n2 X[, 2]         0.166    0.0653      2.54 0.0639    -0.0154     0.347\n\nNot only can we use tidy() and glance() to obtain coefficient and model-level summaries, but we can also use augment(), anova(), or any other function that takes a fitted model as its input.\n\nWhat if Error Variances are Unknown?\nThe previous example assumed that the variance‚Äìcovariance matrix of the residuals was known. In practice, this is almost never the case. When we do not know the error variances, we need to estimate them from the data.\nOne method for estimating the error variances for each observation, is:\nFit an OLS model to the data, and obtain the residuals.\nSquare these residuals and regress them (using OLS) on the same set of predictors.\nObtain the fitted values from Step 2.\nCreate the weights using \\(w_i = \\frac{1}{\\hat{y}_i}\\) where \\(\\hat{y}_i\\) are the fitted values from Step 3.\nFit the WLS using the weights from Step 4.\nThis is a two-stage process in which we (1) estimate the weights, and (2) use those weights in the WLS estimation. We will illustrate this methodology using the SLID data.\n\n\n# Step 1: Fit the OLS regression\nlm_step_1 = lm(wages ~ 1 + age + education + male + age:education, data = slid)\n\n# Step 2: Obtain the residuals and square them\nout_1 = augment(lm_step_1) %>%\n  mutate(\n    e_sq = .resid ^ 2\n  )\n\n# Step 2: Regresss e^2 on the predictors from Step 1\nlm_step_2 = lm(e_sq ~ 1 + age + education + male + age:education, data = out_1)\n\n# Step 3: Obtain the fitted values from Step 2\ny_hat = fitted(lm_step_2)\n\n\n# Step 4: Create the weights\nw_i = 1 / (y_hat ^ 2)\n\n# Step 5: Use the fitted values as weights in the WLS\nlm_step_5 = lm(wages ~ 1 + age + education + male + age:education, data = slid, weights = w_i)\n\n\n\nBefore examining any output from this model, let‚Äôs examine the residual plots. The residual plots suggest that the homoskedasticity assumption is much more reasonably satisfied after using WLS estimation; although it is still not perfect. The normality assumption looks untenable here.\n\nOne way to proceed would be to apply a variance stabilizing transformation to y (e.g., log-transform) and then fit a WLS model. To do this you would go through the steps of estimating the weights again based on the transformed y.\n\n\n\nFigure 8: Residual plots for the model that includes the main effects of age, education level, and sex fitted with WLS estimation.\n\n\n\nThe WLS coefficient estimates, standard errors, and coefficient-level inference are presented below.\n\n\n# Examine coefficient-level output\ntidy(lm_step_5, conf.int = TRUE)\n\n\n# A tibble: 5 √ó 7\n  term          estimate std.error statistic   p.value conf.low conf.high\n  <chr>            <dbl>     <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)     4.97    0.227        21.9  1.37e-100   4.52      5.41  \n2 age             0.0801  0.00591      13.5  6.95e- 41   0.0685    0.0917\n3 education      -0.201   0.0316       -6.36 2.30e- 10  -0.263    -0.139 \n4 male            2.24    0.166        13.5  1.56e- 40   1.92      2.57  \n5 age:education   0.0185  0.000921     20.1  1.77e- 85   0.0167    0.0203\n\n\nAdjusting the Standard Errors: Sandwich Estimation\nSince the primary effect of heteroskedasticity is that the sampling variances and covariances are incorrect, one method of dealing with this assumption violation is to use the OLS coefficients (which are still unbiased under heteroskedasticity), but make adjustments to the variance‚Äìcovariance matrix of the coefficients. We can compute the adjusted variance‚Äìcovariance matrix of the regression coefficients using:\n\\[\nV(\\mathbf{b}) = (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}\\boldsymbol{\\Sigma}\\mathbf{X} (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\n\\]\nwhere, \\(\\boldsymbol{\\Sigma}\\) is the variance-covariance matrix of the residuals.\n\nThis is often referred to as a sandwich estimator because the \\(\\mathbf{X}^{\\intercal}\\boldsymbol{\\Sigma}\\mathbf{X}\\) is ‚Äúsandwiched‚Äù between two occurrences of \\((\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\).\nNote that under the standard regression assumptions (including homoskedasticity), \\(\\boldsymbol{\\Sigma} = \\sigma^2_{\\epsilon}\\mathbf{I}\\), and this whole expression can be simplified to the matrix expression of the variance‚Äìcovariance matrix for the coefficients under the OLS model.:\n\\[\n\\begin{split}\nV(\\mathbf{b}) &= (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}\\sigma^2_{\\epsilon}\\mathbf{IX} (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1} \\\\[2ex]\n&= \\sigma^2_{\\epsilon}(\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}\\mathbf{IX} (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1} \\\\[2ex]\n&= \\sigma^2_{\\epsilon}(\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}\\mathbf{X} (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1} \\\\[2ex]\n&= \\sigma^2_{\\epsilon} \\mathbf{I} (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1} \\\\[2ex]\n&= \\sigma^2_{\\epsilon} (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\n\\end{split}\n\\]\nIf the errors are, however, heteroskedastic, then we need to use the heteroskedastic variance‚Äìcovariance of the residuals,\n\\[\n\\boldsymbol{\\Sigma} =  \\begin{bmatrix}\\sigma^2_{1} & 0 & 0 & \\ldots & 0 \\\\ 0 & \\sigma^2_{2} & 0 & \\ldots & 0\\\\ 0 & 0 & \\sigma^2_{3} & \\ldots & 0\\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & 0 & \\ldots & \\sigma^2_{n}\\end{bmatrix},\n\\]\nOne of the computational formulas for variance of a random variable X, using the rules of expectation is:\n\\[\n\\sigma^2_X = \\mathbb{E}\\bigg(\\big[X_i - \\mathbb{E}(X)\\big]^2\\bigg)\n\\]\nThis means for the ith error variance, \\(\\sigma^2_i\\), can be computed as\n\\[\n\\sigma^2_{i} = \\mathbb{E}\\bigg(\\big[\\epsilon_i - \\mathbb{E}(\\epsilon)\\big]^2\\bigg)\n\\]\nWhich, since \\(\\mathbb{E}(\\epsilon)=0\\) simplifies to\n\\[\n\\sigma^2_{i} = \\mathbb{E}\\big(\\epsilon_i^2\\big)\n\\]\nThis suggests that we can estimate \\(\\boldsymbol{\\Sigma}\\) as:\n\\[\n\\hat{\\boldsymbol{\\Sigma}} =  \\begin{bmatrix}\\epsilon^2_{1} & 0 & 0 & \\ldots & 0 \\\\ 0 & \\epsilon^2_{2} & 0 & \\ldots & 0\\\\ 0 & 0 & \\epsilon^2_{3} & \\ldots & 0\\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & 0 & \\ldots & \\epsilon^2_{n}\\end{bmatrix},\n\\]\nIn other words, the estimated variance‚Äìcovariance matrix of the residuals under heteroskedasticity is a diagonal matrix with elements that are the squared residuals from the OLS model. This\nGOing back to our SLID example, we can compute the adjusted variance‚Äìcovariance matrix of the coefficients by using the sandwich estimation method.\n\n\n# Fit OLS model\nlm.ols = lm(wages ~ 1 + age + education + male, data = slid)\n\n# Design matrix\nX = model.matrix(lm.1)\n\n# Create Sigma matrix\ne_squared = augment(lm.1)$.resid ^ 2\nSigma = e_squared * diag(3997)\n\n# Variance-covariance matrix for B\nV_b_adj = solve(t(X) %*% X) %*% t(X) %*% Sigma %*% X %*% solve(t(X) %*% X)\n\n# Compute SEs\nsqrt(diag(V_b_adj))\n\n\n(Intercept)         age   education        male \n0.635836527 0.008807793 0.038468695 0.207141705 \n\nThe SEs we produce from this method are typically referred to as Huber-White standard errors because they were introduced in a paper by Huber (1967) and their some of their statistical properties were proved in a paper by White (1980).\n\nModifying the Huber-White Estimates\nSimulation studies by Long & Ervin (2000) suggest a slight modification to the Huber-White estimates; by using a slightly different \\(\\boldsymbol\\Sigma\\) matrix:\n\\[\n\\hat{\\boldsymbol{\\Sigma}} =  \\begin{bmatrix}\\frac{\\epsilon^2_{1}}{(1-h_{11})^2} & 0 & 0 & \\ldots & 0 \\\\ 0 & \\frac{\\epsilon^2_{2}}{(1-h_{22})^2} & 0 & \\ldots & 0\\\\ 0 & 0 & \\frac{\\epsilon^2_{3}}{(1-h_{33})^2} & \\ldots & 0\\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & 0 & \\ldots & \\frac{\\epsilon^2_{n}}{(1-h_{nn})^2}\\end{bmatrix},\n\\] where, \\(h_{ii}\\) is the ith diagonal element from the H matrix.\nWe can compute this modification by adjusting the e_squared value in the R syntax as:\n\n\n# Sigma matrix\ne_squared = augment(lm.1)$.resid ^ 2  / ((1 - augment(lm.1)$.hat) ^ 2)\nSigma = e_squared * diag(3997)\n\n# Variance-covariance matrix for B\nV_b_hw_mod = solve(t(X) %*% X) %*% t(X) %*% Sigma %*% X %*% solve(t(X) %*% X)\n\n# Compute adjusted SEs\nsqrt(diag(V_b_hw_mod))\n\n\n(Intercept)         age   education        male \n0.637012622 0.008821005 0.038539628 0.207364732 \n\nWe could use these SEs to compute the t-values, associated p-values, and confidence intervals for each of the coefficients.\nThe three sets of SEs are:\n\n\n\n\n\nSE\n\n\n\nCoefficient\n\n\nOLS\n\n\nHuber-White\n\n\nModified Huber-White\n\n\nIntercept\n\n\n0.5989773\n\n\n0.6358365\n\n\n0.6370126\n\n\nAge\n\n\n0.0086640\n\n\n0.0088078\n\n\n0.0088210\n\n\nEducation\n\n\n0.0342567\n\n\n0.0384687\n\n\n0.0385396\n\n\nAge x Education\n\n\n0.2070092\n\n\n0.2071417\n\n\n0.2073647\n\n\nIn these data, the modified Huber-White adjusted SEs are quite similar to the SEs we obtained from OLS, despite the heteroskedasticity observed in the residuals. One advantage of this method is that we do not have to have a preconceived notion of the underlying pattern of variation like we do to use WLS estimation. (We can estimate the pattern using the multi-step approach introduced earlier, but this assumes that the method of estimation correctly mimics the pattern of variation.) If, however, we can identify the pattern of variation, then WLS estimation will produce more efficient (smaller) standard errors than sandwich estimation.\n\n\n\nBox, G. E. P., & Cox, D. R. (1964). An analysis of transformations. Journal of the Royal Statisistical Society, Series B, 26, 211‚Äì246.\n\n\nHuber, P. J. (1967). The behavior of maximum likelihood estimates under nonstandard conditiona. In L. M. Le Cam & J. Neyman (Eds.), Proceedings of the fifth berkeley symposium on mathematical statistics and probability (pp. 221‚Äì233). University of California Press.\n\n\nLong, J. S., & Ervin, L. H. (2000). Using heteroskedasticity consistent standard errors in the linear regression model. The American Statistician, 54, 217‚Äì224.\n\n\nWhite, H. (1980). A heteroskedasticity-consistent covariance matrix estimator and a direct test for heteroskedasticity. Econometrica, 38, 817‚Äì838.\n\n\n\n\n",
    "preview": "posts/2021-09-27-notes-tools-for-dealing-with-heteroskedasticity/distill-preview.png",
    "last_modified": "2021-09-29T12:15:07-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-23-notes-regression-diagnostics/",
    "title": "üìù Regression Diagnostics",
    "description": "Notes",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-09-26",
    "categories": [
      "Notes"
    ],
    "contents": "\nHere are links to several script files and handouts that we will use in class:\nSlides is a set of slides we will cover in class.\nScript File is a script file that provides syntax for generating data from a given population regression model.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-26T08:39:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-24-assignment-03/",
    "title": "ü¶Ñ Assignment 03",
    "description": "The goal of this assignment is to give you experience using regression diagnostics for detecting problematic observations.",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-09-24",
    "categories": [
      "Assignments"
    ],
    "contents": "\nThere is a hypothesis in political science that suggests that income inequality is related to the democratic experience and economic development of a country. In this assignment, you are going to examine whether this hypothesis is supported by empirical evidence by using the data provided in the file stack-1979.csv. In particular, you are going to regress income inequality on voter turnout (democratic experience) and energy consumption (economic development).\n[CSV]\n[Data Codebook]\nInstructions\nSubmit a printed document of your responses to the following questions. Please adhere to the following guidelines for further formatting your assignment:\nAll plots should be resized so that they do not take up more room than necessary.\nAll figures and tables should have a name (e.g., Figure 1) and an appropriate caption.\nIn questions that ask you to ‚Äúuse matrix algebra‚Äù to solve the problem, you can either show your syntax and output from carrying out the matrix operations, or you can use Equation Editor to input the matrices involved in your calculations.\nThis assignment is worth 18 points.\n\nExploratory Analysis\nStart by creating scatterplots to examine the relationship between each of the predictors and the outcome. Are there observations that look problematic in these plots? If so, identify the country(ies).\nFit the regression model (specified in the introduction) to the data. Report the fitted equation.\nCreate and include a set of plots that allow you to examine the assumptions for linear regression. Based on these plots, comment on the tenability of these assumptions.\n\nOutliers, Leverage, and Influence\nCompute the externally studentized residuals for the observations based on the fitted regression. Based on these values, identify any countries that you would consider as regression outliers. Explain why you identified these countries as regression outliers.\nFit a mean-shift model that will allow you to test whether the observation with the largest absolute studentized residual is statistically different from zero. Report the coefficient-level output (B, SE, t, and p) for this model.\nFind (and report) the Bonferroni adjusted p-value for the observation with the largest absolute studentized residual. Based on this p-value, is there statistical evidence to call this observation a regression outlier? Explain.\nCreate and include an index plot of the leverage values. Include a line in this plot that displays the cutpoint for ‚Äúhigh‚Äù leverage. Based on this plot, identify any countries with large leverage values.\nBased on the evidence you have looked at in Questions #4‚Äì7, do you suspect that any of the countries might influence the regression coefficients? Explain.\n\nInfluence Measures\nFor each of the influence measures listed below, create and include an index plot of the influence measure. For each plot, also include a line that displays the cutpoint for ‚Äúhigh‚Äù influence. (3pts)\nScaled (standardized) DFBETA values\nCook‚Äôs D\nDFFITS\nCOVRATIO\n\nShow how the Cook‚Äôs D value for the country with the largest Cook‚Äôs D value is calculated using the country‚Äôs leverage value and standardized residual.\nCreate and include the added-variable plots for each each of the coefficients. Based on these plots, identify any countries that you believe may be jointly influencing the regression coefficients.\n\nRemove and Refit\nBased on all of the evidence from the different influence measures you examined, identify and report the country(ies) that are influential. Explain how you decided on this set of observations.\nRemove the observations you identified in Question #12 from the data and refit the regression model omitting these observations. Report the fitted equation.\nCreate and include a set of plots that allow you to examine the assumptions for linear regression. Based on these plots, comment on the tenability of these assumptions.\nCompare and contrast the coefficient-level inferences from the model fitted with the full data and that fitted with the omitted observations.\nCompare and contrast the model-level summaries, namely \\(R^2\\) and the RMSE, from the model fitted with the full data and that fitted with the omitted observations.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-23T11:40:15-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-23-reading-regression-diagnostics/",
    "title": "üìñ Regression Diagnostics",
    "description": "Reading",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-09-23",
    "categories": [
      "Reading"
    ],
    "contents": "\nRead the following chapter. It should be accessible via the link after logging in with your x500 and password.\nFox, J. (1991). Outlying and influential data. In Regression diagnostics (pp.¬†21‚Äì40). Sage. doi: https://dx-doi-org.ezp1.lib.umn.edu/10.4135/9781412985604.n4\n\nAdditional Resources\nKim, B. (2015). Understanding diagnostic plots for linear regression analysis. University of Virginia Library.\nCook, R. D. (1998). Regression graphics: Ideas for studying regressions through graphics. Wiley.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-29T11:55:12-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-16-assignment-02/",
    "title": "ü¶í Assignment 02",
    "description": "Simulating from the Regression Model",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-09-20",
    "categories": [
      "Assignments"
    ],
    "contents": "\nThe goal of this assignment is to give you experience using simulation to explore properties of the regression model.\n\n\n\n\n\nInstructions\nSubmit a printed document of your responses to the following questions. Please adhere to the following guidelines for further formatting your assignment:\nAll plots should be resized so that they do not take up more room than necessary.\nAll figures and tables should have a name (e.g., Figure 1) and an appropriate caption.\nIn questions that ask you to ‚Äúuse matrix algebra‚Äù to solve the problem, you can either show your syntax and output from carrying out the matrix operations, or you can use Equation Editor to input the matrices involved in your calculations.\nThis assignment is worth 16 points.\n\nSimulation 1: Modeling Heteroskedasticity\nIn this simulation, you will explore what happens to the regression estimates when the assumption of homoskedasticity is violated. For this simulation use a sample size of 200.\nCreate the fixed X values you will use in each trial of the simulation. To do this, draw \\(n=200\\) X-values from a uniform distribution with a minimum of \\(-2\\) and a maximum of \\(+2\\). Prior to drawing these values, set your starting seed to 678910. Report the syntax you used, and the first six X values.\nCreate a the Y-values for the first trial of the simulation by using the model:\n\\[\n\\begin{split}\ny_i &= -3.2 + 1.75(x_i) + \\epsilon_i \\\\[2ex]\n\\epsilon_i &\\overset{i.i.d.}{\\sim} \\mathcal{N}(0, \\sigma)\n\\end{split}\n\\]\n\nwhere\n\n\\[\n\\begin{split}\n\\sigma &= e^{\\gamma(x_i)}\\\\[2ex]\ne &\\mathrm{~is~Euhler's~constant~}(\\approx2.718282) \\\\[2ex]\n\\gamma &= 1.5\n\\end{split}\n\\]\n\nHere the variation in the random error is a function of X and random noise. Report the syntax you used, and the first six Y values.\n\nCreate and report the scatterplot of the Y-values versus the X-values for this first trial of the simulation.\nDescribe the pattern of heteroscedasticity.\nDoes the pattern of heteroscedasticity you described in Question 4 make sense given how the error term was created. Explain.\nCarry out 1000 trials of the simulation. (Reminder: Be sure to use these same X values in each trial of the simulation; they are fixed.) For each trial, collect: (a) the estimate of the intercept, (b) the estimate of the slope, and (c) the estimate of the residual standard error.\nCompute and report the mean value for the residual standard error.\nSimulation 2: Homoskedastic Model\nTo evaluate the different estimates from the heteroskedasticity model, we need to compare them to estimates drawn from a homoskedastic model with the same population coefficients. To make the comparisons ‚Äúfair‚Äù, so that we are only evaluating the effects of the heteroskedasticity, we also need to run this simulation using a residual standard error that is equal to the mean from the heteroskedastic simulation (i.e., \\(\\mathtt{sd\\neq1}\\) in the rnorm() function).\nCarry out 1000 trials of the simulation for the appropriate homoskedastic model. (Reminder: Be sure to use these same X values in this simulation as in the previous simulation.) For each trial, collect: (a) the estimate of the intercept, (b) the estimate of the slope, and (c) the estimate of the residual standard error. Report your syntax.\nComparing Results from the Two Simulations: Evaluating the Effects of Hetroskedasticity\nCreate a density plot of the distribution of intercept estimates. Show the density curve for both models on the same plot, differentiating the curves using color, linetype, or both. Also add a vertical line to this plot at the population value of the intercept.\nBased on your responses to Question 8, does the intercept estimate seem to be biased under heteroskedasticity? Explain.\nBased on your responses to Question 8, does the intercept estimate seem to be less efficient under heteroskedasticity? Explain.\nCreate a density plot of the distribution of slope estimates. Show the density curve for both models on the same plot, differentiating the curves using color, linetype, or both. Also add a vertical line to this plot at the population value of the slope\nBased on your responses to Question 11, does the slope estimate seem to be biased under heteroskedasticity? Explain.\nBased on your responses to Question 11, does the slope estimate seem to be less efficient under heteroskedasticity? Explain.\nCreate a density plot of the distribution of residual standard error estimates. Show the density curve for both models on the same plot, differentiating the curves using color, linetype, or both. Also add a vertical line to this plot at the population value of the residual standard error\nBased on your responses to Question 14, does the residual standard error estimate seem to be biased under heteroskedasticity? Explain.\nBased on your responses to Question 14, does the residual standard error estimate seem to be less efficient under heteroskedasticity? Explain.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-20T10:33:18-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-20-notes-simulating-from-the-regression-model/",
    "title": "üìù Simulating from the Regression Model",
    "description": "Notes",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-09-20",
    "categories": [
      "Notes"
    ],
    "contents": "\nHere are links to several script files and handouts that we will use in class:\nGenerating Data\nGenerating Random Data from a Regression Model is a script file that provides syntax for generating data from a given population regression model.\nSimulation 1: Simulating from a Regression Model\nSimulating from a Regression Model is a script file that provides syntax for carrying out a simulation to produce distributions of estimates from a regression model.\nVizualization of Simulating from a Regression Model is a handout visualizing the simulation process for generating data from a given population regression model.\nSimulation 2: Simulating from a Null Regression Model\nSimulating from a Null Regression Model is a script file that provides syntax for carrying out a simulation to produce distributions of estimates assuming certain parameters in the regression model are zero.\nVizualization of Simulating from a Null Regression Model is a handout visualizing the simulation process for generating data to produce distributions of estimates assuming certain parameters in the regression model are zero.\nResources\nHere are several resources to help your understanding of simulation. The chapters from Monte Carlo Simulation and Resampling Methods for Social Science should be accessible via the links after logging in with your x500 and password.\nProbability: Common probability distributions and how to compute with them.\nRandom Number Generation: Learn how to draw random numbers from different distributions. Also information about repeating processes in R, including writing your own functions, using for loops, and using if-else functions.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-20T13:18:51-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-18-reading-simulating-from-the-regression-model/",
    "title": "üìñ Simulating from the Regression Model",
    "description": "Reading",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-09-18",
    "categories": [
      "Reading"
    ],
    "contents": "\nRead the following chapters from Carsey, T. M., & Harden, J. J. (2014). Monte Carlo simulation and resampling methods for social science. Sage. It should be accessible via the link after logging in with your x500 and password.\nIntroduction: This chapter gives a short introduction to the use of simulation in the social sciences.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-20T13:18:20-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-09-notes-ols-regression/",
    "title": "üìù OLS Regression Using Matrices and Its Properties",
    "description": "Notes",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-09-09",
    "categories": [
      "Notes"
    ],
    "contents": "\nHere are links to several PDF handouts:\nSummation, Expectation, Variance, Covariance, and Correlation is a handout that provides several mathematical rules for working with sums, expectations, variances, covariances, and correlation.\nOLS Estimators and Their Properties is a handout that steps through estimating the OLS regression estimators and also derives some of the properties of those estimators\nAssumptions for OLS Regression and the Gauss-Markov Theorem is a handout that examines the assumptions underlying the Gauss-Markov theorem; the theorem showing that the OLS estimators are BLUE.\nStatistical Inference for the Regression Model is a handout working through how we carry out coefficient-level and model-level statistical inference.\nA Regression Example in Practice is a handout that walks through using matrix algebra to compute many of the things we are interested in as applied researchers. It also show the equivalent built-in R functions for obtaining this.\nThe handouts include more detail than I will cover in class. I will highlight some important ideas from each of them, and you can work through some of the mathematical derivation on your own if it is of interest.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-10T09:14:00-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-14-reading-ols/",
    "title": "üìñ OLS Regression using Matrices and its Properties",
    "description": "Reading",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-08-14",
    "categories": [
      "Reading"
    ],
    "contents": "\nRead the following (short) chapters in Matrix Algebra for Educational Scientists:\nSystems of Equations\nStatistical Application: Estimating Regression Coefficients\nIn class, we will be working through the ideas in the following chapters:\nImportant Matrices in Regression\nSums of Squares in Regression\nStandard Errors and Variance Estimates\nAssumptions of the Regression Model\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-20T13:12:23-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-11-worksheet-introduction-to-matrix-algebra/",
    "title": "üìù Introduction to Matrix Algebra",
    "description": "In-class worksheet",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-08-13",
    "categories": [
      "Worksheet"
    ],
    "contents": "\nDirections\nComplete the problems on this worksheet with your smallgroup. You may want to refer to the Matrix Algebra for Educational Scientists text.\nYou will likely be learning (or re-encountering) many new mathematical terms. It is a good idea to note and define all the vocabulary/terms that you encounter as you work through this worksheet. You may want to do this individually or create a shared document that you can all contribute to.\n\nProblems\nConsider the following matrices:\n\\[\n\\mathbf{A} = \\begin{bmatrix}3 & -2\\\\5 & 1\\end{bmatrix} \\quad \\mathbf{B} = \\begin{bmatrix}3 & -1\\\\-1 & 2\\end{bmatrix}\\quad \\mathbf{C} = \\begin{bmatrix}1 & 2 & 3\\\\0 & 1 & 2\\end{bmatrix}\n\\]\nMake sure everyone in your group can solve each of these problem by hand and using R.\nWhat are the dimensions of A? C?\nIs C a square matrix? Explain.\nFind the trace of A.\nFind the determinant of A.\nAdd A and B\nFind the transpose of C.\nBy referring to the dimensions, can you compute AC? How about CA?\nCompute AC.\nCompute BI\nCreate a \\(3\\times3\\) diagonal matrix whose trace is 10.\nHow do you know that B has an inverse? Explain.\nCompute \\(\\mathbf{B}^{-1}\\)\nCreate a \\(3\\times3\\) matrix that has rank 2. Verify this using R.\nCreate a \\(3\\times3\\) matrix that is symmetric and is not I.\nSolve the system of linear equations using algebra (e.g., substitution, elimination) and then solve them using matrix methods (with R). To do this you will need to read the Systems of Equations chapter in Matrix Algebra for Educational Scientists.\n\\[\n\\begin{split}\nx + y + z &= 2 \\\\\n6x - 4y + 5z &= 31 \\\\\n5x + 2y + 2z &= 13\n\\end{split}\n\\]\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-07T15:54:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-12-reading-introduction-to-matrix-algebra/",
    "title": "üìñ Introduction to Matrix Algebra",
    "description": "Review some common mathematical ideas of matrix algebra.",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-08-12",
    "categories": [
      "Reading"
    ],
    "contents": "\nRead the following (short) chapters in Matrix Algebra for Educational Scientists:\nIntroduction\nData Structures\nVectors\nVector Operations\nIn class, we will be working through some problems to cement these ideas. We will also examine and work through some problems related to matrix operations, so you could also read through:\nMatrices\nMatrix Addition and Subtraction\nMatrix Multiplication\nMatrix Transposition\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-13T08:51:11-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-09-assignment-01/",
    "title": "üêß Assignment 01",
    "description": "Matrix Algebra for Linear Regression",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-08-09",
    "categories": [
      "Assignments"
    ],
    "contents": "\nThe goal of this assignment is to give you experience using matrix algebra to compute various analytic output for regression. In this assignment, you will use the data given below that includes measurements for 10 countries on: infant mortality rate per 1000 live births (infant), the per-capita income (pci) and world region (region) of the country.\n\ncountry\ninfant\npci\nregion\nAlgeria\n86.3\n400\nAfrica\nBolivia\n60.4\n200\nAmericas\nBurundi\n150.0\n68\nAfrica\nDominican Republic\n48.8\n406\nAmericas\nKenya\n55.0\n169\nAfrica\nMalawi\n148.3\n130\nAfrica\nNicaragua\n46.0\n507\nAmericas\nParaguay\n38.6\n347\nAmericas\nRwanda\n132.9\n61\nAfrica\nTrinidad & Tobago\n26.2\n732\nAmericas\n\n\n\n\n\n\nInstructions\nSubmit a printed document of your responses to the following questions. Please adhere to the following guidelines for further formatting your assignment:\nAll plots should be resized so that they do not take up more room than necessary.\nAll figures and tables should have a name (e.g., Figure 1) and an appropriate caption.\nIn questions that ask you to ‚Äúuse matrix algebra‚Äù to solve the problem, you can either show your syntax and output from carrying out the matrix operations, or you can use Equation Editor to input the matrices involved in your calculations.\nThis assignment is worth 20 points.\n\nUnstandardized Regression\nYou will be fitting the model lm(infant ~ 1 + pci + region + pci:region). Within this model, use dummy coding to encode the region predictor and make Americas the reference group.\nWrite out the elements of the matrix \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\), where \\(\\mathbf{X}\\) is the design matrix.\nDoes \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) have an inverse? Explain.\nCompute (using matrix algebra) and report the vector of coefficients, b for the OLS regression.\nCompute (using matrix algebra) and report the variance‚Äìcovariance matrix of the coefficients.\nUse the values from b (Question 3) and from the variance‚Äìcovariance matrix you reported in the previous question to find the 95% CI for the coefficient associated with the main-effect of PCI. (Hint: If you need to refresh yourself on how CIs are computed, see here.)\nCompute (using matrix algebra) and report the hat-matrix, H. Also show how you would use the values in the hat-matrix to find \\(\\hat{y}_1\\) (the predicted value for Algeria).\nCompute (using matrix algebra) and report the vector of residuals, e.\nCompute (using matrix algebra) and report the estimated value for the RMSE.\nGiven the assumptions of the OLS model and the RMSE estimate you computed in the previous question, compute and report the variance‚Äìcovariance matrix of the residuals.\n\nANOVA Decomposition\nIn this section you will be re-creating the output from the ANOVA decomposition for the model fitted in the previous section.\nCompute (using matrix algebra) and report the model, residual, and total sum of squares terms in the ANOVA decomposition table. (2pts)\nCompute (using matrix algebra) and report the model, residual, and total degrees of freedom terms in the ANOVA decomposition table. (2pts)\nUse the values you obtained in Questions 11 and 12 to compute the model and residual mean square terms.\nUse the mean square terms you found in Question 13 to compute the F-value for the model (i.e., to test \\(H_0:\\rho^2=0\\)). Also compute the p-value associated with this F-value. (Hint: If you need to refresh yourself on how F-values or p-values are computed, see here.)\n\nRegression: Effects-Coding\nNow consider fitting the model to the data to examine whether there is an effect of region (no other predictors) on infant mortality. In this model, we will use effects-coding to encode the region variable (see here). This model is often expressed as:\n\\[\n\\mathrm{Infant~Mortality}_i = \\mu + \\alpha_{\\mathrm{Region}} + \\epsilon_i \n\\]\nWrite out the design matrix that would be used to fit this model.\nCompute (using matrix algebra) and report the vector of coefficients, b, from the OLS regression.\nCompute (using matrix algebra) and report the variance‚Äìcovariance matrix for the coefficients.\nExplain why the sampling variances for the coefficients are the same and why the sampling covariance is zero by referring to computations produced in the matrix algebra. (2pts)\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-13T15:54:27-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-09-message-to-students/",
    "title": "A Message to Students",
    "description": "A message to students.",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-08-09",
    "categories": [],
    "contents": "\nIn this class, we will work together to develop a learning community that is inclusive and respectful, and where every student is supported in the learning process. As a class full of diverse individuals (reflected by differences in race, culture, age, religion, gender identity, sexual orientation, socioeconomic background, abilities, professional goals, and other social identities and life experiences) I expect that different students may need different things to support and promote their learning.\nThe TAs and I will do everything we can to help with this, but, as we only know what we know, we need you to communicate with us if things are not working for you or you need something we are not providing. I hope you all feel comfortable in helping to promote an inclusive classroom through respecting one another‚Äôs individual differences, speaking up, and challenging oppressive/problematic ideas. Finally, I look forward to learning from each of you and the experiences you bring to the class.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-09T16:10:51-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-09-prerequisites-and-resources/",
    "title": "Prerequisites and Resources",
    "description": "Prerequisite knowledge and resources for brushing up on that knowledge.",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-08-09",
    "categories": [
      "Resources"
    ],
    "contents": "\nPrerequisites\nThe pre-requisites for this course are EPsy 8251 and EPsy 8252. Prerequisite knowledge include topics from a basic statistics course:\nFoundational topics in data analysis;\nDesign (e.g., random assignment and random sampling)\nDescriptive statistics and plots\nOne- and two-sample tests\n\nAnd, topics from EPsy 8251: Methods in Data Analysis for Educational Research I:\nStatistical Computation\nUsing R\nData wrangling/manipulation\nPlotting\n\nCorrelation;\nSimple regression analysis;\nModel-level and coefficient-level interpretation\nOrdinary least squares estimation\nStandardized regression\nPartitioning sums of squares\nModel-level and coefficient-level inference\nAssumption checking/residual analysis\n\nMultiple linear regression\nModel-level and coefficient-level interpretation and inference\nAssumption checking/residual analysis\nWorking with categorical predictors (including adjusting p-values for multiple tests)\nInteraction effects\n\nAnd topics from EPsy 8252: Methods in Data Analysis for Educational Research II:\nDealing with nonlinearity;\nQuadratic effects\nLog-transformations\n\nProbability distributions;\nProbability density\n\nMaximum likelihood estimation;\nModel selection;\nInformation criteria\n\nLinear mixed-effects models (cross-sectional/longitudinal)\nBasic ideas of mixed-effects models\nFitting models with random-intercepts and random-slopes\nAssumptions\nLikelihood ratio tests\n\nGeneralized linear models\nLogistic models\n\n\nResources\nFor the topics listed, students would be expected to be able to carry out an appropriate data analysis and properly interpret the results. It is also assumed that everyone enrolled in the course has some familiarity with using R. If you need a refresher on any of these topics, see:\nComputational Toolkit for Educational Scientists\nStatistical Modeling and Computation for Educational Scientists [EPsy 8251 material]\nEPsy 8252 website\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-10T11:14:10-05:00",
    "input_file": {}
  }
]
