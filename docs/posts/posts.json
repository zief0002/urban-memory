[
  {
    "path": "posts/2021-10-04-reading-diagnosing-collinearity/",
    "title": "üìñ Diagnosing Collinearity",
    "description": "Reading",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-10-04",
    "categories": [
      "Reading"
    ],
    "contents": "\nRead the following:\nRodriguez, M., & Zieffler, A. (2021). Eigenvalues and Eigenvectors. In Matrix algebra for educational scientists.\n\nAdditional Resources\nCook, D. (2019). How to use a tour to check if your model suffers from multicollinearity. Personal blog.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-10-04T12:57:42-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-30-notes-diagnosing-collinearity/",
    "title": "üìù Diagnosing Collinearity",
    "description": "A brief introduction to empirical diagnostics to detect collinearity. Example taken from @Chatterjee:2012.",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-09-30",
    "categories": [
      "Notes"
    ],
    "contents": "\n\n\n\nIn 1964, the US Congress passed the Civil Rights Act and also ordered a survey of school districts to evaluate the availability of equal educational opportunity in public education. The results of this survey were reported on in Coleman et al. (1966) and Mosteller & Moynihan (1972). We will use a subset of the data collected (in equal-educational-opportunity.csv) to mimic one of the original regression analyses performed; examining whether the level of school facilities was an important predictor of student achievement after accounting for the variation in faculty credentials and peer influence.\n[CSV]\n[R Script File]\nThe data come from a random sample of 70 schools in 1965. The variables, which have all been mean-centered and standardized, include:\nachievement: Measurement indicating the student achievement level\nfaculty: Measurement indicating the faculty‚Äôs credentials\npeer: Measurement indicating the influence of peer groups in the school\nschool: Measurement indicating the school facilities (e.g., building, teaching materials)\n\n\n# Load libraries\nlibrary(broom)\nlibrary(car)\nlibrary(corrr)\nlibrary(tidyverse)\nlibrary(patchwork)\n\n# Read in data\neeo = read_csv(\"~/Documents/github/epsy-8264/data/equal-education-opportunity.csv\")\nhead(eeo)\n\n\n# A tibble: 6 √ó 4\n  achievement faculty    peer school\n        <dbl>   <dbl>   <dbl>  <dbl>\n1      -0.431   0.608  0.0351  0.166\n2       0.800   0.794  0.479   0.534\n3      -0.925  -0.826 -0.620  -0.786\n4      -2.19   -1.25  -1.22   -1.04 \n5      -2.85    0.174 -0.185   0.142\n6      -0.662   0.202  0.128   0.273\n\n# Source the residual_plots() function from the script file\nsource(\"../../scripts/residual_plots.R\")\n\n\n\n\nRegression Analysis\nTo examine the RQ, the following model was posited:\n\\[\n\\mathrm{Achievement}_i = \\beta_0 + \\beta_1(\\mathrm{Faculty}_i) + \\beta_2(\\mathrm{Peer}_i) + \\beta_3(\\mathrm{School}_i) + \\epsilon_i\n\\]\n\n\n\nFigure 1: Residual plots for the model that includes the main effects of faculty credentials, influence of peer groups, and measure of school facilities to predict variation in student achievement.\n\n\n\n\n\n# Index plots of several regression diagnostics\ninfluenceIndexPlot(lm.1)\n\n\n\n\nFigure 2: Diagnostic plots for the model that includes the main effects of faculty credentials, influence of peer groups, and measure of school facilities to predict variation in student achievement.\n\n\n\nSchool 28 may be problematic, but removing this observation (work not shown) made little improvement in the residual plots. As such, School 28 was retained in the data. As the assumptions seem reasonably met, we next look to the model-level and coefficient-level output:\n\n\n# Model-level information\nprint(glance(lm.1), width = Inf)\n\n\n# A tibble: 1 √ó 12\n  r.squared adj.r.squared sigma statistic p.value    df logLik   AIC\n      <dbl>         <dbl> <dbl>     <dbl>   <dbl> <dbl>  <dbl> <dbl>\n1     0.206         0.170  2.07      5.72 0.00153     3  -148.  306.\n    BIC deviance df.residual  nobs\n  <dbl>    <dbl>       <int> <int>\n1  318.     283.          66    70\n\n# Coefficient-level information\ntidy(lm.1, conf.int = 0.95)\n\n\n# A tibble: 4 √ó 7\n  term        estimate std.error statistic p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>   <dbl>    <dbl>     <dbl>\n1 (Intercept)  -0.0700     0.251    -0.279   0.781   -0.570     0.430\n2 faculty       1.10       1.41      0.781   0.438   -1.72      3.92 \n3 peer          2.32       1.48      1.57    0.122   -0.635     5.28 \n4 school       -2.28       2.22     -1.03    0.308   -6.71      2.15 \n\nExamining this information we find:\n20% of the variation in student achievement is explained by the model; which is statistically significant \\(F(3, 66)=5.72\\); \\(p=0.002\\).\nHowever, none of the individual coefficients are statistically significant!\nThis is a bit of a paradox since we rejected the model-level null hypothesis that \\(H_0:\\beta_{\\mathrm{Faculty~Credentials}}=\\beta_{\\mathrm{Peer~Influence}}=\\beta_{\\mathrm{School~Facilities}}=0\\), yet the coefficient-level results are consistent with \\(H_0:\\beta_{\\mathrm{Faculty~Credentials}}=0\\), \\(H_0:\\beta_{\\mathrm{Peer~Influence}}=0\\), and \\(H_0:\\beta_{\\mathrm{School~Facilities}}=0\\). These inconsistencies between the model- and coefficient-level results are typical when there is collinearity in the model.\n\nWhat is Collinearity?\nRecall from our introduction to matrix algebra that two vectors are collinear if they span the same subspace. In regression, collinearity occurs when any of the columns of the design matrix, X, is a perfect linear combination of the other columns:\n\\[\n\\mathbf{X_j} = c_0(\\mathbf{1}) + c_1\\mathbf{X_1} + c_2\\mathbf{X_2} + c_3\\mathbf{X_3} + \\ldots + c_k\\mathbf{X_k}\n\\]\nand the constants, \\(c_1, c_2, c_3,\\ldots, c_k\\) are not all 0. In this situation, X is not of full column rank, and the \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) matrix is singular.\n\nSometimes ‚Äòcollinearity‚Äô is referred to as ‚Äòmulticollinearity‚Äô. The two terms are synonomous.\nEffects of Collinearity\nIf the design matrix is not of full rank, and \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) is singular, then the OLS normal equations do not have a unique solution. Moreover, the sampling variances for the coefficient are all infinitely large. To understand why this is the case, we can examine one formula for the sampling variance of a slope in a multiple regression:\n\\[\n\\mathrm{Var}(B_j) = \\frac{1}{1 - R^2_j} \\times \\frac{\\sigma^2_\\epsilon}{(n-1)S^2_j}\n\\]\nwhere\n\\(R^2_j\\) is the squared multiple correlation for the regression of \\(X_j\\) on the the other predictors;\n\\(S^2_j\\) is the sample variance of predictor \\(X_j\\) defined by \\(S^2_j = \\dfrac{\\sum(X_{ij}-\\bar{X}_j)^2}{n-1}\\);\n\\(\\sigma^2_{\\epsilon}\\) is the variance of the residuals based on regressing \\(Y\\) on all the \\(X\\)‚Äôs\n\nRecall that the multiple correlation is the correlation between the outcome and the predicted values.\nThe first term in this product is referred to as the variance inflation factor (VIF). When one of the predictors is perfectly collinear with the others, the value of \\(R^2_j\\) is 1 and the VIF is infinity. Thus the sampling variance of \\(B_j=\\infty\\).\n\nPerfect Collinearity in Practice: Model Mis-specification\nIn practice, it is unlikely that you will have exact collinearity. When it does happen it is often the result of mis-formulating the model (e.g., including dummy variables in the model for all levels of a categorical variable, as well as the intercept). As an example of this, imagine that you were creating the design matrix for a regression model that included occupational status (employed/not employed) to predict some outcome for 5 cases.\n\n\n# Create design matrix\nX = data.frame(\n  b_0 = rep(1, 5),\n  employed = c(1, 1, 0, 0, 1),\n  not_employed = c(0, 0, 1, 1, 0)\n)\n\n# View design matrix\nX\n\n\n  b_0 employed not_employed\n1   1        1            0\n2   1        1            0\n3   1        0            1\n4   1        0            1\n5   1        1            0\n\nThe columns in this design matrix are collinear because we can express any one of the columns as a linear combination of the others. For example,\n\\[\nb_0 = 1(\\mathrm{employed}) + 1(\\mathrm{not~employed})\n\\]\nChecking the rank of this matrix, we find that this matrix has a rank of 2. Since there are three columns, X is not full column rank; it is rank deficient.\n\n\nMatrix::rankMatrix(X)\n\n\n[1] 2\nattr(,\"method\")\n[1] \"tolNorm2\"\nattr(,\"useGrad\")\n[1] FALSE\nattr(,\"tol\")\n[1] 1.110223e-15\n\nIncluding all three coefficients in the model results in overparameterization. The simple solution here is to drop one of the predictors from the model. This is why we only include a single dummy variable in a model that includes an intercept for a dichotomous categorical predictor.\n\nIncluding the intercept is not imperative, although it has a useful interpretation when using dummy coding. One could also include the two dummy-coded predictors and omit the intercept. This gives the means for the two groups, but does not provide a comparison of those means.\n\n\n# Create vector of outcomes\nY = c(15, 15, 10, 15, 30)\n\n# Create data frame of Y and X\nmy_data = cbind(Y, X)\nmy_data\n\n\n   Y b_0 employed not_employed\n1 15   1        1            0\n2 15   1        1            0\n3 10   1        0            1\n4 15   1        0            1\n5 30   1        1            0\n\n# Coefficients (including all three terms)\ncoef(lm(Y ~ 1 + employed + not_employed, data = my_data))\n\n\n (Intercept)     employed not_employed \n        12.5          7.5           NA \n\n# Coefficients (omitting intercept)\ncoef(lm(Y ~ -1 + employed + not_employed, data = my_data))\n\n\n    employed not_employed \n        20.0         12.5 \n\nIf you overparameterize a model with lm(), one or more of the coefficients will not be estimated (the last parameters entered in the model).\n\nConstraining some parameters is another way to produce a full rank design matrix. For example the ANOVA model has a constraint that the sum of the effect-coded variable is 0. This constraint ensures that the design matrix will be of full rank.\n\nNon-Exact Collinearity\nIt is more likely, in practice, that you will have less-than-perfect collinearity, and that this will have an adverse effect on the computational estimates of the coefficients‚Äô sampling variances. Again, we look toward how the sampling variances for the coefficent‚Äôs are computed:\n\\[\n\\mathrm{Var}(B_j) = \\frac{1}{1 - R^2_j} \\times \\frac{\\sigma^2_\\epsilon}{(n-1)S^2_j}\n\\]\nWhen the predictors are completely independent, all of the columns of the design matrix will be orthogonal and the correlation between \\(X_j\\) and the other \\(X\\)s will be 0. In this situation, the VIF is 1 and the second term in the product completely defines the sampling variance. This means that the sampling variance is a function of the model‚Äôs residual variance, sample size, and the predictor‚Äôs variance‚Äîthe factors we typically think of affecting the sampling variance of a coefficient.\nIn cases where the columns in ths design matrix are not perfectly orthogonal, the correlation between \\(X_j\\) and the other \\(X\\)s is larger than 0. (Perfect collinearity results in \\(R^2_j=1\\).) For these situations, the VIF has a value that is greater than 1. When this happens the VIF acts as a multiplier of the second term, inflating the the sampling variance and reducing the precision of the estimate (i.e., increasing the uncertainty).\nHow much the uncertainty in the estimate increases is a function of how correlated the predictors are. Here we can look at various multiple correlations (\\(R_j\\)) between \\(X_j\\) and the predicted values from using the other \\(X\\)‚Äôs to predict \\(X_j\\).\n\n\nTable 1: Impact of various \\(R_j\\) values on the VIF and size of the CI for \\(B_j\\).\n\n\n\\(R_j\\)\n\n\nVIF\n\n\nCI Factor\n\n\n0.0\n\n\n1.00\n\n\n1.00\n\n\n0.1\n\n\n1.01\n\n\n1.01\n\n\n0.2\n\n\n1.04\n\n\n1.02\n\n\n0.3\n\n\n1.10\n\n\n1.05\n\n\n0.4\n\n\n1.19\n\n\n1.09\n\n\n0.5\n\n\n1.33\n\n\n1.15\n\n\n0.6\n\n\n1.56\n\n\n1.25\n\n\n0.7\n\n\n1.96\n\n\n1.40\n\n\n0.8\n\n\n2.78\n\n\n1.67\n\n\n0.9\n\n\n5.26\n\n\n2.29\n\n\n1.0\n\n\nInf\n\n\nInf\n\n\nFor example, a multiple correlation of 0.7 results in a VIF of 1.96, which in turn means that the CI (which is based on the square root of the sampling variance) will increase by a factor of 1.4. This inflation increases the uncertainty of the estimate making it harder to make decisions or understand the effect of \\(B_j\\).\nTo sum things up, while perfect collinearity is rare in practice, less-than-perfect collinearity is common. In these cases the VIF will be less than 1, but can still have an adverse effect on the sampling variances; sometimes making them quite large.\n\nIdentifying Collinearity\nIn our case study example, we were alerted to the possible collinearity by finding that the predictors jointly were statistically significant, but that each of the individual predictors were not. Other signs that you may have collinearity problems are:\nLarge changes in the size of the estimated coefficients when variables are added to the model;\nLarge changes in the size of the estimated coefficients when an observation is added or deleted;\nThe signs of the estimated coefficients do not conform to their prior substantively hypothesized directions;\nLarge SEs on variables that are expected to be important predictors.\n\nCollinearity Diagnostics\nWe can also empirically diagnose problematic collinearity in the data (D. A. Belsley, 1991; D. Belsley et al., 1980). Before we do, however, it is important that the functional form of the model has been correctly specified. Since, a model needs to be specified before we can estimate coefficients or their sampling variances, and collinearity produces unstable estimates of these estimates, collinearity should only be investigated after the model has been satisfactorily specified.\nBelow we will explore some of the diagnostic tools available to an applied researcher.\n\nHigh Correlations among Predictors\nCollinearity can sometimes be anticipated by examining the pairwise correlations between the predictors. If the correlation between predictors is large, this might be indicative of collinearity problems.\n\n\neeo %>%\n  select(faculty, peer, school) %>%\n  correlate()\n\n\n# A tibble: 3 √ó 4\n  term    faculty   peer school\n  <chr>     <dbl>  <dbl>  <dbl>\n1 faculty  NA      0.960  0.986\n2 peer      0.960 NA      0.982\n3 school    0.986  0.982 NA    \n\nIn this example, all three of the predictors are highly correlated with one another. This is likely a good indicator that their may be problems in the estimation of coefficients, inflated standard errors, or both; especially given that the correlations are all very high. Unfortunately the source of collinearity may be due to more than just the simple relationships among the predictors. As such, just examining the pairwise correlations is not enough to detect collinearity (although it is a good first step).\n\nRegress each Predictor on the Other Predictors\nSince collinearity is defined as linear dependence within the set of predictors, a better way to diagnose collinearity than just examining the pairwise correlation coefficients is to regress each of the predictors on the remaining predictors and evaluate the \\(R^2\\) value. If all the \\(R^2\\) values are close to zero there is no collinearity problems. If one or more of the \\(R^2\\) values are close to 1, there is a collinearity problem.\n\n\n# Use faculty as outcome; obtain R2\nsummary(lm(faculty ~ 1 + peer + school, data = eeo))$r.squared\n\n\n[1] 0.9733906\n\n# Use faculty as outcome; obtain R2\nsummary(lm(peer ~ 1 + faculty + school, data = eeo))$r.squared\n\n\n[1] 0.9669002\n\n# Use faculty as outcome; obtain R2\nsummary(lm(school ~ 1 + faculty + peer, data = eeo))$r.squared\n\n\n[1] 0.9879743\n\nAll three \\(R^2\\) values are quite high, which is indicative of collinearity.\nOne shortcoming with this method of diagnosing collinearity is that when the predictor space is large, you would need to look at the \\(R^2\\) values from several models. And, while this could be automated in an R function, there are other common methods that allow us to diagnose collinearity.\nWe will examine three additional common methods statisticians use to empirically detect collinearity: (1) computing variance inflation factors for the coefficients; (2) examining the eigenvalues of the correlation matrix; and (3) examining the condition indices of the correlation matrix.\nVariance Inflation Factor (VIF)\nPerhaps the most common method applied statisticians use to diagnose collinaerity is to compute and examine variance inflation factors. Recall that the variance inflation factor (VIF) is an indicator of the degree of collinearity, where VIF is:\n\\[\n\\mathrm{VIF} = \\frac{1}{1 - R^2_j}\n\\]\nThe VIF impacts the size of the variance estimates for the regression coefficients, and as such, can be used as a diagnostic of collinearity. In practice, since it is more conventional to use the SE to measure uncertainty, it is typical to use the square root of the VIF as a diagnostic of collinearity in practice. The square root of the VIF expresses the proportional change in the CI for the coefficients. We can use the vif() function from the car package to compute the variance inflation factors for each coefficient.\n\n\n# VIF\nvif(lm.1)\n\n\n faculty     peer   school \n37.58064 30.21166 83.15544 \n\n# Square root of VIF\nsqrt(vif(lm.1))\n\n\n faculty     peer   school \n6.130305 5.496513 9.118960 \n\nAll three coefficients are impacted by VIF. The SEs for these coefficients are all more than five times as large as they would be if the predictors were independent.\nRemember, the VIF can range from 1 (independence among the predictors) to infinity (perfect collinearity). There is not consensus among statisticians about how high the VIF has to be to constitute a problem. Some references cite \\(\\mathrm{VIF}>10\\) as problematic (which increases the size of the CI for the coefficient by a factor of over three); while others cite \\(\\mathrm{VIF}>4\\) as problematic (which increases the size of the CI for the coefficient by a factor of two). As you consider what VIF value to use as an indicator of problematic inflation, it is more important to consider what introducing that much uncertainty would mean in your substantive problem. For example, would you be comfortable with tripling the uncertainty associated with the coefficient? What about doubling it? Once you make that decision, you can determine your VIF cutoff.\nThere are several situations in which high VIF values are expected and not problematic:\nThe variables with high VIFs are control variables, and the variables of interest do not have high VIFs. Since we would not be interested in inference around the control variables, high VIF values on those variables would not\nThe high VIFs are caused by the inclusion of powers or products of other variables. The p-value for a product term is not affected by the multicollinearity. Centering predictors prior to creating the powers or the products will reduce the correlations, but the p-value the products will be exactly the same whether or not you center. Moreover the results for the other effects will be the same in either case indicating that multicollinearity has no adverse consequences.\nThe variables with high VIFs are indicator (dummy) variables that represent a categorical variable with three or more categories. This is especially true when the reference category used has a small proportion of cases. In this case, p-values for the indicator variables may be high, but the overall test that all indicators have coefficients of zero is unaffected by the high VIFs. And nothing else in the regression is affected. To avoid the high VIF values in this situaton, just choose a reference category with a larger proportion of cases.\n\nEigenvalues of the Correlation Matrix\nA second common method of evaluating collinearity is to compute and evaluate the eigenvalues of the correlation matrix for the predictors. Recall that each square (\\(k \\times k\\)) matrix has a set of k scalars, called eigenvalues (denoted \\(\\lambda\\)) associated with it. These eigenvalues can be arranged in descending order such that,\n\\[\n\\lambda_1 \\geq \\lambda_2 \\geq \\lambda_3 \\geq \\ldots \\geq \\lambda_k\n\\]\nBecause any correlation matrix is a square matrix, we can find a corresponding set of eigenvalues for the correlation matrix. If any of these eigenvalues is exactly equal to zero, it indicates a linear dependence among the variables making up the correlation matrix.\nAs a diagnostic, rather than looking at the size of all the eigenvalues, we compute the sum of the reciprocals of the eigenvalues:\n\\[\n\\sum_{i=1}^k \\frac{1}{\\lambda_i}\n\\]\nIf the predictors are orthogonal to one another (independent) then \\(\\lambda_i = 1\\) and the sum of the reciprocal values will be equal to the number of predictors, \\(\\sum_{i=1}^k \\frac{1}{\\lambda_i} = k\\).\nIf the predictors are collinear with one another (dependent) then \\(\\lambda_i = 0\\) and the sum of the reciprocal values will be equal to infinity, \\(\\sum_{i=1}^k \\frac{1}{\\lambda_i} = \\infty\\).\nWhen there is nonperfect collinearity then \\(0 < \\lambda_i < 1\\), and the sum of the reciprocal values will be greater than the number of predictors, \\(\\sum_{i=1}^k \\frac{1}{\\lambda_i} > k\\).\n\nIn an orthogonal matrix, the eigenvalues are all \\(\\pm1\\), but since the correlation matrix is positive semidefinite, the eigenvalues are all \\(+1\\).\nLarger sums of the reciprocal values of the eigenvalues is indicative of higher degrees of collinearity. In practice, we might use some cutoff to indicate when the collinearity is problematic. One such cutoff used is, if the sum is greater than five times the number of predictors, it is a sign of collinearity.\n\\[\n\\mathrm{IF} \\quad \\sum_{i=1}^k \\frac{1}{\\lambda_i} > 5k \\quad \\mathrm{THEN} \\quad \\mathrm{collnearity~is~a~problem}\n\\]\n\nIn practice, perfect collinearity is rare, but near perfect collinearity can exist and is indicated when at least one of the eigenvalues is near zero, and is quite a bit smaller than the others.\nUsing R to Compute the Eigenvalues of the Correlation Matrix\nBecause collinearity indicates dependence among the predictors, we would want to compute the eigenvalues for the correlation matrix of the predictors (do not include the outcome when computing this matrix). We can then use the eigen() function to compute the eigenvalues of a square matrix.\nIn previous classes, I have been using the correlate() function from the {corrr} package to produce correlation matrices. This function produces a formatted output that is nice for displaying the correlation matrix, but, because of its formatting, is not truly a matrix object. Instead, we will use the cor() function, which produces a matrix object, to produce the correlation matrix.\n\n\n# Correlation matrix of predictors\nr_xx = cor(eeo[c(\"faculty\", \"peer\", \"school\")])\nr_xx\n\n\n          faculty      peer    school\nfaculty 1.0000000 0.9600806 0.9856837\npeer    0.9600806 1.0000000 0.9821601\nschool  0.9856837 0.9821601 1.0000000\n\nOnce we have the correlation matrix, we can use the eigen() function to compute the eigenvalues (and eigenvectors) of the inputted correlation matrix.\n\n\n# Compute eigenvalues and eigenvectors\neigen(r_xx)\n\n\neigen() decomposition\n$values\n[1] 2.951993158 0.040047507 0.007959335\n\n$vectors\n           [,1]        [,2]       [,3]\n[1,] -0.5761385  0.67939712 -0.4544052\n[2,] -0.5754361 -0.73197527 -0.3648089\n[3,] -0.5804634  0.05130072  0.8126687\n\n# Sum of reciprocal of eigenvalues\nsum(1 / eigen(r_xx)$values)\n\n\n[1] 150.9477\n\nWe compare the sum of the reciprocal of the eigenvalues to five times the number of predictors; \\(5 \\times 3 =15\\). Since this sum is greater than 15, we would conclude that there is a collinearity problem for this model.\n\nCondition Indices\nA third common diagnostic measure of collinearity, called condition indices, is also based on the eigenvalues of the model predictors‚Äô correlation matrix. Each eigenvalue has an associated value called its condition index and denoted \\(\\kappa\\). The jth condition index is defined as\n\\[\n\\kappa_j = \\sqrt{\\frac{\\lambda_1}{\\lambda_j}}\n\\]\nfor \\(j=1,2,3,\\ldots,k\\), where \\(\\lambda_1\\) is the first (largest) eigenvalue and \\(\\lambda_j\\) is the jth eigenvalue.\nThe first condition index, \\(\\kappa_1\\), will always be equal to 1, and the other condition indices will be larger than one. The largest condition index will be,\n\\[\n\\kappa_k = \\sqrt{\\frac{\\lambda_1}{\\lambda_k}}\n\\]\nwhere \\(\\lambda_k\\) is the smallest eigenvalue. This is referred to as the condition number of the correlation matrix. If the condition number is small, it indicates that the predictors are not collinear, whereas large condition numbers are evidence supporting collinearity.\nFrom empirical work, condition numbers that exceed 15 are typically problematic (this indicates that the maximum eigenvalue is more than 225 times greater than the maximum eigenvalue). When the condition number exceeds 30, corrective action will almost surely need to be taken. Here we compute the condition indices and the condition number for our empirical example.\n\n\n# Sort eigenvalues from largest to smallest\nlambda = sort(eigen(r_xx)$values, decreasing = TRUE)\n\n# View eigenvalues\nlambda\n\n\n[1] 2.951993158 0.040047507 0.007959335\n\n# Compute condition indices\nsqrt(max(lambda) / lambda)\n\n\n[1]  1.000000  8.585586 19.258359\n\nThe condition number of the correlation matrix, \\(\\kappa = 19.26\\), suggests strong collinearity among the predictors.\n\nFixing Collinearity in Practice\nAlthough there are several solutions to ‚Äúfix‚Äù collinearity in practice, none are a magic bullet. Here are three potential fixes:\nRe-specify the model\nDrop one (or more) of the collinear predictors‚ÄîThis changes what you are controlling for;\nCombine collinear predictors;\n\nBiased estimation\nTrade small amount of bias for a reduction in coefficient variability;\n\nIntroduce prior information about the coefficients\nThis can be done formally in the analysis (e.g., Bayesian analysis);\nIt can be used to give a different model specification.\n\nNote that although collinearity is a data problem, the most common fixes in practice are to change the model. In upcoming notes, we will look at methods for combining collinear predictors and performing biased estimation.\nFor example, we could alleviate the collinearity by dropping any two of the predictors and re-fitting the model with only one predictor. This would fix the problem, but would be unsatisfactory because the resulting model would not allow us to answer the research question.\nThe highly correlated relationships between the predictors is an inherent characteristic of the data generating process we are studying. This makes it difficult to estimate the individual effects of the predictors. Instead, we could look for underlying causes that would explain the relationships we found among the predictors and perhaps re-formulate the model using these underlying causes.\n\n\n\n\n\n\nBelsley, D. A. (1991). Conditioning diagnostics, collinearity and weak data in regression. John Wiley & Sons.\n\n\nBelsley, D., Kuh, E., & Welsch, R. (1980). Regression diagnostics. Wiley.\n\n\nChatterjee, S., & Hadi, A. S. (2012). Regression analysis by example. Wiley.\n\n\nColeman, J. S., Cambell, E. Q., Hobson, C. J., McPartland, J., Mood, A. M., Weinfield, F. D., & York, R. L. (1966). Equality of educational opportunity. U.S. Government Printing Office.\n\n\nMosteller, F., & Moynihan, D. F. (1972). On equality of educational opportunity. Random House.\n\n\n\n\n",
    "preview": "posts/2021-09-30-notes-diagnosing-collinearity/distill-preview.png",
    "last_modified": "2021-10-04T12:44:41-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-29-assignment-04/",
    "title": "üêâ Assignment 04",
    "description": "Using WLS to Model Data with Outliers",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-09-29",
    "categories": [
      "Assignments"
    ],
    "contents": "\nThe goal of this assignment is to give you experience using methods for estimating regression results under violation of homoskedasticity. You will again use the data from the file stack-1979.csv to evaluate the hypothesis from political science that suggests that countries that have a stronger Socialist party have less income inequality.\n[CSV]\n[Data Codebook]\n\nInstructions\nSubmit a printed document of your responses to the following questions. Please adhere to the following guidelines for further formatting your assignment:\nAll plots should be resized so that they do not take up more room than necessary.\nAll figures and tables should have a name (e.g., Figure 1) and an appropriate caption.\nIn questions that ask you to ‚Äúuse matrix algebra‚Äù to solve the problem, you can either show your syntax and output from carrying out the matrix operations, or you can use Equation Editor to input the matrices involved in your calculations.\nThis assignment is worth 20 points.\n\nExploratory Analysis\nStart by creating a scatterplot to examine the relationship between socialist party strength and income inequality (outcome).\nAre there observations that look problematic in this plot? If so, identify the country(ies).\nFit a linear model regressing income inequality on socialist party strength. Examine and report a set of regression diagnostics that allow you to identify any observations that are regression outliers.\n\nWeighted Least Squares Estimation\nRather than removing regression outliers from the data, we can instead fit a model that accounts for these observations. For example, fitting a model that allows for higher variance at \\(x\\)-values that have outliers. With higher variances, we would expect more extreme observations because of the increased variance. The WLS model allows for heteroskedasticity and can be used to model data that have extreme observations.\nCompute the empirical weights that you will use in the WLS estimation. Report the weight for the United States. (Hint: We do not know the true variances in the population.)\nFit the WLS model. Report the fitted equation.\nBased on the model results, what is suggested about the research hypothesis that countries with more socialist tendencies have less income inequality?\nCreate a scatterplot that shows the relationship between socialist party strength and income inequality. Include the country names as labels (or instead of the points). Include both the OLS and WLS regression lines on this plot.\nBased on the plot, comment on how the residuals from the WLS model compare to the residuals from the OLS model.\nBased on your response to Question #8, how will the model-level \\(R^2\\) value from the WLS model compare to the model-level \\(R^2\\) from the OLS model. Explain.\nThe mathematical formulaa for computing the studentized residuals for both the OLS and WLS models is given below. Compute and report the studentized residuals, using this formula, from both the OLS and WLS models for any regression outliers you identified in Question #2. (Hint: Remember that in an OLS regression the weight is 1 for each observation.)\n\\[\ne^{\\prime}_i = \\frac{e_i}{s_{e(-i)}\\sqrt{1-h_{ii}}} \\times \\sqrt{w_i}\n\\]\nBased on the values of the studentized residuals in the WLS model, are the observations you identified as regression outliers from the OLS model still regression outliers in the WLS model? Why or why not?\nExplain why the is the case by referring to the formula.\nCreate and report residual plots of the studentized residuals versus the fitted values for the OLS and WLS models. Comment on which model better fits the assumptions.\n\nIncluding Covariates\nNow include the energy covariate into the model to examine the effect of socilist strength after controlling for economic development. Since the model has changed, we need to re-compute the weights and re-carry out the WLS analysis.\nUse matrix algebra to compute the empirical weights based on the two-predictor model and report the weight for the United States.\nFit the two-predictor WLS model using matrix algebra. Report the fitted equation.\nCompute and report the standard errors of the two-predictor WLS model using matrix algebra.\nUsing your results from Questions #14 and #15, compute and report the t-values and p-values. While you can use the output of the tidy(), summary(), or other functions that automatically compute p-values to check your work, use the pt() function to answer this question. (Show your work or syntax for full credit.)\nBased on the two-predictor WLS model results, what is suggested about the research hypothesis that countries with more socialist tendencies have less income inequality?\nBased on the two-predictor OLS model results, what is suggested about the research hypothesis that countries with more socialist tendencies have less income inequality?\nWhich set of the model results should we trust. Explain by referring to the tenability of the assumptions.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-29T11:30:00-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-29-reading-variance-stabilizing-transformations/",
    "title": "üìñ Variance Stabilizing Transformations",
    "description": "Reading",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-09-29",
    "categories": [
      "Reading"
    ],
    "contents": "\nRead the following article.\nOsborne, J. W. (2009). Notes on the use of data transformations. Practical Assessment, Research & Evaluation, 8(6).\n\nAdditional Resources\nKaufman, R. L. (2013). Heteroskedasticity in regression: Detection and correction. Sage. https://dx-doi-org.ezp1.lib.umn.edu/10.4135/9781452270128.n4\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-29T11:56:55-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-29-reading-wls-and-sandwich-estimation/",
    "title": "üìñ Weighted Least Squares (WLS) and Sandwich Estimation",
    "description": "Reading",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-09-29",
    "categories": [
      "Reading"
    ],
    "contents": "\nRead the following chapter. It should be accessible via the link after logging in with your x500 and password.\nRead Kaufman, R. L. (2013). Heteroskedasticity-consistent (robust) standard errors. In Heteroskedasticity in regression: Detection and correction (pp.¬†43‚Äì50). Sage. https://dx-doi-org.ezp1.lib.umn.edu/10.4135/9781452270128.n4\n\nAdditional Resources\nShin, H.-C. (1998). Weighted least squares estimation with sampling weights. Journal of Econometrics, 8(2), 251‚Äì271.\nSolon, G., Haider, S. J., & Woolridge, J. (2013). What are we weighting for? (Working Paper No.¬†18859; NBER Working Paper Series). National Bureau of Economic Research.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-29T12:00:56-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-27-notes-tools-for-dealing-with-heteroskedasticity/",
    "title": "üìù Tools for Dealing with Heteroskedasticity",
    "description": "Notes",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-09-27",
    "categories": [
      "Notes"
    ],
    "contents": "\n\n\n\nIn this set of notes, we will use data from Statistics Canada‚Äôs Survey of Labour and Income Dynamics (SLID) to explain variation in the hourly wage rate of employed citizens in Ontario. The file slid.csv includes data collected in 1994 from employed citizens living in Ontario between the ages of 16 and 65.\n[CSV]\n[R Script File]\nThe variables in the dataset are:\nwages: Composite hourly wage rate based on all the participant‚Äôs jobs\nage: Age of the participant (in years)\neducation: Number of years of schooling\nmale: A dummy-coded predictor for sex (0=Non-male; 1=Male)\n\nData Exploration\nAs with any potential regression analysis, we will begin by importing the data and examining the scatterplot of each predictor with the outcome. These plots suggest that each of the predictors is related to the outcome.\n\n\n# Load libraries\nlibrary(broom)\nlibrary(car)\nlibrary(corrr)\nlibrary(ggExtra)\nlibrary(patchwork)\nlibrary(tidyverse)\n\n# Import data\nslid = read_csv(\"https://raw.githubusercontent.com/zief0002/epsy-8264/master/data/slid.csv\")\nhead(slid)\n\n\n# A tibble: 6 √ó 4\n  wages   age education  male\n  <dbl> <dbl>     <dbl> <dbl>\n1  10.6    40        15     1\n2  11      19        13     1\n3  17.8    46        14     1\n4  14      50        16     0\n5   8.2    31        15     1\n6  17.0    30        13     0\n\n\nNote that the read_csv() function can take a URL for a dataset stored on the web.\nHere we examine the marginal density plots for the outcome and each continuous predictor, along with the scatterplots showing the relationship between each predictor and the outcome.\n\n\n\nFigure 1: Scatterplot of hourly wage versus each predictor. The fitted regression line is also displayed in each plot.\n\n\n\nBased on what we see in these plots, the outcome (hourly wage) looks to be right-skewed. A skewed outcome may or may not be problematic. (It often leads to violations of the conditional normality or homoskedasticity assumption, although we cannot confirm until after we fit the model and examine the residuals.) The relationships between hourly wage and each of the three potential predictors seem linear. The plot with the age predictor, however, foreshadows that we might violate the homoskedasticity assumption (the variance of hourly wages seems to grow for higher ages), but we will withhold judgment until after we fit our multi-predictor model.\n\nFitting a Multi-Predictor Model\nNext, we fit a model regressing wages on the three predictors simultaneously and examine the residual plots. Because we will be looking at residual plots for many different fitted models, we will write and then use a function that creates these plots.\n\n\n# Function to create residual plots\nresidual_plots = function(object){\n  # Get residuals and fitted values\n  aug_lm = broom::augment(object)\n  \n  # Create residual plot\n  p1 = ggplot(data = aug_lm, aes(x =.resid)) +\n    educate::stat_density_confidence(model = \"normal\") +\n    geom_density() +\n    theme_light() +\n    xlab(\"Residuals\") +\n    ylab(\"Probability Density\")\n  \n  # Create residual plot\n  p2 = ggplot(data = aug_lm, aes(x =.fitted, y = .resid)) +\n    geom_hline(yintercept = 0, linetype = \"dashed\") +\n    geom_point() +\n    geom_smooth(method = \"loess\", se = TRUE, n = 50, span = 0.67) +\n    theme_light() +\n    xlab(\"FItted values\") +\n    ylab(\"Residuals\")\n  \n  \n  return(p1 | p2)\n}\n\n\n\n\nNote that the {educate} package is not available from CRAN, and only available via GitHub. To install this package use the install_github() function from the {remotes} package to install it. The full syntax is: remotes::install_github(\"zief0002/educate\").\nNow we can use our new function to examine the residual plots from the main-effects model.\n\n\n# Fit model\nlm.1 = lm(wages ~ 1 + age + education + male, data = slid)\n\n# Examine residual plots\nresidual_plots(lm.1)\n\n\n\n\nFigure 2: Residual plots for the model that includes the main effects of age, education level, and sex.\n\n\n\nExamining the residual plots:\nThe linearity assumption may be violated; the loess line suggests some nonlinearity (maybe due to omitted interaction/polynomial terms)\nThe normality assumption may be violated; the upper end of the distribution deviates from what would be expected from a normal distribution in the QQ-plot.\nThe homoskedasticity assumption is likely violated; the plot of studentized residuals versus the fitted values shows severe fanning; the variation in residuals seems to increase for higher fitted values.\nBecause of the nonlinearity, we might consider including interaction terms. The most obvious interaction is that between age and education level, as it seems like the effect of age on hourly wage might be moderated by education level. (Remember, do NOT include interactions unless they make theoretical sense!) Below we fit this model, still controlling for sex, and examine the residuals.\n\n\n\nFigure 3: Residual plots for the model that includes an interaction effect between age and education level.\n\n\n\nIncluding the age by education interaction term (age:education) seems to alleviate the nonlinearity issue, but the residual plots indicate there still may be violations of the normality and homoskedasticity assumptions. Violating normality is less problematic here since, given our sample size, the Central Limit Theorem will ensure that the inferences are still approximately valid. Violating homoskedasticity, on the other hand, is more problematic.\n\nViolating Homoskedasticity\nViolating the distributional assumption of homoskedasticity results in:\nIncorrect computation of the sampling variances and covariances; and because of this\nThe OLS estimates are no longer BLUE (Best Linear Unbiased Estimator).\nThis means that the SEs (and resulting t- and p-values) for the coefficients are incorrect. In addition, the OLS estimators are no longer the most efficient estimators. How bad this is depends on several factors (e.g., how much the variances differ, sample sizes).\n\nHeteroskedasticity: What is it and How do we Deal with It?\nRecall that the variance‚Äìcovariance matrix for the residuals under the asssumption of homoskedasticity was:\n\\[\n\\boldsymbol{\\sigma^2}(\\boldsymbol{\\epsilon}) =  \\begin{bmatrix}\\sigma^2_{\\epsilon} & 0 & 0 & \\ldots & 0 \\\\ 0 & \\sigma^2_{\\epsilon} & 0 & \\ldots & 0\\\\ 0 & 0 & \\sigma^2_{\\epsilon} & \\ldots & 0\\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & 0 & \\ldots & \\sigma^2_{\\epsilon}\\end{bmatrix}\n\\]\nHomoskedasticity implies that the variance for each residual was identical, namely \\(\\sigma^2_{\\epsilon}\\). Since the variance estimate for each residual was the same, we could estimate a single value for these variances, the residual variance, and use that to obtain the sampling variances and covariances for the coefficients:\n\\[\n\\boldsymbol{\\sigma^2_B} = \\sigma^2_{\\epsilon} (\\mathbf{X}^{\\prime}\\mathbf{X})^{-1}\n\\]\nHeteroskedasticy implies that the residual variances are not constant. We can represent the variance‚Äìcovariance matrix of the residuals under heteroskedasticity as:\n\\[\n\\boldsymbol{\\sigma^2}(\\boldsymbol{\\epsilon}) =  \\begin{bmatrix}\\sigma^2_{1} & 0 & 0 & \\ldots & 0 \\\\ 0 & \\sigma^2_{2} & 0 & \\ldots & 0\\\\ 0 & 0 & \\sigma^2_{3} & \\ldots & 0\\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & 0 & \\ldots & \\sigma^2_{n}\\end{bmatrix}\n\\]\nIn this matrix, each residual has a potentially different variance. Now, there is more than one residual variance, and estimating these variance becomes more complicated, as does estimating the sampling variances and covariances of the regression coefficients.\nThere are at least three primary methods for dealing with heteroskedasticity: (1) transform the y-values using a variance stablizing transformation; (2) fit the model using weighted least squares rather than OLS; or (3) adjust the SEs and covariances to account for the non-constant variances. We will examine each of these in turn.\nVariance Stabilizing Transformations\nThe idea behind using a variance stabilizing transformation on the outcome (y) is that the transformed y-values will be homoskedastic. If so, we can fit the OLS regression model using the transformed y-values; the inferences will be valid; and, if necessary, we can back-transform for better interpretations. There are several transformations that can be applied to y that might stabilize the variances. Two common transformations are:\nLog-transformation; \\(\\ln(Y)_i\\)\nSquare-root transformation; \\(\\sqrt{Y_i}\\)\n\nPrior to applying these transformations, you may need to add a constant value to each y-value so that all \\(y_i>0\\) (log-transformation) or all \\(y_i \\geq 0\\) (square-root transformation).\nBoth of these transformations are power transformations. Power transformations have the mathematical form:\n\\[\ny^*_i = y_i^{p}\n\\]\nwhere \\(y^*_i\\) is the transformed y-value, \\(y_i\\) is the original y-value, and p is an integer. The following are all power transformations of y:\n\\[\n\\begin{split}\n& ~\\vdots \\\\[0.5em]\n& Y^4 \\\\[0.5em]\n& Y^3 \\\\[0.5em]\n& Y^2 \\\\[1.5em]\n& Y^1 = Y \\\\[1.5em]\n& Y^{0.5} = \\sqrt{Y} \\\\[0.5em]\n& Y^0 \\equiv \\ln(Y) \\\\[0.5em]\n& Y^{-1} = \\frac{1}{Y} \\\\[0.5em]\n& Y^{-2} = \\frac{1}{Y^2} \\\\[0.5em]\n& ~\\vdots\n\\end{split}\n\\]\nPowers such that \\(p<1\\) are referred to as downward transformations, and those with \\(p>1\\) are referred to as upward transformations. Both the log-transformation and square-root transformation are downward transformations of y. Here we will fit the main effects model using the square-root trnsformation and the log-transformation of the hourly wage values.\n\n\n# Create transformed y-values\nslid = slid %>%\n  mutate(\n    sqrt_wages = sqrt(wages),\n    ln_wages = log(wages)\n  )\n\n# Fit models\nlm_sqrt = lm(sqrt_wages ~ 1 + age + education + male, data = slid)\nlm_ln = lm(ln_wages ~ 1 + age + education + male, data = slid)\n\n\n\nThe plots below show the residuals based on fitting a model with each of these transformations applied to the wages data.\n\n\n\nFigure 4: TOP: Residual plots for the main effects model that used a square root transformation on y. BOTTOM: Residual plots for the main effects model that used a logarithmic transformation on y.\n\n\n\nBoth of these residual plots seem to show less heterogeneity than the residuals from the model with untransformed wages. However, neither transformation seems to have ‚Äúfixed‚Äù the problem completely.\n\nBox-Cox Transformation\nIs there a power transformation that would better ‚Äúfix‚Äù the heteroskedasticity? In their seminal paper, Box & Cox (1964) proposed a series of power transformations that could be applied to data in order to better meet assumptions such as linearity, normality, and homoskedasticity. The general form of the Box-Cox model is:\n\\[\nY^{(\\lambda)}_i = \\beta_0 + \\beta_1(X1_{i}) + \\beta_2(X2_{i}) + \\ldots + \\beta_k(Xk_{i}) + \\epsilon_i\n\\]\nwhere the errors are independent and \\(\\mathcal{N}(0,\\sigma^2_{\\epsilon})\\), and\n\\[\nY^{(\\lambda)}_i = \\begin{cases}\n   \\frac{Y_i^{\\lambda}-1}{\\lambda} & \\text{for } \\lambda \\neq 0 \\\\[1em]\n   \\ln(Y_i)       & \\text{for } \\lambda = 0\n  \\end{cases}\n\\]\nThis is only defined for positive values of Y.\nThe powerTransform() function from the {car} library can be used to determine the optimal value of \\(\\lambda\\). The boxCox() function from the same library gives a profile plot showing the log-likelohoods for a sequence of \\(\\lambda\\) values. If you do not specify the sequence of \\(\\lambda\\) values it will use lambda = seq(from = -2, to = 2, by = 1/10) by default.\n\n\n# Find optimal power transformation using Box-Cox\npowerTransform(lm.1)\n\n\nEstimated transformation parameter \n        Y1 \n0.08598786 \n\nThe output from the powerTransform() function gives the optimal power for the transformation of y, namely \\(\\lambda = 0.086\\). To actually implement the power transformation we use the transform Y based on the Box-Cox algorithm presented earlier.\n\n\nslid = slid %>%\n  mutate(\n    bc_wages = (wages ^ 0.086 - 1) / 0.086\n  )\n\n# Fit models\nlm_bc = lm(bc_wages ~ 1 + age + education + male, data = slid)\n\n\n\nThe residual plots (shown below) indicate better behaved residuals for the main-effects model, although even this optimal transformation still shows some evidence of heteroskedasticity.\n\n\n\nFigure 5: Residual plots for the main effects model that used a Box-Cox transformation on Y with \\(\\lambda=0.086\\).\n\n\n\nOne problem with using this transformation is that the regression coefficients do not have a direct interpretation. For example, looking at the coefficent-level output:\n\n\ntidy(lm_bc, conf.int = TRUE)\n\n\n# A tibble: 4 √ó 7\n  term        estimate std.error statistic   p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)   1.04    0.0475        21.9 1.80e-100   0.947     1.13  \n2 age           0.0227  0.000687      33.0 2.85e-211   0.0213    0.0240\n3 education     0.0707  0.00272       26.0 5.14e-138   0.0654    0.0760\n4 male          0.282   0.0164        17.2 4.99e- 64   0.250     0.315 \n\nThe age coefficient would be interpreted as: each one-year difference in age is associated with a 0.0227-unit difference in the transformed Y controlling for differences in education and sex. But what does a 0.227-unit difference in transformed Y mean when we translate that back to wages?\n\nProfile Plot for Different Transformations\nMost of the power transformations under Box-Cox would produce coefficients that are difficult to interpret. The exception is when \\(\\lambda=0\\). This is the log-transformation which is directly interpretable. Since the optimal \\(\\lambda\\) value of 0.086 is quite close to 0, we might wonder whether we could just use the log-transformation (\\(\\lambda=0\\)). The Box-Cox algorithm optimizes the log-likelihood of a given model, so the statistical question is whether there is a difference in the log-likelihood produced by the optimal transformation and that for the log-transformation.\nTo evaluate this, we can plot of the log-likelihood for a given model using a set of lambda values. This is called a profile plot of the log-likelihood. The boxCox() function creates a profile plot of the log-likelihood for a defined sequence of \\(\\lambda\\) values. Here we will plot the profile of the log-likelihood for \\(-2 \\leq \\lambda \\leq 2\\).\n\n\n# Plot of the log-likelihood for a given model versus a sequence of lambda values\nboxCox(lm.1, lambda = seq(from = -2, to = 2, by = 0.1))\n\n\n\n\nFigure 6: Plot of the log-likelihood profile for a given model versus a sequence of lambda values. The lambda that produces the highest log-likelihood is 0.086, the optimal lambda value.\n\n\n\nThe profile plot shows that the optimal lambda value, 0.86, produces the maximum log-likelihood value for the given model. We also are shown the 95% confidence limits for lambda based on a test of the curvature of the log-likelihood function. This interval offers a range of \\(\\lambda\\) values that will give comparable transformations. Since the values associated with the confidence limits are not outputted by the boxCox() function, we may need to zoom in to determine these limits by tweaking the sequence of \\(\\lambda\\) values in the boxCox() function.\n\n\nboxCox(lm.1, lambda = seq(from = 0.03, to = 0.2, by = .001))\n\n\n\n\nFigure 7: Plot of the log-likelihood profile for a given model versus a narrower sequence of lambda values.\n\n\n\nIt looks as though \\(.03 \\leq \\lambda \\leq 0.14\\) all give comparable transformations. Unfortunately, 0 is not included in those limits. This means that the \\(\\lambda\\) value of 0.086 will produce a higher log-likelihood than the log-transformation. It is important to remember that even though the log-likelihood will be optimized, the compatibility with the assumptions may or may not be improved when we use \\(\\lambda=0.086\\) versus \\(\\lambda=0\\). The only way to evaluate this is to fit the models and check the residuals.\n\nWeighted Least Squares Estimation\nAnother method for dealing with heteroskedasticity is to change the method we use for estimating the coefficients and standard errors. The most common method for doing this is to use weighted least squares (WLS) estimation rather than ordinary least squares (OLS).\nUnder heteroskedasticity recall that the residual variance of the ith residual is \\(\\sigma^2_i\\), and the variance‚Äìcovariance matrix of the residuals is defined as,\n\\[\n\\boldsymbol{\\Sigma} =  \\begin{bmatrix}\\sigma^2_{1} & 0 & 0 & \\ldots & 0 \\\\ 0 & \\sigma^2_{2} & 0 & \\ldots & 0\\\\ 0 & 0 & \\sigma^2_{3} & \\ldots & 0\\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & 0 & \\ldots & \\sigma^2_{n}\\end{bmatrix},\n\\]\nThis implies that the n observations no longer have the same reliability (i.e., precision of estimation). Observations with small variances have more reliability than observations with large variances. The idea behind WLS estimation is that those observations that are less reliable are down-weighted in the estimation of the overall error variance.\nAssume Error Variances are Known\nLet‚Äôs assume that each of the error variances, \\(\\sigma^2_i\\), are known. This is generally not a valid assumption, but it gives us a point to start from. If we know these values, we can modify the likelihood function from OLS by substituting these values in for the OLS error variance, \\(\\sigma^2_{\\epsilon}\\).\n\\[\n\\begin{split}\n\\mathrm{OLS:} \\qquad \\mathcal{L}(\\boldsymbol{\\beta}) &= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2_{\\epsilon}}}\\exp\\left[-\\frac{1}{2\\sigma^2_{\\epsilon}} \\big(Y_i-\\beta_0 - \\beta_1X_{1i} - \\beta_2X_{2i} - \\ldots - \\beta_kX_{ki}\\big)^2\\right] \\\\[1em]\n\\mathrm{WLS:} \\qquad \\mathcal{L}(\\boldsymbol{\\beta}) &= \\prod_{i=1}^n \\frac{1}{\\sqrt{2\\pi\\sigma^2_{i}}}\\exp\\left[-\\frac{1}{2\\sigma^2_{i}} \\big(Y_i-\\beta_0 - \\beta_1X_{1i} - \\beta_2X_{2i} - \\ldots - \\beta_kX_{ki}\\big)^2\\right]\n\\end{split}\n\\]\nNext, we define the reciprocal of the error variances as \\(w_i\\), or weight:\n\\[\nw_i = \\frac{1}{\\sigma^2_i}\n\\]\nThis can be used to simplify the likelihood function for WLS:\n\\[\n\\begin{split}\n\\mathcal{L}(\\boldsymbol{\\beta}) &= \\bigg[\\prod_{i=1}^n \\sqrt{\\frac{w_i}{2\\pi}}\\bigg]\\exp\\left[-\\frac{1}{2} \\sum_{i=1}^n w_i\\big(Y_i-\\beta_0 - \\beta_1X_{1i} - \\beta_2X_{2i} - \\ldots - \\beta_kX_{ki}\\big)^2\\right]\n\\end{split}\n\\]\nWe can then find the coefficient estimates by maximizing \\(\\mathcal{L}(\\boldsymbol{\\beta})\\) with respect to each of the coefficients; these derivatives will result in k normal equations. Solving this system of normal equations we find that:\n\\[\n\\mathbf{b}_{\\mathrm{WLS}} = (\\mathbf{X}^{\\intercal}\\mathbf{W}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}\\mathbf{W}\\mathbf{y}\n\\]\nwhere W is a diagonal matrix of the weights,\n\\[\n\\mathbf{W} =  \\begin{bmatrix}w_{1} & 0 & 0 & \\ldots & 0 \\\\ 0 & w_{2} & 0 & \\ldots & 0\\\\ 0 & 0 & w_{3} & \\ldots & 0\\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & 0 & \\ldots & w_{n}\\end{bmatrix}\n\\]\nThe variance‚Äìcovariance matrix for the regression coefficients can then be computed using:\n\\[\n\\boldsymbol{\\sigma^2}(\\mathbf{B}) = \\sigma^2_{\\epsilon}(\\mathbf{X}^{\\intercal}\\mathbf{W}\\mathbf{X})^{-1}\n\\]\nwhere the estimate for \\(\\sigma^2_{\\epsilon}\\) is based on a weighted sum of squares:\n\\[\n\\hat\\sigma^2_{\\epsilon} = \\frac{\\sum_{i=1}^n w_i \\times \\epsilon_i^2}{n - k - 1}\n\\]\nWhich can be expressed in matrix algebra as a function of the weight matrix and residual vector as:\n\\[\n\\hat\\sigma^2_{\\epsilon} = \\frac{(\\mathbf{We})^{\\intercal}\\mathbf{e}}{n - k - 1}\n\\]\n\nAn Example of WLS Estimation\nTo illustrate WLS, consider the following data which includes average ACT scores for a classroom of students, ACT score for the teacher, and the standard deviation of the class ACT scores.\n\n\nClass Average ACT\n\n\nTeacher ACT\n\n\nClass SD\n\n\n17.3\n\n\n21\n\n\n5.99\n\n\n17.1\n\n\n20\n\n\n3.94\n\n\n16.4\n\n\n19\n\n\n1.90\n\n\n16.4\n\n\n18\n\n\n0.40\n\n\n16.1\n\n\n17\n\n\n5.65\n\n\n16.2\n\n\n16\n\n\n2.59\n\n\nSuppose we want to use the teacher‚Äôs ACT score to predict variation in the class average ACT score. Fitting this model using OLS, we can compute the coefficient estimates and the standard errors for each coefficient.\n\n\n# Enter y vector\ny = c(17.3, 17.1, 16.4, 16.4, 16.1, 16.2)\n\n# Create design matrix\nX = matrix(\n  data = c(rep(1, 6), 21, 20 , 19, 18, 17, 16),\n  ncol = 2\n)\n\n# Compute coefficients\nb = solve(t(X) %*% X) %*% t(X) %*% y\n\n# Compute SEs for coefficients\ne = y - X %*% b\nsigma2_e = t(e) %*% e / (6 - 1 - 1) \nV_b = as.numeric(sigma2_e) * solve(t(X) %*% X)\nsqrt(diag(V_b))\n\n\n[1] 0.98356794 0.05294073\n\nWe could also have used built-in R functions to obtain these values:\n\n\nlm.ols = lm(y ~ 1 + X[ , 2])\ntidy(lm.ols, conf.int = TRUE)\n\n\n# A tibble: 2 √ó 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)   12.1      0.984      12.3  0.000252   9.36      14.8  \n2 X[, 2]         0.243    0.0529      4.59 0.0101     0.0959     0.390\n\nThe problem, of course, is that the variation in the residuals is not constant as the reliability for the 10 class average ACT values is not the same for each class; the standard deviations are different. Because of this, we may want to fit a WLS regression model rather than an OLS model.\n\n\n# Set up weight matrix, W\nclass_sd = c(5.99, 3.94, 1.90, 0.40, 5.65, 2.59)\nw_i = 1 / (class_sd ^ 2)\nW = diag(w_i)\nW\n\n\n          [,1]       [,2]      [,3] [,4]       [,5]      [,6]\n[1,] 0.0278706 0.00000000 0.0000000 0.00 0.00000000 0.0000000\n[2,] 0.0000000 0.06441805 0.0000000 0.00 0.00000000 0.0000000\n[3,] 0.0000000 0.00000000 0.2770083 0.00 0.00000000 0.0000000\n[4,] 0.0000000 0.00000000 0.0000000 6.25 0.00000000 0.0000000\n[5,] 0.0000000 0.00000000 0.0000000 0.00 0.03132587 0.0000000\n[6,] 0.0000000 0.00000000 0.0000000 0.00 0.00000000 0.1490735\n\n# Compute coefficients\nb_wls = solve(t(X) %*% W %*% X) %*% t(X) %*% W %*% y\nb_wls\n\n\n           [,1]\n[1,] 13.4154764\n[2,]  0.1658431\n\n# Compute standard errors for coefficients\ne_wls = y - X %*% b_wls                                 # Compute errors from WLS\nmse_wls = (t(W %*% e_wls) %*% e_wls) / (6 - 1 - 1)      # Compute MSE estimate\nv_b_wls = as.numeric(mse_wls) * solve(t(X) %*% W %*% X) # Compute variance-covariance matrix for B\nsqrt(diag(v_b_wls))\n\n\n[1] 1.17680463 0.06527187\n\nThe results of fitting both the OLS and WLS models appear below. Comparing the two sets of results, there is a difference in the coefficient values and in the estimated SEs when using WLS estimation rather than OLS estimation. This would also impact any statistical inference as well.\n\n\n\n\n\nOLS\n\n\n\n\nWLS\n\n\n\nCoefficient\n\n\nB\n\n\nSE\n\n\nB\n\n\nSE\n\n\nIntercept\n\n\n12.0905\n\n\n0.9836\n\n\n13.4155\n\n\n1.1768\n\n\nEffect of Teacher ACT Score\n\n\n0.2429\n\n\n0.0529\n\n\n0.1658\n\n\n0.0653\n\n\n\nFitting the WLS estimation in the lm() Function\nThe lm() function can also be used to fit a model using WLS estimation. To do this we include the weights= argument in lm(). This takes a vector of weights representing the \\(w_i\\) values for each of the n observations.\n\n\n# Create weights vector\nw_i = 1 / (class_sd ^ 2)\n\n# Fit WLS model\nlm_wls = lm(y ~ 1 + X[ , 2], weights = w_i)\ntidy(lm_wls, conf.int = TRUE)\n\n\n# A tibble: 2 √ó 7\n  term        estimate std.error statistic  p.value conf.low conf.high\n  <chr>          <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)   13.4      1.18       11.4  0.000338  10.1       16.7  \n2 X[, 2]         0.166    0.0653      2.54 0.0639    -0.0154     0.347\n\nNot only can we use tidy() and glance() to obtain coefficient and model-level summaries, but we can also use augment(), anova(), or any other function that takes a fitted model as its input.\n\nWhat if Error Variances are Unknown?\nThe previous example assumed that the variance‚Äìcovariance matrix of the residuals was known. In practice, this is almost never the case. When we do not know the error variances, we need to estimate them from the data.\nOne method for estimating the error variances for each observation, is:\nFit an OLS model to the data, and obtain the residuals.\nSquare these residuals and regress them (using OLS) on the same set of predictors.\nObtain the fitted values from Step 2.\nCreate the weights using \\(w_i = \\frac{1}{\\hat{y}_i}\\) where \\(\\hat{y}_i\\) are the fitted values from Step 3.\nFit the WLS using the weights from Step 4.\nThis is a two-stage process in which we (1) estimate the weights, and (2) use those weights in the WLS estimation. We will illustrate this methodology using the SLID data.\n\n\n# Step 1: Fit the OLS regression\nlm_step_1 = lm(wages ~ 1 + age + education + male + age:education, data = slid)\n\n# Step 2: Obtain the residuals and square them\nout_1 = augment(lm_step_1) %>%\n  mutate(\n    e_sq = .resid ^ 2\n  )\n\n# Step 2: Regresss e^2 on the predictors from Step 1\nlm_step_2 = lm(e_sq ~ 1 + age + education + male + age:education, data = out_1)\n\n# Step 3: Obtain the fitted values from Step 2\ny_hat = fitted(lm_step_2)\n\n\n# Step 4: Create the weights\nw_i = 1 / (y_hat ^ 2)\n\n# Step 5: Use the fitted values as weights in the WLS\nlm_step_5 = lm(wages ~ 1 + age + education + male + age:education, data = slid, weights = w_i)\n\n\n\nBefore examining any output from this model, let‚Äôs examine the residual plots. The residual plots suggest that the homoskedasticity assumption is much more reasonably satisfied after using WLS estimation; although it is still not perfect. The normality assumption looks untenable here.\n\nOne way to proceed would be to apply a variance stabilizing transformation to y (e.g., log-transform) and then fit a WLS model. To do this you would go through the steps of estimating the weights again based on the transformed y.\n\n\n\nFigure 8: Residual plots for the model that includes the main effects of age, education level, and sex fitted with WLS estimation.\n\n\n\nThe WLS coefficient estimates, standard errors, and coefficient-level inference are presented below.\n\n\n# Examine coefficient-level output\ntidy(lm_step_5, conf.int = TRUE)\n\n\n# A tibble: 5 √ó 7\n  term          estimate std.error statistic   p.value conf.low conf.high\n  <chr>            <dbl>     <dbl>     <dbl>     <dbl>    <dbl>     <dbl>\n1 (Intercept)     4.97    0.227        21.9  1.37e-100   4.52      5.41  \n2 age             0.0801  0.00591      13.5  6.95e- 41   0.0685    0.0917\n3 education      -0.201   0.0316       -6.36 2.30e- 10  -0.263    -0.139 \n4 male            2.24    0.166        13.5  1.56e- 40   1.92      2.57  \n5 age:education   0.0185  0.000921     20.1  1.77e- 85   0.0167    0.0203\n\n\nAdjusting the Standard Errors: Sandwich Estimation\nSince the primary effect of heteroskedasticity is that the sampling variances and covariances are incorrect, one method of dealing with this assumption violation is to use the OLS coefficients (which are still unbiased under heteroskedasticity), but make adjustments to the variance‚Äìcovariance matrix of the coefficients. We can compute the adjusted variance‚Äìcovariance matrix of the regression coefficients using:\n\\[\nV(\\mathbf{b}) = (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}\\boldsymbol{\\Sigma}\\mathbf{X} (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\n\\]\nwhere, \\(\\boldsymbol{\\Sigma}\\) is the variance-covariance matrix of the residuals.\n\nThis is often referred to as a sandwich estimator because the \\(\\mathbf{X}^{\\intercal}\\boldsymbol{\\Sigma}\\mathbf{X}\\) is ‚Äúsandwiched‚Äù between two occurrences of \\((\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\).\nNote that under the standard regression assumptions (including homoskedasticity), \\(\\boldsymbol{\\Sigma} = \\sigma^2_{\\epsilon}\\mathbf{I}\\), and this whole expression can be simplified to the matrix expression of the variance‚Äìcovariance matrix for the coefficients under the OLS model.:\n\\[\n\\begin{split}\nV(\\mathbf{b}) &= (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}\\sigma^2_{\\epsilon}\\mathbf{IX} (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1} \\\\[2ex]\n&= \\sigma^2_{\\epsilon}(\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}\\mathbf{IX} (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1} \\\\[2ex]\n&= \\sigma^2_{\\epsilon}(\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}\\mathbf{X} (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1} \\\\[2ex]\n&= \\sigma^2_{\\epsilon} \\mathbf{I} (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1} \\\\[2ex]\n&= \\sigma^2_{\\epsilon} (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\n\\end{split}\n\\]\nIf the errors are, however, heteroskedastic, then we need to use the heteroskedastic variance‚Äìcovariance of the residuals,\n\\[\n\\boldsymbol{\\Sigma} =  \\begin{bmatrix}\\sigma^2_{1} & 0 & 0 & \\ldots & 0 \\\\ 0 & \\sigma^2_{2} & 0 & \\ldots & 0\\\\ 0 & 0 & \\sigma^2_{3} & \\ldots & 0\\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & 0 & \\ldots & \\sigma^2_{n}\\end{bmatrix},\n\\]\nOne of the computational formulas for variance of a random variable X, using the rules of expectation is:\n\\[\n\\sigma^2_X = \\mathbb{E}\\bigg(\\big[X_i - \\mathbb{E}(X)\\big]^2\\bigg)\n\\]\nThis means for the ith error variance, \\(\\sigma^2_i\\), can be computed as\n\\[\n\\sigma^2_{i} = \\mathbb{E}\\bigg(\\big[\\epsilon_i - \\mathbb{E}(\\epsilon)\\big]^2\\bigg)\n\\]\nWhich, since \\(\\mathbb{E}(\\epsilon)=0\\) simplifies to\n\\[\n\\sigma^2_{i} = \\mathbb{E}\\big(\\epsilon_i^2\\big)\n\\]\nThis suggests that we can estimate \\(\\boldsymbol{\\Sigma}\\) as:\n\\[\n\\hat{\\boldsymbol{\\Sigma}} =  \\begin{bmatrix}\\epsilon^2_{1} & 0 & 0 & \\ldots & 0 \\\\ 0 & \\epsilon^2_{2} & 0 & \\ldots & 0\\\\ 0 & 0 & \\epsilon^2_{3} & \\ldots & 0\\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & 0 & \\ldots & \\epsilon^2_{n}\\end{bmatrix},\n\\]\nIn other words, the estimated variance‚Äìcovariance matrix of the residuals under heteroskedasticity is a diagonal matrix with elements that are the squared residuals from the OLS model. This\nGOing back to our SLID example, we can compute the adjusted variance‚Äìcovariance matrix of the coefficients by using the sandwich estimation method.\n\n\n# Fit OLS model\nlm.ols = lm(wages ~ 1 + age + education + male, data = slid)\n\n# Design matrix\nX = model.matrix(lm.1)\n\n# Create Sigma matrix\ne_squared = augment(lm.1)$.resid ^ 2\nSigma = e_squared * diag(3997)\n\n# Variance-covariance matrix for B\nV_b_adj = solve(t(X) %*% X) %*% t(X) %*% Sigma %*% X %*% solve(t(X) %*% X)\n\n# Compute SEs\nsqrt(diag(V_b_adj))\n\n\n(Intercept)         age   education        male \n0.635836527 0.008807793 0.038468695 0.207141705 \n\nThe SEs we produce from this method are typically referred to as Huber-White standard errors because they were introduced in a paper by Huber (1967) and their some of their statistical properties were proved in a paper by White (1980).\n\nModifying the Huber-White Estimates\nSimulation studies by Long & Ervin (2000) suggest a slight modification to the Huber-White estimates; by using a slightly different \\(\\boldsymbol\\Sigma\\) matrix:\n\\[\n\\hat{\\boldsymbol{\\Sigma}} =  \\begin{bmatrix}\\frac{\\epsilon^2_{1}}{(1-h_{11})^2} & 0 & 0 & \\ldots & 0 \\\\ 0 & \\frac{\\epsilon^2_{2}}{(1-h_{22})^2} & 0 & \\ldots & 0\\\\ 0 & 0 & \\frac{\\epsilon^2_{3}}{(1-h_{33})^2} & \\ldots & 0\\\\ \\vdots & \\vdots & \\vdots & \\ddots & \\vdots \\\\ 0 & 0 & 0 & \\ldots & \\frac{\\epsilon^2_{n}}{(1-h_{nn})^2}\\end{bmatrix},\n\\] where, \\(h_{ii}\\) is the ith diagonal element from the H matrix.\nWe can compute this modification by adjusting the e_squared value in the R syntax as:\n\n\n# Sigma matrix\ne_squared = augment(lm.1)$.resid ^ 2  / ((1 - augment(lm.1)$.hat) ^ 2)\nSigma = e_squared * diag(3997)\n\n# Variance-covariance matrix for B\nV_b_hw_mod = solve(t(X) %*% X) %*% t(X) %*% Sigma %*% X %*% solve(t(X) %*% X)\n\n# Compute adjusted SEs\nsqrt(diag(V_b_hw_mod))\n\n\n(Intercept)         age   education        male \n0.637012622 0.008821005 0.038539628 0.207364732 \n\nWe could use these SEs to compute the t-values, associated p-values, and confidence intervals for each of the coefficients.\nThe three sets of SEs are:\n\n\n\n\n\nSE\n\n\n\nCoefficient\n\n\nOLS\n\n\nHuber-White\n\n\nModified Huber-White\n\n\nIntercept\n\n\n0.5989773\n\n\n0.6358365\n\n\n0.6370126\n\n\nAge\n\n\n0.0086640\n\n\n0.0088078\n\n\n0.0088210\n\n\nEducation\n\n\n0.0342567\n\n\n0.0384687\n\n\n0.0385396\n\n\nAge x Education\n\n\n0.2070092\n\n\n0.2071417\n\n\n0.2073647\n\n\nIn these data, the modified Huber-White adjusted SEs are quite similar to the SEs we obtained from OLS, despite the heteroskedasticity observed in the residuals. One advantage of this method is that we do not have to have a preconceived notion of the underlying pattern of variation like we do to use WLS estimation. (We can estimate the pattern using the multi-step approach introduced earlier, but this assumes that the method of estimation correctly mimics the pattern of variation.) If, however, we can identify the pattern of variation, then WLS estimation will produce more efficient (smaller) standard errors than sandwich estimation.\n\n\n\nBox, G. E. P., & Cox, D. R. (1964). An analysis of transformations. Journal of the Royal Statisistical Society, Series B, 26, 211‚Äì246.\n\n\nHuber, P. J. (1967). The behavior of maximum likelihood estimates under nonstandard conditiona. In L. M. Le Cam & J. Neyman (Eds.), Proceedings of the fifth berkeley symposium on mathematical statistics and probability (pp. 221‚Äì233). University of California Press.\n\n\nLong, J. S., & Ervin, L. H. (2000). Using heteroskedasticity consistent standard errors in the linear regression model. The American Statistician, 54, 217‚Äì224.\n\n\nWhite, H. (1980). A heteroskedasticity-consistent covariance matrix estimator and a direct test for heteroskedasticity. Econometrica, 38, 817‚Äì838.\n\n\n\n\n",
    "preview": "posts/2021-09-27-notes-tools-for-dealing-with-heteroskedasticity/distill-preview.png",
    "last_modified": "2021-09-29T12:15:07-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-23-notes-regression-diagnostics/",
    "title": "üìù Regression Diagnostics",
    "description": "Notes",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-09-26",
    "categories": [
      "Notes"
    ],
    "contents": "\nHere are links to several script files and handouts that we will use in class:\nSlides is a set of slides we will cover in class.\nScript File is a script file that provides syntax for generating data from a given population regression model.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-26T08:39:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-24-assignment-03/",
    "title": "ü¶Ñ Assignment 03",
    "description": "The goal of this assignment is to give you experience using regression diagnostics for detecting problematic observations.",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-09-24",
    "categories": [
      "Assignments"
    ],
    "contents": "\nThere is a hypothesis in political science that suggests that income inequality is related to the democratic experience and economic development of a country. In this assignment, you are going to examine whether this hypothesis is supported by empirical evidence by using the data provided in the file stack-1979.csv. In particular, you are going to regress income inequality on voter turnout (democratic experience) and energy consumption (economic development).\n[CSV]\n[Data Codebook]\nInstructions\nSubmit a printed document of your responses to the following questions. Please adhere to the following guidelines for further formatting your assignment:\nAll plots should be resized so that they do not take up more room than necessary.\nAll figures and tables should have a name (e.g., Figure 1) and an appropriate caption.\nIn questions that ask you to ‚Äúuse matrix algebra‚Äù to solve the problem, you can either show your syntax and output from carrying out the matrix operations, or you can use Equation Editor to input the matrices involved in your calculations.\nThis assignment is worth 18 points.\n\nExploratory Analysis\nStart by creating scatterplots to examine the relationship between each of the predictors and the outcome. Are there observations that look problematic in these plots? If so, identify the country(ies).\nFit the regression model (specified in the introduction) to the data. Report the fitted equation.\nCreate and include a set of plots that allow you to examine the assumptions for linear regression. Based on these plots, comment on the tenability of these assumptions.\n\nOutliers, Leverage, and Influence\nCompute the externally studentized residuals for the observations based on the fitted regression. Based on these values, identify any countries that you would consider as regression outliers. Explain why you identified these countries as regression outliers.\nFit a mean-shift model that will allow you to test whether the observation with the largest absolute studentized residual is statistically different from zero. Report the coefficient-level output (B, SE, t, and p) for this model.\nFind (and report) the Bonferroni adjusted p-value for the observation with the largest absolute studentized residual. Based on this p-value, is there statistical evidence to call this observation a regression outlier? Explain.\nCreate and include an index plot of the leverage values. Include a line in this plot that displays the cutpoint for ‚Äúhigh‚Äù leverage. Based on this plot, identify any countries with large leverage values.\nBased on the evidence you have looked at in Questions #4‚Äì7, do you suspect that any of the countries might influence the regression coefficients? Explain.\n\nInfluence Measures\nFor each of the influence measures listed below, create and include an index plot of the influence measure. For each plot, also include a line that displays the cutpoint for ‚Äúhigh‚Äù influence. (3pts)\nScaled (standardized) DFBETA values\nCook‚Äôs D\nDFFITS\nCOVRATIO\n\nShow how the Cook‚Äôs D value for the country with the largest Cook‚Äôs D value is calculated using the country‚Äôs leverage value and standardized residual.\nCreate and include the added-variable plots for each each of the coefficients. Based on these plots, identify any countries that you believe may be jointly influencing the regression coefficients.\n\nRemove and Refit\nBased on all of the evidence from the different influence measures you examined, identify and report the country(ies) that are influential. Explain how you decided on this set of observations.\nRemove the observations you identified in Question #12 from the data and refit the regression model omitting these observations. Report the fitted equation.\nCreate and include a set of plots that allow you to examine the assumptions for linear regression. Based on these plots, comment on the tenability of these assumptions.\nCompare and contrast the coefficient-level inferences from the model fitted with the full data and that fitted with the omitted observations.\nCompare and contrast the model-level summaries, namely \\(R^2\\) and the RMSE, from the model fitted with the full data and that fitted with the omitted observations.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-23T11:40:15-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-23-reading-regression-diagnostics/",
    "title": "üìñ Regression Diagnostics",
    "description": "Reading",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-09-23",
    "categories": [
      "Reading"
    ],
    "contents": "\nRead the following chapter. It should be accessible via the link after logging in with your x500 and password.\nFox, J. (1991). Outlying and influential data. In Regression diagnostics (pp.¬†21‚Äì40). Sage. doi: https://dx-doi-org.ezp1.lib.umn.edu/10.4135/9781412985604.n4\n\nAdditional Resources\nKim, B. (2015). Understanding diagnostic plots for linear regression analysis. University of Virginia Library.\nCook, R. D. (1998). Regression graphics: Ideas for studying regressions through graphics. Wiley.\n\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-29T11:55:12-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-16-assignment-02/",
    "title": "ü¶í Assignment 02",
    "description": "Simulating from the Regression Model",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-09-20",
    "categories": [
      "Assignments"
    ],
    "contents": "\nThe goal of this assignment is to give you experience using simulation to explore properties of the regression model.\n\n\n\n\n\nInstructions\nSubmit a printed document of your responses to the following questions. Please adhere to the following guidelines for further formatting your assignment:\nAll plots should be resized so that they do not take up more room than necessary.\nAll figures and tables should have a name (e.g., Figure 1) and an appropriate caption.\nIn questions that ask you to ‚Äúuse matrix algebra‚Äù to solve the problem, you can either show your syntax and output from carrying out the matrix operations, or you can use Equation Editor to input the matrices involved in your calculations.\nThis assignment is worth 16 points.\n\nSimulation 1: Modeling Heteroskedasticity\nIn this simulation, you will explore what happens to the regression estimates when the assumption of homoskedasticity is violated. For this simulation use a sample size of 200.\nCreate the fixed X values you will use in each trial of the simulation. To do this, draw \\(n=200\\) X-values from a uniform distribution with a minimum of \\(-2\\) and a maximum of \\(+2\\). Prior to drawing these values, set your starting seed to 678910. Report the syntax you used, and the first six X values.\nCreate a the Y-values for the first trial of the simulation by using the model:\n\\[\n\\begin{split}\ny_i &= -3.2 + 1.75(x_i) + \\epsilon_i \\\\[2ex]\n\\epsilon_i &\\overset{i.i.d.}{\\sim} \\mathcal{N}(0, \\sigma)\n\\end{split}\n\\]\n\nwhere\n\n\\[\n\\begin{split}\n\\sigma &= e^{\\gamma(x_i)}\\\\[2ex]\ne &\\mathrm{~is~Euhler's~constant~}(\\approx2.718282) \\\\[2ex]\n\\gamma &= 1.5\n\\end{split}\n\\]\n\nHere the variation in the random error is a function of X and random noise. Report the syntax you used, and the first six Y values.\n\nCreate and report the scatterplot of the Y-values versus the X-values for this first trial of the simulation.\nDescribe the pattern of heteroscedasticity.\nDoes the pattern of heteroscedasticity you described in Question 4 make sense given how the error term was created. Explain.\nCarry out 1000 trials of the simulation. (Reminder: Be sure to use these same X values in each trial of the simulation; they are fixed.) For each trial, collect: (a) the estimate of the intercept, (b) the estimate of the slope, and (c) the estimate of the residual standard error.\nCompute and report the mean value for the residual standard error.\nSimulation 2: Homoskedastic Model\nTo evaluate the different estimates from the heteroskedasticity model, we need to compare them to estimates drawn from a homoskedastic model with the same population coefficients. To make the comparisons ‚Äúfair‚Äù, so that we are only evaluating the effects of the heteroskedasticity, we also need to run this simulation using a residual standard error that is equal to the mean from the heteroskedastic simulation (i.e., \\(\\mathtt{sd\\neq1}\\) in the rnorm() function).\nCarry out 1000 trials of the simulation for the appropriate homoskedastic model. (Reminder: Be sure to use these same X values in this simulation as in the previous simulation.) For each trial, collect: (a) the estimate of the intercept, (b) the estimate of the slope, and (c) the estimate of the residual standard error. Report your syntax.\nComparing Results from the Two Simulations: Evaluating the Effects of Hetroskedasticity\nCreate a density plot of the distribution of intercept estimates. Show the density curve for both models on the same plot, differentiating the curves using color, linetype, or both. Also add a vertical line to this plot at the population value of the intercept.\nBased on your responses to Question 8, does the intercept estimate seem to be biased under heteroskedasticity? Explain.\nBased on your responses to Question 8, does the intercept estimate seem to be less efficient under heteroskedasticity? Explain.\nCreate a density plot of the distribution of slope estimates. Show the density curve for both models on the same plot, differentiating the curves using color, linetype, or both. Also add a vertical line to this plot at the population value of the slope\nBased on your responses to Question 11, does the slope estimate seem to be biased under heteroskedasticity? Explain.\nBased on your responses to Question 11, does the slope estimate seem to be less efficient under heteroskedasticity? Explain.\nCreate a density plot of the distribution of residual standard error estimates. Show the density curve for both models on the same plot, differentiating the curves using color, linetype, or both. Also add a vertical line to this plot at the population value of the residual standard error\nBased on your responses to Question 14, does the residual standard error estimate seem to be biased under heteroskedasticity? Explain.\nBased on your responses to Question 14, does the residual standard error estimate seem to be less efficient under heteroskedasticity? Explain.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-20T10:33:18-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-20-notes-simulating-from-the-regression-model/",
    "title": "üìù Simulating from the Regression Model",
    "description": "Notes",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-09-20",
    "categories": [
      "Notes"
    ],
    "contents": "\nHere are links to several script files and handouts that we will use in class:\nGenerating Data\nGenerating Random Data from a Regression Model is a script file that provides syntax for generating data from a given population regression model.\nSimulation 1: Simulating from a Regression Model\nSimulating from a Regression Model is a script file that provides syntax for carrying out a simulation to produce distributions of estimates from a regression model.\nVizualization of Simulating from a Regression Model is a handout visualizing the simulation process for generating data from a given population regression model.\nSimulation 2: Simulating from a Null Regression Model\nSimulating from a Null Regression Model is a script file that provides syntax for carrying out a simulation to produce distributions of estimates assuming certain parameters in the regression model are zero.\nVizualization of Simulating from a Null Regression Model is a handout visualizing the simulation process for generating data to produce distributions of estimates assuming certain parameters in the regression model are zero.\nResources\nHere are several resources to help your understanding of simulation. The chapters from Monte Carlo Simulation and Resampling Methods for Social Science should be accessible via the links after logging in with your x500 and password.\nProbability: Common probability distributions and how to compute with them.\nRandom Number Generation: Learn how to draw random numbers from different distributions. Also information about repeating processes in R, including writing your own functions, using for loops, and using if-else functions.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-20T13:18:51-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-18-reading-simulating-from-the-regression-model/",
    "title": "üìñ Simulating from the Regression Model",
    "description": "Reading",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-09-18",
    "categories": [
      "Reading"
    ],
    "contents": "\nRead the following chapters from Carsey, T. M., & Harden, J. J. (2014). Monte Carlo simulation and resampling methods for social science. Sage. It should be accessible via the link after logging in with your x500 and password.\nIntroduction: This chapter gives a short introduction to the use of simulation in the social sciences.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-20T13:18:20-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-09-09-notes-ols-regression/",
    "title": "üìù OLS Regression Using Matrices and Its Properties",
    "description": "Notes",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-09-09",
    "categories": [
      "Notes"
    ],
    "contents": "\nHere are links to several PDF handouts:\nSummation, Expectation, Variance, Covariance, and Correlation is a handout that provides several mathematical rules for working with sums, expectations, variances, covariances, and correlation.\nOLS Estimators and Their Properties is a handout that steps through estimating the OLS regression estimators and also derives some of the properties of those estimators\nAssumptions for OLS Regression and the Gauss-Markov Theorem is a handout that examines the assumptions underlying the Gauss-Markov theorem; the theorem showing that the OLS estimators are BLUE.\nStatistical Inference for the Regression Model is a handout working through how we carry out coefficient-level and model-level statistical inference.\nA Regression Example in Practice is a handout that walks through using matrix algebra to compute many of the things we are interested in as applied researchers. It also show the equivalent built-in R functions for obtaining this.\nThe handouts include more detail than I will cover in class. I will highlight some important ideas from each of them, and you can work through some of the mathematical derivation on your own if it is of interest.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-10T09:14:00-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-14-reading-ols/",
    "title": "üìñ OLS Regression using Matrices and its Properties",
    "description": "Reading",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-08-14",
    "categories": [
      "Reading"
    ],
    "contents": "\nRead the following (short) chapters in Matrix Algebra for Educational Scientists:\nSystems of Equations\nStatistical Application: Estimating Regression Coefficients\nIn class, we will be working through the ideas in the following chapters:\nImportant Matrices in Regression\nSums of Squares in Regression\nStandard Errors and Variance Estimates\nAssumptions of the Regression Model\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-20T13:12:23-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-11-worksheet-introduction-to-matrix-algebra/",
    "title": "üìù Introduction to Matrix Algebra",
    "description": "In-class worksheet",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-08-13",
    "categories": [
      "Worksheet"
    ],
    "contents": "\nDirections\nComplete the problems on this worksheet with your smallgroup. You may want to refer to the Matrix Algebra for Educational Scientists text.\nYou will likely be learning (or re-encountering) many new mathematical terms. It is a good idea to note and define all the vocabulary/terms that you encounter as you work through this worksheet. You may want to do this individually or create a shared document that you can all contribute to.\n\nProblems\nConsider the following matrices:\n\\[\n\\mathbf{A} = \\begin{bmatrix}3 & -2\\\\5 & 1\\end{bmatrix} \\quad \\mathbf{B} = \\begin{bmatrix}3 & -1\\\\-1 & 2\\end{bmatrix}\\quad \\mathbf{C} = \\begin{bmatrix}1 & 2 & 3\\\\0 & 1 & 2\\end{bmatrix}\n\\]\nMake sure everyone in your group can solve each of these problem by hand and using R.\nWhat are the dimensions of A? C?\nIs C a square matrix? Explain.\nFind the trace of A.\nFind the determinant of A.\nAdd A and B\nFind the transpose of C.\nBy referring to the dimensions, can you compute AC? How about CA?\nCompute AC.\nCompute BI\nCreate a \\(3\\times3\\) diagonal matrix whose trace is 10.\nHow do you know that B has an inverse? Explain.\nCompute \\(\\mathbf{B}^{-1}\\)\nCreate a \\(3\\times3\\) matrix that has rank 2. Verify this using R.\nCreate a \\(3\\times3\\) matrix that is symmetric and is not I.\nSolve the system of linear equations using algebra (e.g., substitution, elimination) and then solve them using matrix methods (with R). To do this you will need to read the Systems of Equations chapter in Matrix Algebra for Educational Scientists.\n\\[\n\\begin{split}\nx + y + z &= 2 \\\\\n6x - 4y + 5z &= 31 \\\\\n5x + 2y + 2z &= 13\n\\end{split}\n\\]\n\n\n\n",
    "preview": {},
    "last_modified": "2021-09-07T15:54:53-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-12-reading-introduction-to-matrix-algebra/",
    "title": "üìñ Introduction to Matrix Algebra",
    "description": "Review some common mathematical ideas of matrix algebra.",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-08-12",
    "categories": [
      "Reading"
    ],
    "contents": "\nRead the following (short) chapters in Matrix Algebra for Educational Scientists:\nIntroduction\nData Structures\nVectors\nVector Operations\nIn class, we will be working through some problems to cement these ideas. We will also examine and work through some problems related to matrix operations, so you could also read through:\nMatrices\nMatrix Addition and Subtraction\nMatrix Multiplication\nMatrix Transposition\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-13T08:51:11-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-09-assignment-01/",
    "title": "üêß Assignment 01",
    "description": "Matrix Algebra for Linear Regression",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-08-09",
    "categories": [
      "Assignments"
    ],
    "contents": "\nThe goal of this assignment is to give you experience using matrix algebra to compute various analytic output for regression. In this assignment, you will use the data given below that includes measurements for 10 countries on: infant mortality rate per 1000 live births (infant), the per-capita income (pci) and world region (region) of the country.\n\ncountry\ninfant\npci\nregion\nAlgeria\n86.3\n400\nAfrica\nBolivia\n60.4\n200\nAmericas\nBurundi\n150.0\n68\nAfrica\nDominican Republic\n48.8\n406\nAmericas\nKenya\n55.0\n169\nAfrica\nMalawi\n148.3\n130\nAfrica\nNicaragua\n46.0\n507\nAmericas\nParaguay\n38.6\n347\nAmericas\nRwanda\n132.9\n61\nAfrica\nTrinidad & Tobago\n26.2\n732\nAmericas\n\n\n\n\n\n\nInstructions\nSubmit a printed document of your responses to the following questions. Please adhere to the following guidelines for further formatting your assignment:\nAll plots should be resized so that they do not take up more room than necessary.\nAll figures and tables should have a name (e.g., Figure 1) and an appropriate caption.\nIn questions that ask you to ‚Äúuse matrix algebra‚Äù to solve the problem, you can either show your syntax and output from carrying out the matrix operations, or you can use Equation Editor to input the matrices involved in your calculations.\nThis assignment is worth 20 points.\n\nUnstandardized Regression\nYou will be fitting the model lm(infant ~ 1 + pci + region + pci:region). Within this model, use dummy coding to encode the region predictor and make Americas the reference group.\nWrite out the elements of the matrix \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\), where \\(\\mathbf{X}\\) is the design matrix.\nDoes \\(\\mathbf{X}^{\\intercal}\\mathbf{X}\\) have an inverse? Explain.\nCompute (using matrix algebra) and report the vector of coefficients, b for the OLS regression.\nCompute (using matrix algebra) and report the variance‚Äìcovariance matrix of the coefficients.\nUse the values from b (Question 3) and from the variance‚Äìcovariance matrix you reported in the previous question to find the 95% CI for the coefficient associated with the main-effect of PCI. (Hint: If you need to refresh yourself on how CIs are computed, see here.)\nCompute (using matrix algebra) and report the hat-matrix, H. Also show how you would use the values in the hat-matrix to find \\(\\hat{y}_1\\) (the predicted value for Algeria).\nCompute (using matrix algebra) and report the vector of residuals, e.\nCompute (using matrix algebra) and report the estimated value for the RMSE.\nGiven the assumptions of the OLS model and the RMSE estimate you computed in the previous question, compute and report the variance‚Äìcovariance matrix of the residuals.\n\nANOVA Decomposition\nIn this section you will be re-creating the output from the ANOVA decomposition for the model fitted in the previous section.\nCompute (using matrix algebra) and report the model, residual, and total sum of squares terms in the ANOVA decomposition table. (2pts)\nCompute (using matrix algebra) and report the model, residual, and total degrees of freedom terms in the ANOVA decomposition table. (2pts)\nUse the values you obtained in Questions 11 and 12 to compute the model and residual mean square terms.\nUse the mean square terms you found in Question 13 to compute the F-value for the model (i.e., to test \\(H_0:\\rho^2=0\\)). Also compute the p-value associated with this F-value. (Hint: If you need to refresh yourself on how F-values or p-values are computed, see here.)\n\nRegression: Effects-Coding\nNow consider fitting the model to the data to examine whether there is an effect of region (no other predictors) on infant mortality. In this model, we will use effects-coding to encode the region variable (see here). This model is often expressed as:\n\\[\n\\mathrm{Infant~Mortality}_i = \\mu + \\alpha_{\\mathrm{Region}} + \\epsilon_i \n\\]\nWrite out the design matrix that would be used to fit this model.\nCompute (using matrix algebra) and report the vector of coefficients, b, from the OLS regression.\nCompute (using matrix algebra) and report the variance‚Äìcovariance matrix for the coefficients.\nExplain why the sampling variances for the coefficients are the same and why the sampling covariance is zero by referring to computations produced in the matrix algebra. (2pts)\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-13T15:54:27-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-09-message-to-students/",
    "title": "A Message to Students",
    "description": "A message to students.",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-08-09",
    "categories": [],
    "contents": "\nIn this class, we will work together to develop a learning community that is inclusive and respectful, and where every student is supported in the learning process. As a class full of diverse individuals (reflected by differences in race, culture, age, religion, gender identity, sexual orientation, socioeconomic background, abilities, professional goals, and other social identities and life experiences) I expect that different students may need different things to support and promote their learning.\nThe TAs and I will do everything we can to help with this, but, as we only know what we know, we need you to communicate with us if things are not working for you or you need something we are not providing. I hope you all feel comfortable in helping to promote an inclusive classroom through respecting one another‚Äôs individual differences, speaking up, and challenging oppressive/problematic ideas. Finally, I look forward to learning from each of you and the experiences you bring to the class.\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-09T16:10:51-05:00",
    "input_file": {}
  },
  {
    "path": "posts/2021-08-09-prerequisites-and-resources/",
    "title": "Prerequisites and Resources",
    "description": "Prerequisite knowledge and resources for brushing up on that knowledge.",
    "author": [
      {
        "name": "andy",
        "url": {}
      }
    ],
    "date": "2021-08-09",
    "categories": [
      "Resources"
    ],
    "contents": "\nPrerequisites\nThe pre-requisites for this course are EPsy 8251 and EPsy 8252. Prerequisite knowledge include topics from a basic statistics course:\nFoundational topics in data analysis;\nDesign (e.g., random assignment and random sampling)\nDescriptive statistics and plots\nOne- and two-sample tests\n\nAnd, topics from EPsy 8251: Methods in Data Analysis for Educational Research I:\nStatistical Computation\nUsing R\nData wrangling/manipulation\nPlotting\n\nCorrelation;\nSimple regression analysis;\nModel-level and coefficient-level interpretation\nOrdinary least squares estimation\nStandardized regression\nPartitioning sums of squares\nModel-level and coefficient-level inference\nAssumption checking/residual analysis\n\nMultiple linear regression\nModel-level and coefficient-level interpretation and inference\nAssumption checking/residual analysis\nWorking with categorical predictors (including adjusting p-values for multiple tests)\nInteraction effects\n\nAnd topics from EPsy 8252: Methods in Data Analysis for Educational Research II:\nDealing with nonlinearity;\nQuadratic effects\nLog-transformations\n\nProbability distributions;\nProbability density\n\nMaximum likelihood estimation;\nModel selection;\nInformation criteria\n\nLinear mixed-effects models (cross-sectional/longitudinal)\nBasic ideas of mixed-effects models\nFitting models with random-intercepts and random-slopes\nAssumptions\nLikelihood ratio tests\n\nGeneralized linear models\nLogistic models\n\n\nResources\nFor the topics listed, students would be expected to be able to carry out an appropriate data analysis and properly interpret the results. It is also assumed that everyone enrolled in the course has some familiarity with using R. If you need a refresher on any of these topics, see:\nComputational Toolkit for Educational Scientists\nStatistical Modeling and Computation for Educational Scientists [EPsy 8251 material]\nEPsy 8252 website\n\n\n\n",
    "preview": {},
    "last_modified": "2021-08-10T11:14:10-05:00",
    "input_file": {}
  }
]
