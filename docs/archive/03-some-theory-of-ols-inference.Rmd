---
title: "Some Theory of OLS: Inference"
ipsum_meta:
  twitter_card: "Simple Linear Regression Theory: Inference"
  twitter_site: "\\@sitehandle"
  twitter_creator: "\\@creatorhandle"
  og_url: "https\\://example.com/open/graph/finalURLfor/this"
  og_description: "A modest size description of the content"
  og_image: "https\\://example.com/open/graph/imageURLfor/this"
output: 
  hrbrthemes::ipsum:
    toc: true
    css: note.css
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE,  dev = "cairo_pdf")

library(hrbrthemes)
library(ggplot2)
library(Cairo)
library(extrafont)

extrafont::loadfonts()
```

# Preparation

```{r message=FALSE}
# Load libraries
library(broom)
library(dplyr)
library(readr)
library(tidyr)

# Read in corrected Davis data and drop missing cases
davis = read_csv("../data/davis-corrected.csv") %>%
  select(weight, repwt) %>%
  drop_na()

head(davis)
```


<!-- ### Roboto Condensed -->

<!-- ```{r dev = "cairo_pdf"} -->
<!-- ggplot(mtcars, aes(wt, mpg)) + -->
<!--   geom_point() + -->
<!--   labs(title="Roboto Condensed", subtitle="This is a subtitle") + -->
<!--   theme_ipsum_rc() -->
<!-- ``` -->

One reason that OLS estimation is so useful is that, under a certain set of assumptions underlying the classical linear regression model, the estimators $B_0$ and $B_1$ have several desirable statistical properties. These properties include:

- The least squares estimators are *linear estimators*; they are linear functions of the observations. (This property helps us derive the sampling distributions for $B_0$ and $B_1$, which allows for statistical inference.)
- The least squares estimators are *unbiased estimators* of the population coefficients.
- The least squares estimators have sampling variances.
- Of all the linear, unbiased estimators, the least squares estimators have the smallest sampling variance (most precise/efficient).
- Under the assumption of normality, the least squares estimators are also normally distributed; they are approximately normal under other conditions, especially with large sample sizes.
- Under the full set of assumptions, the least squares estimators are the maximum-likelihood estimators of the population coefficients.



## Assumptions Underlying the Regression Model

- **Assumption 1:** The model is correctly specified; it is descriptive of the population or sampling process; $y_i = \beta_0 + \beta_1(x_i) + \epsilon_i$.

For the distribution of errors, conditional on $X$, we further assume:

- **Assumption 2:** Linearity; $\mathbb{E}(\epsilon_i|x_i) = 0$. This also implies that $\mathbb{E}(y_i|x_i) = \beta_0 + \beta_1(x_i)$
- **Assumption 3:** Constant variance; $\mathrm{Var}(\epsilon_i|x_i) = \sigma^2_{\epsilon}$
- **Assumption 4:** Independence of observations; $\mathrm{Cov}(\epsilon_i,\epsilon_j|x_i) = 0 \quad \mathrm{for~all~} i \neq j$
- **Assumption 5:** Conditional Normality; $\epsilon_i|x_i \sim \mathcal{N}\bigg(0,\sigma^2_{\epsilon}\bigg)$ 

Two other assumptions are that:

- **Assumption 6:** $X$ is either fixed or measured without error; 
- **Assumption 7:** $X$ is not invariant; $x_i$ has at least two distict values


By invoking some or all of these assumptions, we can mathematically prove and derive certain properties related to the OLS estimates. 

Recall that the least squares estimators are:

$$
\begin{split}
B_1 &= \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2} \\[1em]
B_0 &= \bar{y} - B_1(\bar{x}) \\[1em]
\end{split}
$$

## Property 1: The Regression Estimators are Linear Functions of the Observations

We first show that we can express the slope estimator $B_1$ as a linear function of the observations, namely,

$$
B_1 = \sum w_iY_i \qquad \mathrm{for~some~} w_i 
$$

To do this, we will use the definition of the slope estimator, that 

$$
B_1 = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2}
$$

<div class="proof">

$$
\begin{split}
B_1 &= \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2} \\[1em]
&= \frac{\sum (x_i - \bar{x})y_i}{\sum (x_i - \bar{x})^2} - \frac{\sum (x_i - \bar{x})\bar{y}}{\sum (x_i - \bar{x})^2} \\[1em]
&= \frac{\sum (x_i - \bar{x})y_i}{\sum (x_i - \bar{x})^2} - \frac{\bar{y}\sum (x_i - \bar{x})}{\sum (x_i - \bar{x})^2} \\[1em]
\end{split}
$$

Then we make use of the fact that the sum of mean deviations of a random variable is always zero; $\sum (x_i - \bar{x}) = 0$.

$$
\begin{split}
B_1 &= \frac{\sum (x_i - \bar{x})y_i}{\sum (x_i - \bar{x})^2} - \frac{\bar{y}(0)}{\sum (x_i - \bar{x})^2} \\[1em]
&= \frac{\sum (x_i - \bar{x})y_i}{\sum (x_i - \bar{x})^2} - 0\\[1em]
&= \frac{\sum (x_i - \bar{x})y_i}{\sum (x_i - \bar{x})^2}
\end{split}
$$
</div>

This means that $B_1$ can be written as a linear function of the observations $y_i$:

$$
B_1 = \sum w_iY_i \qquad \mathrm{where~} w_i = \frac{x_i-\bar{x}}{\sum(x_i-\bar{x})^2}
$$

We can carry out a similar process for $B_0$, but it is far easier. Recall that,

$$
B_0 = \bar{y} - B_1\bar{x}
$$

Since we just showed that $B_1 = \sum w_iy_i$, we can substitute that in for $B_1$.

$$
B_0 = \bar{y} - \sum w_iy_i\bar{x}
$$

Since $\bar{x}$ and $\bar{y}$ are constants, $B_0$ has been shown to be a linear function of the observations, $y_i$.

## Property 2: The Estimators are Unbiased

Unbiasedness of the estimators is an important sampling property. It says that when sampling repeatedly from a population, the least squares estimator is "correct", on average. This in and of itself does not mean that $B_0$ and $B_1$ are good estimators of $\beta_0$ and $\beta_1$, respectively, but it is part of the story. 

It is important to remember that unbiasedness is a property of an estimator and is dependent on long-run averages from drawing many samples of data from the same population. It is NOT a property of the estimates from any one single set of data. 

### Slope Estimator is Unbiased

To show that the slope estimator is unbiased, we need to show that $\mathbb{E}(B_1) = \beta$. To do this, we will make use of the fact that,


$$
\begin{split}
\sum(x_i-\bar{x})^2 &= \sum(x_i-\bar{x})(x_i-\bar{x})\\
&= \sum(x_i^2 -2x_i\bar{x} + \bar{x}^2) \\
&= \sum x_i^2 -2\bar{x} \sum x_i + \sum \bar{x}^2 \\
&= \sum x_i^2 -2\bar{x}(n\bar{x}) + n \bar{x}^2 \\
&= \sum x_i^2 -2n\bar{x}^2 + n \bar{x}^2 \\
&= \sum x_i^2 - n \bar{x}^2
\end{split}
$$

and the assumption of linearity in the population (Assumption \#2), namely that,

$$
\mathbb{E}(Y_i) = \beta_0 + \beta_1(x_i)
$$

Since,

$$
B_1 = \sum \frac{(x_i-\bar{x})y_i}{\sum x_i^2 - n \bar{x}^2},
$$

then the proof is as follows:


<div class="proof">
$$
\begin{split}
\mathbb{E}(B_1) &= \mathbb{E}\bigg(\sum \frac{(x_i-\bar{x})y_i}{\sum x_i^2 - n \bar{x}^2}\bigg) \\
&= \mathbb{E}\bigg(\frac{1}{\sum x_i^2 - n \bar{x}^2} \times \sum \bigg[(x_i-\bar{x})y_i\bigg] \bigg) \\
&= \frac{1}{\sum x_i^2 - n \bar{x}^2} \times \mathbb{E}\bigg(\sum \bigg[(x_i-\bar{x})y_i\bigg] \bigg) \\
&= \frac{1}{\sum x_i^2 - n \bar{x}^2} \times \sum\bigg(\mathbb{E} \bigg[(x_i-\bar{x})y_i\bigg] \bigg) \\
&= \frac{1}{\sum x_i^2 - n \bar{x}^2} \times \sum\bigg((x_i-\bar{x})\mathbb{E} \big[y_i\big] \bigg) \\
&= \frac{1}{\sum x_i^2 - n \bar{x}^2} \times \sum\bigg((x_i-\bar{x})\big[\beta_0 + \beta_1(x_i)\big] \bigg) \\
&= \frac{1}{\sum x_i^2 - n \bar{x}^2} \times \sum\bigg(x_i\beta_0-\bar{x}\beta_0 + \beta_1x_i^2 - \beta_1x_i\bar{x} \bigg) \\
&= \frac{1}{\sum x_i^2 - n \bar{x}^2} \times \beta_0\sum x_i - n\bar{x}\beta_0 + \beta_1 \sum x_i^2 - \beta_1\bar{x}\sum x_i \bigg) \\
&= \frac{1}{\sum x_i^2 - n \bar{x}^2} \times \beta_0n\bar{x} - n\bar{x}\beta_0 + \beta_1 \bigg( \sum x_i^2 - \bar{x}n\bar{x} \bigg) \\
&= \frac{1}{\sum x_i^2 - n \bar{x}^2} \times \beta_1 \bigg( \sum x_i^2 - n\bar{x}^2 \bigg) \\
&= \beta_1
\end{split}
$$

</div>

### Intercept Estimator is Unbiased

We can also prove the unbiasedness of the $B_0$ estimator. To do this, we need to show that $\mathbb{E}(B_0) = \beta_0$. Again, we first start with the definition of the intercept estimator:

$$
B_0 = \bar{y} - B_1(\bar{x})
$$

<div class="proof">
We start with the regression equation from Assumption 1, $y_i = \beta_0 + \beta_1(x_i)+ \epsilon_i$, and manipulate it to take the sum of both sides of the equation, and then divide by $n$.

$$
\begin{split}
\sum y_i &= \sum \bigg(\beta_0 + \beta_1(x_i)+ \epsilon_i\bigg) \\[1em]
&= \sum \beta_0 + \sum \beta_1(x_i) + \sum \epsilon_i \\[1em]
&= n \beta_0 + \beta_1 \sum (x_i) + \sum \epsilon_i \\[1em]
\frac{\sum y_i}{n} &= \frac{n \beta_0}{n} + \frac{\beta_1 \sum (x_i)}{n} + \frac{\sum \epsilon_i}{n}
\end{split}
$$

This last expression we can re-write as,

$$
\bar{y} = \beta_0 + \beta_1(\bar{x}) + \bar{\epsilon}
$$

Now we will substitute this into $\bar{y}$ in the definition of $B_0$.

$$
\begin{split}
B_0 &= \bar{y} - B_1(\bar{x}) \\[1em]
&= \beta_0 + \beta_1(\bar{x}) + \bar{\epsilon} - B_1(\bar{x}) \\[1em]
&= \beta_0 + \bar{x}\bigg(\beta_1 - B_1 \bigg) + \bar{\epsilon}
\end{split}
$$

Lastly, we take the expectation (conditional on $X$).

$$
\begin{split}
\mathbb{E}(B_0) &= \mathbb{E}\bigg(\beta_0 + \bar{x}(\beta_1 - B_1) + \bar{\epsilon}\bigg) \\[1em]
&= \mathbb{E}(\beta_0) + \mathbb{E}\bigg(\bar{x}(\beta_1 - B_1)\bigg) + \mathbb{E}(\bar{\epsilon}) \\[1em]
&= \beta_0 + \bar{x}\mathbb{E}(\beta_1 - B_1) + \mathbb{E}(\bar{\epsilon}) \\[1em]
&= \beta_0 + \bar{x}\bigg(\mathbb{E}(\beta_1) - \mathbb{E}(B_1)\bigg) + \mathbb{E}(\bar{\epsilon}) \\[1em]
&= \beta_0 + \bar{x}\bigg(\beta_1 - \mathbb{E}(B_1)\bigg) + \mathbb{E}(\bar{\epsilon}) 
\end{split}
$$

Using Assumption \#2, we find that $\mathbb{E}(\bar{\epsilon}) = 0$. We also just proved that $\mathbb{E}(B_1) = \beta_1$. So,

$$
\begin{split}
\mathbb{E}(B_0) &= \beta_0 + \bar{x}\bigg(\beta_1 - \beta_1\bigg) +0 \\[1em]
&= \beta_0 + \bar{x}(0) \\[1em]
&= \beta_0
\end{split}
$$

</div>

To prove that the estimators were unbiased, we only relied on a subset of the assumptions underlying the regression model, namely that the model was correctly specified (Assumption \#1) and that of linearity (Assumption \#2).

## Derivation of Sampling Variances for the Regression Estimators

We can derive the sampling variance for both $B_1$ and $B_0$. The sampling variances turn out to be:

$$
\begin{split}
\mathrm{Var}(B_1) &= \frac{\sigma^2_e}{\sum (x_i - \bar{x})^2} \\[1em]
\mathrm{Var}(B_0) &= \frac{\sigma^2_e \sum x_i^2}{n \sum (x_i - \bar{x})^2} \\[1em]
\end{split}
$$


To derive these quantities, we will have to take advantage of the assumptions that the conditional variances are equal (Assumption \#3) and independence of errors/observations conditional on $X$ (Assumption \#4). We will start by deriving the sampling variance for $B_1$.

<div class="proof">
Since $B_1 = \sum w_iy_i$, then

$$
\begin{split}
\mathrm{Var}(B_1) &= \mathrm{Var}(\sum w_iy_i) \\[1em]
&= \sum \mathrm{Var}(w_iy_i) \\[1em]
&= \sum w_i^2 \mathrm{Var}(y_i)
\end{split}
$$

Now, recall that the variance of $Y$ is conditional on $X$, then

$$
\begin{split}
\mathrm{Var}(y_i | x_i) &= \mathrm{Var}(\hat{y}_i + e_i \big| x_i) \\[1em]
&= \mathrm{Var}(\hat{y}_i \big| x_i) + \mathrm{Var}(e_i \big| x_i) + 2\mathrm{Cov}(\hat{y}_i,\epsilon_1 \big| x_i) \\[1em]
&= 0 + \mathrm{Var}(e_i \big| x_i) + 2(0) \\[1em]
&= \mathrm{Var}(e_i \big| x_i)  = \sigma^2_{\epsilon}
\end{split}
$$

So,

$$
\begin{split}
\mathrm{Var}(B_1) &= \sum w_i^2 \sigma^2_\epsilon \\[1em]
&= \sigma^2_\epsilon \sum w_i^2 \\[1em]
&= \sigma^2_\epsilon \sum \bigg(\frac{x_i - \bar{x}}{\sum (x_i - \bar{x})^2}\bigg)^2 \\[1em]
&= \sigma^2_\epsilon \frac{\sum (x_i - \bar{x})^2}{\sum (x_i - \bar{x})^2 \sum (x_i - \bar{x})^2} \\[1em]
&= \frac{\sigma^2_\epsilon}{\sum (x_i - \bar{x})^2}
\end{split}
$$

</div>

We can re-write this as

$$
\mathrm{Var}(B_1) = \frac{\sigma^2_\epsilon}{(n-1)s^2_x}
$$

This helps us think about when the precision of $B_1$ will be high (low variance and SE):

- When the error variance, $\sigma^2_\epsilon$, is small;
- When the sample size, $n$, is large; and
- When the variance in the predictor values, $s^2_x$, is large.


We can perform a similar derivation to obtain the sampling variance of $B_0$ (not shown). The formula for the sampling variance for $B_0$, also offers us insight for when the precision of $B_0$ will be high (low variance and SE):

$$
\mathrm{Var}(B_0) = \frac{\sigma^2_e \sum x_i^2}{n \sum (x_i - \bar{x})^2}
$$

Because the denominator is similar to that of $\mathrm{Var}(B_1)$, we will have high precision around $B_0$ when:

- The error variance, $\sigma^2_\epsilon$, is small;
- The sample size, $n$, is large; and
- The variance in the predictor values, $s^2_x$, is large.

But, because of the added sum term in the numerator, when $\bar{x} \approx 0$, then that will essentially cancel out the sum terms in the numerator and denominator, which will lead to higher precision, so also when

- The $X$-values are centered near 0.

## Covariance between the Regression Estimators

The covariance between $B_0$ and $B_1$ is defined as:

$$
\mathrm{Cov}(B_0, B_1) = \frac{\sigma^2_\epsilon \bar{x}}{\sum (x_i - \bar{x})^2}
$$

We can derive this using the covariance and expectations rules.

<div class="proof">

$$
\begin{split}
\mathrm{Cov}(B_0, B_1) &= \mathbb{E}\bigg[\big(B_0 - \mathbb{E}[B_0]\big)\big(B_1 - \mathbb{E}[B_1]\big)\bigg] \\[1em]
\mathrm{Cov}(B_0, B_1) &= \mathbb{E}\bigg[(B_0 - \beta_0)(B_1 - \beta_1)\bigg] \\[1em]
\end{split}
$$

Now we use the result that $B_0-\beta_0 = -\bar{x}(B_1 - \beta_1)$. You can show this by using the result that $\bar{y} = B_0 + B_1(\bar{x})$, which implies $B_0 = \bar{y} - B_1(\bar{x})$. Substituting $-\bar{x}(B_1 - \beta_1)$ in for $B_0-\beta_0$, we get:

$$
\begin{split}
\mathrm{Cov}(B_0, B_1) &= \mathbb{E}\bigg[-\bar{x}(B_1 - \beta_1)^2\bigg] \\[1em]
&= -\bar{x} \times \mathbb{E}\bigg[(B_1 - \beta_1)^2\bigg] \\[1em]
&= -\bar{x} \times \mathrm{Var}(B_1) \\[1em]
&= -\bar{x} \times \frac{\sigma^2_\epsilon}{\sum (x_i - \bar{x})^2}
\end{split}
$$

</div>

This formula provides insight into the sampling errors of the regression estimators. Since both $\sigma^2_\epsilon$ and $\sum (x_i - \bar{x})^2$ are values greater than zero, the covariance between $B_0$ and $B_1$ depends on the sign of $\bar{x}$.

- If $\bar{x} > 0$, then $\mathrm{Cov}(B_0,B_1) < 0$. This implies that the sampling errors $(B_0-\beta_0)$ and $(B_1-\beta_1)$ have opposite signs.
- If $\bar{x} < 0$, then $\mathrm{Cov}(B_0,B_1) > 0$. This implies that the sampling errors $(B_0-\beta_0)$ and $(B_1-\beta_1)$ have the same signs.





## Sampling Distributions for the Estimators

To derive the samling distributions for the estimators, which are the basis for statistical inference, we need to also take advantage of the normality assumption (**A.5**). Recall that we used the $t$-distribution with $n-2$ degrees of freedom to test hypotheses about the slope and intercept. The general form of a hypotheses test for a regression coefficient, is

$$
H_0: \beta_j = b \quad \mathrm{where~b~is~the~tested~value}
$$

To test this, we create a studentized test statistic using

$$
\frac{B_j - b}{\mathrm{SE}(B_j)}
$$

This statistic follows a $t$-distribution with $n-2$ degrees of freedom. 

Recall from introductory statistics if we could assume that the population was normally distributed, then the distribution of $T = \frac{\bar{y}-\mu}{\mathrm{SE}(\bar{y})}$ was $t$-distributed with $n-1$ degrees of freedom. Since $\bar{y}$ is a linear combination of the observations, the distribution of $\bar{y}$ is also normally distributed, and estimating the SE of $\bar{y}$ introduced additional error; making the distribution of $T$ follow a $t$-distribution.

<div class="proof">
This comes from a theorem which says that (1) if $Z$ is a standard normal variable and $W$ is chi-squared distributed with $\nu$ degrees of freedom, and (2) $Z$ and $W$ are independent, then

$$
T = \frac{Z}{\sqrt{W/\nu}}
$$
will have a $t$-distribution with $\nu$ degrees of freedom.

In the case of the test of the mean, we can write $T = \frac{\bar{y}-\mu}{\mathrm{SE}(\bar{y})}$ in this form as:

$$
T = \frac{\sqrt{n}(\bar{y} - \mu) / \sigma_y}{\sqrt{\bigg[(n-1)s^2_y / \sigma^2_y\bigg] / (n-1)}}
$$
Thus, the distribution of $T$ will be $t$-distributed with $n-1$ degrees of freedom.

</div>

For the regression estimators, under the assumption of normality (Assumption \#5), the least squares estimators are also normally distributed. This is true since $B_j$ is a linear combination of the observations, and we are assuming the observations to be normally distributed (linear shifts do not change the distribution). Thus, we have,

$$
\begin{split}
T &= \frac{B_j - b}{\mathrm{SE}(B_j)} \\[1em]
&= \frac{\frac{B_j - b}{\sigma_{B_j}}}{\frac{\mathrm{SE}(B_j)}{\sigma_{B_j}}}
\end{split}
$$

From this it is clear that the numerator of $T$ is a standard normal variable. The denominator is

$$
\begin{split}
\frac{\mathrm{SE}(B_j)}{\sigma_{B_j}} &= \sqrt{\frac{\mathrm{Var}(B_j)}{\sigma^2_{B_j}}} \\[1em]
&= \sqrt{\frac{\frac{\mathrm{MSE}}{\sum (x_i - \bar{x})^2}}{\frac{\sigma^2_\epsilon}{\sum (x_i - \bar{x})^2}}} \\[1em]
&= \sqrt{\frac{\mathrm{MSE}}{\sigma^2_\epsilon}} \\[1em]
&= \sqrt{\frac{\frac{\mathrm{SSE}}{n-2}}{\sigma^2_\epsilon}} \\[1em]
&= \sqrt{\frac{\mathrm{SSE}}{\sigma^2_\epsilon (n-2)}}
\end{split}
$$

At this point we rely on a common theorem from regression theory which says that $\frac{\mathrm{SSE}}{\sigma^2_\epsilon}$ is distributed as $\chi^2$ with $n-2$ degrees of freedom and is independent of both $B_0$ and $B_1$. Relying on this,

$$
\begin{split}
T &= \frac{B_j - b}{\mathrm{SE}(B_j)} = \frac{z}{\sqrt{\frac{\chi^2(n-2)}{ (n-2)}}}
\end{split}
$$

Since $z$ is a function of $B_0$ and $B_1$, then $z$ and $\chi^2$ are also independent, and it follows that $\frac{B_j - b}{\mathrm{SE}(B_j)}$ is $t$-distributed with $n-2$ degrees of freedom.



