---
title: "Assignment 05"
author: "Variable Reduction"
date: "Answer Key"
header-includes:
   - \usepackage{xcolor}
   - \definecolor{umn}{HTML}{FF2D21}
   - \usepackage{caption}
   - \captionsetup[table]{textfont={it}, labelfont={bf}, singlelinecheck=false, labelsep=period}
   - \captionsetup[figure]{textfont={it}, labelfont={bf}, singlelinecheck=false, labelsep=period}
   - \usepackage{floatrow}
   - \floatsetup[figure]{capposition=top}
   - \usepackage{xfrac}
   - \usepackage{booktabs}
   - \usepackage{float}
   - \floatstyle{plaintop}
   - \restylefloat{table}
   - \usepackage{siunitx}
   - \newcolumntype{d}{S[table-format=3.2]}
output: 
  pdf_document:
    highlight: tango
    latex_engine: xelatex
    fig_width: 6
    fig_height: 6
mainfont: "Sabon"
sansfont: "Helvetica Neue UltraLight"
monofont: "Inconsolata"
urlcolor: "umn"
always_allow_html: yes
---

<!-- Override the max of 10 columns in bmatrix-->
\makeatletter
\renewcommand*\env@matrix[1][c]{\hskip -\arraycolsep
  \let\@ifnextchar\new@ifnextchar
  \array{*\c@MaxMatrixCols #1}}
\makeatother


```{r setup, include=FALSE, message=FALSE}
library(broom)
library(car)
library(ggplot2)
library(dplyr)
library(knitr)
library(kableExtra)

knitr::opts_chunk$set(echo = TRUE, eval = TRUE)

iowa = readr::read_csv("~/Dropbox/My Mac (CEHD-m1752583)/Documents/github/epsy-8264/data/iowa-judges.csv")
```


This assignment is worth 22 points. 


## Exploratory Analysis

**1. Compute and report the eigenvalues of correlation matrix of the 12 predictors.**

```{r}
# Create correlation matrix
X = cor(iowa[ , 6:17])

#Compute eigen values
round(eigen(X)$values, 2)
```

\vspace{2em}

**2. Based on the eigenvalues, comment on whether there may be any potential collinearity problems. Explain.**

Yes these values portend collinearity problems. They sum of the reciprocals of the eigenvalues (`r round(sum(1/eigen(X)$values), 2)`) is not 0 and is greater than five times the number of predictors ($5k=60$).


\vspace{2em}

**3. Compute and report the condition index for each of the 12 predictors.**

```{r}
# Condition indices
round(sqrt(eigen(X)$values[1] / eigen(X)$values), 2)
```

\vspace{2em}

**4. Based on the condition indices, comment on whether there may be any potential collinearity problems. Explain.**

Yes these values portend collinearity problems. Several of the condition indices are greater than 15, and one is larger than 30.

\newpage

**5. Fit the standardized WLS model that regresses the standardized retention percentage on the 12 standardized predictors. Since the ratings are assigned based on different numbers of attorneys, use a weight equal to the number of respondents. Report the coefficient-level output, including the estimated coefficients (beta weights), standard errors, $t$-values, and $p$-values.**

```{r}
# Fit model
lm.1 = lm(scale(retention) ~ -1 + scale(knowledge) + scale(perception) + scale(punctuality) + 
            scale(attention) + scale(management) + scale(demeanor) + scale(clarity) + 
            scale(promptness) + scale(criticism) + scale(decision) + scale(courteous) + 
            scale(equality), data = iowa, weights = respondents)
```

```{r echo=FALSE}
tidy(lm.1) %>%
  mutate(term = c("Knowledge", "Perception",  "Punctuality", "Attention", "Management", "Demeanor", "Clarity", "Promptness",
                  "Criticism", "Decision", "Courteous", "Equality")) %>%
  kable(
    format = "latex",
    digits = c(0, 2, 3, 2, 3),
    caption = "Coefficients, standard errors, \\textit{t}-values, and \\textit{p}-values for the predictors included in a standardized model to explain variation in judge retention voting.",
    col.names = c("Predictor", "$\\beta$", "SE", "\\textit{t}", "\\textit{p}"),
    booktabs = TRUE,
    escape = FALSE,
    align = c("l", rep("d", 4))
  ) %>%
  kable_styling(latex_options = "HOLD_position") %>%
  row_spec(row = 0, align = c("l", "c", "c", "c", "c"))
```


\vspace{2em}

**6. Compute and report the variance inflation factors.**

```{r warning=FALSE}
# Compute VIF
vif(lm.1)
```


\vspace{2em}

**7. Interpret the largest VIF value.**

The largest VIF value is 96.5 and is associated with the scaled attention variable. This suggests that the variance for this coefficient is 96.5 times larger than it would be if all the variables in the model were independent. OR The SE for associated with the scaled attention coefficient is 9.8 times larger than it would be if all the variables in the model were independent.

\vspace{2em}

**8. Based on the VIF values, comment on whether there may be any potential collinearity problems. Explain.**

Yes these values portend collinearity problems. Most of the VIF values are greater than 10 indicating exacerbated estimates of the uncertainty associated with several of the coefficients.

\vspace{2em}


## Principal Components Analysis

In this section you are going to carry out the principal components analysis by using singular value decomposition on the correlation matrix of the predictors.

**9. Carry out the singular value decomposition on the correlation matrix of the predictors. Report the D matrix.**

```{r}
# Singular value decomposition
diag(round(svd(X)$d, 3))
```



\vspace{2em}

**10. Report the standard deviation for the first principal component based on the values from the D matrix. Show your work.**

$$
\begin{split}
\sqrt{\lambda} &= \sqrt{9.935} \\[0.5em]
&\approx 3.15
\end{split}
$$

\newpage

**11. Compute and interpret the proportion of variance accounted for by the first principal component. Show your work.**

```{r}
# Proportion of variance
svd(X)$d[1] / sum(svd(X)$d)
```
The first principal component accounts for 82.8\% of the variation in the predictors.

\vspace{2em}

**12. Compute the composite score based on the first principal component for the first observation (Judge John J. Bauercamper). Show your work.**

```{r}
# Obtain vector of PC weights
pc1 = svd(X)$v[ , 1]

# Obtained scaled variable values for Obs. 1
judge_1 = scale(iowa[ , 6:17])[1, ]

# Compute composite value
sum(pc1 * judge_1)
```


$$
\begin{split}
\mathrm{PC}_1 &= -0.286(-0.892) -0.303(-1.072) -0.264(0.177) -0.308(-1.205) -0.299(-0.070) -0.290(-0.621) -0.292(-1.232)\\
&~~~~-0.246(-0.437) -0.294(-0.338) -0.304(-0.466) -0.276(-0.511) -0.295(-0.428) \\[0.5em]
&\approx 2.08
\end{split}
$$

*Note: Sign might be negative on the final answer.*

\vspace{2em}


## Choosing the Number of Principal Components

Read the section on scree plots (Section 4) [in this web article](https://medium.com/@bioturing/how-to-read-pca-biplots-and-scree-plots-186246aae063).

**13. Create a scree plot showing the eigenvalues for the 12 principal components from the previous analysis.**

```{r fig.width=10, fig.height=6, out.width='4.5in', fig.cap="Scree plot show the eigenvalues for each of the principal components."}
plot(x = 1:12, y = svd(X)$d, type = "b", xlab = "Principal Component", ylab = "Eigenvalue")
```


\vspace{2em}

**14. Using the "elbow criterion", how many principal components are sufficient to describe the data? Explain by referring to your scree plot.**

It looks like about two principal components are sufficient to describe the data based on where the elbow is located in the scree plot. The eigenvalues are pretty constant after the second principal component.

\newpage

**15. Using the "Kaiser criterion", how many principal components are sufficient to describe the data? Explain.**

```{r}
svd(X)$d > 1
```

Based on the Kaiser criterion, two principal components are sufficient to describe the data, as only two of the 12 components have an eigenvalue above 1.

\vspace{2em}

**16. Using the "80\% proportion of variance criterion", how many principal components are sufficient to describe the data? Explain.**

\vspace{2em}

```{r}
# Compute cumulative proportion of variance
cumsum(svd(X)$d / sum(svd(X)$d))
```

One principal components is sufficient to describe the data if we use the "80\% proportion of variance criterion", since the first PC explains more than 80\% of the variation.

\newpage

## Revisit the Regression Analysis

The evidence from the previous section suggests that the first two principal components are sufficient to explain the variation in the predictor space.


**17. By examining the pattern of correlations (size and directions) in the first two principal components, try to identify which constructs the composites based on these components define. Explain.**

The first principal component seems to be a general measurement of judges' demeanor and professional competence, as the signs of the weights are all in the same direction and the magnitudes of the weights are all of moderate size. The second PC seems to be measuring a contrast between the demeanor variables and the professional conduct variables, as the signs for these weights are in the opposite direction.

\vspace{2em}

**18. Fit the regression analysis using the first two principal components as predictors of retention percentage. (Don't forget your weights.)  Create and report the plot of the residuals vs. fitted values. Fit the same regression model, but this time also include a quadratic effect of the first principal component. Create and report the plot of the residuals vs. fitted values. Place these plots side-by-side and use the caption to comment on the assumption of linearity (the average residual is zero at each fitted value) for each model. (2pts)**


```{r fig.width=8, fig.height=4, out.width='6in', echo=FALSE, fig.cap="LEFT: Plot of the residuals versus the fitted values for Model 1 (linear effects). The curvature of the loess smoother indicates that the average residual at each fitted value is not 0. RIGHT: In contrast, the same plot for Model 2 (inc. quadratic effect of PC1) suggests that this assumption has been satisfied as the loess smoother follows the Y=0 line."}
# Create new data set with PC scores, outcome, and weights
iowa_pc = data.frame(prcomp(iowa[ , 6:17])$x) %>%
  mutate(
    retention = scale(iowa$retention),
    respondents = iowa$respondents
    )
  
# Fit Model 1
lm.1 = lm(retention ~ PC1 + PC2, data = iowa_pc, weights = respondents)

# Fit Model 2
lm.2 = lm(retention ~ PC1 + I(PC1 ^ 2) + PC2, data = iowa_pc, weights = respondents)

par(mfrow = c(1, 2))
residualPlot(lm.1)
residualPlot(lm.2)
par(mfrow = c(1, 1))
```



\newpage

**19. Examine the coefficient-level output and re-fit the model by removing any non-significant predictors from the model. Re-examine the coefficient-level output and create a plot showing the fitted values from the model as a function of the first principal component. Use this plot to interpret the quadratic nature of the effect. (2pts)**

```{r fig.width=6, fig.height=6, out.width='3.5in'}
# Remove PC2
lm.2.2 = lm(retention ~ PC1 + I(PC1 ^ 2), data = iowa_pc, weights = respondents)

# Coefficient-level output
tidy(lm.2.2)

# Create plot
ggplot(data = iowa_pc, aes(x = PC1, y = retention)) +
  geom_point(size = 3, alpha = 0.3) +
  stat_function(
    fun = function(x) {0.157 + 0.580*x - 0.109 * x^2}, 
    color = "blue", 
    linetype = "solid"
  ) +
  theme_bw() +
  xlab("Principal Component 1") +
  ylab("Standardized retention")
```

The effect of the first principal component on retention is non-linear. The same amount of changes in PC values at lower levels of the scale are associated with higher rate of change in retention voting (on average) and less change in retention voting at higher levels of the PC. 

\vspace{2em}

**20. Based on Cook's *D*, identify the name of any judges (and their Cook's *D* value) that are influential observations.**

```{r}
# Cook's D cutoff
cutoff = 4/61

# Get judges with extreme Cook's D values
augment(lm.2.2) %>%
  mutate(judge = iowa$judge) %>%
  select(judge, .cooksd) %>%
  filter(.cooksd > cutoff)
```

*Note: If the decision is based on the index plot, only Rachael Seymour (obs. 45) and Deborah Farmer Minot (Obs. 49) would likely be chosen.*
