---
title: "Regression Using Matrices"
description: |
  An introduction to using matrix algebra to compute regression estimates (e.g., coefficients, standard errors, fitted values).
author:
  - name: Andrew Zieffler 
    url: http://zief0002.github.io/epsy-8264
date: "`r Sys.Date()`"
output: 
  distill::distill_article:
    highlight: zenburn
    css: ['style/notes.css']
bibliography: [epsy8264.bib]
csl: 'style/apa-single-spaced.csl'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```

Recall the model equation we use in simple linear regression

$$
Y_i = \beta_0 + \beta_1(X_i) + \epsilon_i
$$

where the response variable (*Y*) is represented as a linear function of the predictor (*X*) and a residual ($\epsilon$). Equation terms with an *i* subscript vary across subjects. Terms without an *i* subscript are the same (fixed) across subjects. 


<br />

# Representing the Model Equation Using Matrix Algebra

Using the subject-specific subscripts ($1, 2, 3, \ldots, n$), we can write out each subject's equation:

$$
\begin{split}
Y_1 &= \beta_0 + \beta_1(X_1) + \epsilon_1 \\
Y_2 &= \beta_0 + \beta_1(X_2) + \epsilon_2 \\
Y_3 &= \beta_0 + \beta_1(X_3) + \epsilon_3 \\
\vdots &~ ~~~~~\vdots ~~~~~~~~~~\vdots~~~~~~~~~~~~~\vdots  \\
Y_n &= \beta_0 + \beta_1(X_n) + \epsilon_n
\end{split}
$$

These can be arranged into a set of vectors and matrices, namely,

$$
\begin{split}
\begin{bmatrix}Y_1 \\ Y_2 \\ Y_3 \\ \vdots \\ Y_n\end{bmatrix} &= \begin{bmatrix}1 & X_1 \\ 1 & X_2 \\ 1 & X_3 \\ \vdots \\ 1 & X_n\end{bmatrix} \begin{bmatrix}\beta_0 \\ \beta_1\end{bmatrix}+ \begin{bmatrix}\epsilon_1 \\ \epsilon_2 \\ \epsilon_3 \\ \vdots \\ \epsilon_n\end{bmatrix} \\[2ex]
\mathbf{y} &= \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\epsilon}
\end{split}
$$

where,

- **y** is an $n \times 1$ vector of observations on the outcome variable.
- **X** is an $n \times k$ matrix (called the *design matrix*) consisting of a column of ones and the observations for *k* independent predictors. In the simple regression example, $k=2$, and the design matrix has two columns---a column of ones and a column of observations for the predictor *X*.
- $\boldsymbol\beta$ is a $k \times 1$ vector of unknown population parameters that we want to estimate. In the simpke regression model, **b** is a $2 \times 1$ vector consisting of $\beta_0$ and $\beta_1$.
- $\boldsymbol\epsilon$ is a $n \times 1$ vector of residuals.

Note that this model has a systematic (or deterministic) part ($\mathbf{X}\boldsymbol{\beta}$) and a stochastic (or random) part ($\boldsymbol\epsilon$). In a regression analysis, one goal is often to estimate the values of the parameters in the $\boldsymbol\beta$ vector using sample data (i.e., the *Y* and *X* values.

<br />

# Estimating the Regression Coefficients

In ordinary least squares (OLS) estimation, the estimated coefficients minimize the sum of the squared sample residuals (i.e., the SSE), which in scalar form is $\mathrm{SSE}=\sum e_i^2$. Here we are using the notation $e_i$ to denote the sample residuals rather than $\epsilon_1$, which denotes the population residuals. The SSE can be expressed in matrix notation as: 

$$
\begin{split}
\mathrm{SSE} &= \mathbf{e}^{\intercal}\mathbf{e} \\[2ex]
&= \begin{bmatrix}\epsilon_1 & \epsilon_2 & \epsilon_3 & \ldots & \epsilon_n\end{bmatrix}\begin{bmatrix}\epsilon_1 \\ \epsilon_2 \\ \epsilon_3 \\ \vdots \\ \epsilon_n\end{bmatrix}
\end{split}
$$

Since we can also express the residual vector as $\mathbf{e} = \mathbf{y}-\mathbf{Xb}$, the SSE can also be expressed as:

$$
\mathrm{SSE} = (\mathbf{y}-\mathbf{Xb})^{\intercal}(\mathbf{y}-\mathbf{Xb})
$$

Here again, we are using **b** to denote that this denotes the coefficient estimates rather than the actual parameters. This can be re-written as:

$$
\begin{split}
\mathrm{SSE} &= \mathbf{y}^{\intercal}\mathbf{y} - \mathbf{b}^{\intercal}\mathbf{X}^{\intercal}\mathbf{y} - \mathbf{y}^{\intercal}\mathbf{X}\mathbf{b} + \mathbf{b}^{\intercal}\mathbf{X}^{\intercal}\mathbf{X}\mathbf{b} \\[2ex]
&= \mathbf{y}^{\intercal}\mathbf{y} - 2\mathbf{b}^{\intercal}\mathbf{X}^{\intercal}\mathbf{y} + \mathbf{b}^{\intercal}\mathbf{X}^{\intercal}\mathbf{X}\mathbf{b}
\end{split}
$$

To find the values for the elements in **b** that minimize the equation, we use calculus to differentiate this expression with respect to **b**

:::math
Although calculus, especially calculus on matrices, is beyond the scope of this course, @Fox:2009 gives the interested reader some mathematical background on optimization (i.e., minimizing). For now you just need to understand we can optimize a function by computing its derivative, setting the derivative equal to 0, and solve for any remaining unknowns.
:::

This gives the expression:

$$
\mathbf{X}^{\intercal}\mathbf{Xb} = \mathbf{X}^{\intercal}\mathbf{y}
$$

This expression is referred to as the *Normal Equations*. To solve for the elements in **b**, we pre-multiply both sides of the equation by $(\mathbf{X}^{\intercal}\mathbf{X})^{-1}$.

$$
\begin{split}
(\mathbf{X}^{\intercal}\mathbf{X})^{-1}(\mathbf{X}^{\prime}\mathbf{X})\mathbf{b} &= (\mathbf{X}^{\intercal}\mathbf{X})^{-1}(\mathbf{X}^{\intercal}\mathbf{y}) \\[2ex]
\mathbf{I}\mathbf{b} &= (\mathbf{X}^{\intercal}\mathbf{X})^{-1}\mathbf{X}^{\intercal}\mathbf{y} \\[2ex]
\mathbf{b} &= (\mathbf{X}^{\intercal}\mathbf{X})^{-1}\mathbf{X}^{\intercal}\mathbf{y}
\end{split}
$$

The vector of regression coefficients is given as:

$$
\mathbf{b} = (\mathbf{X}^{\intercal}\mathbf{X})^{-1}\mathbf{X}^{\intercal}\mathbf{y}
$$

This implies that the vector of regression coefficients can be obtained directly through manipulation of the design matrix and the vector of outcomes. In other words, the OLS coefficients is a direct function of the data. Note that as of yet, we have made no assumptions about the residuals. The coefficients can be estimated making no assumptions about the distributions of the residuals.

<br />

# Properties of the OLS Estimators

One property of the OLS estimators is that they minimize the sum of squared residuals. There are also several other properties that the OLS estimators have. Remember these estimators are based on the normal equations:

$$
\mathbf{X}^{\intercal}\mathbf{Xb} = \mathbf{X}^{\intercal}\mathbf{y}
$$
If we substitute $\mathbf{Xb}+\mathbf{e}$ in for **y** in this expression, we get:

$$
\begin{split}
\mathbf{X}^{\intercal}\mathbf{Xb} &= \mathbf{X}^{\intercal}(\mathbf{Xb}+\mathbf{e}) \\[2ex]
&= \mathbf{X}^{\intercal}\mathbf{Xb} + \mathbf{X}^{\intercal}\mathbf{e}
\end{split}
$$

To make this equality work, implies that:

$$
\mathbf{X}^{\intercal}\mathbf{e} = \mathbf{0}
$$
Let's examine $\mathbf{X}^{\intercal}\mathbf{e}$:

$$
\begin{split}
\mathbf{X}^{\intercal}\mathbf{e} &= \mathbf{0} \\[2ex]
\begin{bmatrix}1 & 1 & 1 & \ldots & 1\\X_1 & X_2 & X_3 & \ldots & X_n\end{bmatrix}\begin{bmatrix}\epsilon_1 \\ \epsilon_2 \\ \epsilon_3 \\ \vdots \\ \epsilon_n\end{bmatrix} &= \mathbf{0} \\[2ex]
\begin{bmatrix}\epsilon_1 + \epsilon_2 + \epsilon_3 + \ldots + \epsilon_n \\ X_1\epsilon_1 + X_2\epsilon_2 + X_3\epsilon_3 + \ldots + X_n\epsilon_n\end{bmatrix} &= \begin{bmatrix}0 \\ 0 \end{bmatrix}
\end{split}
$$

This implies that for every column in the design matrix, $\mathbf{X}_k$, that $\mathbf{X}_k^{\intercal}\mathbf{e}=0$. In other words, the dot product between $\mathbf{X}_k$ and $\mathbf{e}$ is zero indicating that the two vectors are independent.

**PROPERTY: The observed values of the predictor(s) are uncorrelated with the sample residuals.**

Note that this does not mean that the predictor(s) are uncorrelated with the residuals in the population; that is an assumption we will have to make later on.

<br />

## Properties of the OLS Regressors

If the regression model includes an intercept (the first column of the design matrix is a ones vector) then the following properties also hold. 

**PROPERTY: The sum of the residuals is 0.**

If the first column of the design matrix is a ones vector, then the first element of the $\mathbf{X}^{\intercal}\mathbf{e}$ matrix is $\epsilon_1 + \epsilon_2 + \epsilon_3 + \ldots + \epsilon_n = \sum \epsilon_i$, which is equal to zero since $\mathbf{X}^{\intercal}\mathbf{e}=\mathbf{0}$.

**PROPERTY: The sample mean of the residuals is zero.**

