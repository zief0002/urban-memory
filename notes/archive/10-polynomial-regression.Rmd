---
title: "Polynomial Regression"
description: |
  A brief introduction to polynomial regression and cross-validation for dealing with nonlinearity.
author:
  - name: Andrew Zieffler 
    url: http://www.datadreaming.org/
date: "`r Sys.Date()`"
output: radix::radix_article
bibliography: epsy8264.bib
csl: apa-single-spaced.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.width = 6, fig.height = 6)
library(knitr)
library(kableExtra)
```



There may be relationships that you want to model that are not linear. Polynomial regression is one methodology that we can use to model nonlinearity. To fit a polynomial regression we include additional predictors to the model that are obtained by raising each of the original predictors to a power. For example, to fit a cubic regression model we include the three predictors: $X$, $X^2$, and $X^3$. 

Adding polynomial terms to the model, like any other predictor correlated with the outcome will improve the fit of the model to the data. Figure 1 shows the results of fitting three different polynomial regression models to the *polynomial-example.csv* data and the resulting model-level $R^2$ values.

```{r echo=FALSE, message=FALSE, layout="l-body-outset", fig.height = 4, fig.width = 12}
# Load libraries
library(broom)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(readr)

# Read in data
example = read_csv("~/Dropbox/epsy-8264/data/polynomial-example.csv")

p1 = ggplot(data = example, aes(x = x, y = y)) +
  geom_point() +
  theme_bw() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(
    title = expression(paste(R^2, "= 0.913")),
    subtitle = "y ~ 1 + x"
  )

p2 = ggplot(data = example, aes(x = x, y = y)) +
  geom_point() +
  theme_bw() +
  geom_smooth(method = "lm", se = FALSE, formula = y~poly(x, 5)) +
  labs(
    title = expression(paste(R^2, "= 0.943")),
    subtitle = expression(paste("y ~ 1 + x + ", x^2, " + ", x^3, " + ", x^4, " + ", x^5))
  )

p3 = ggplot(data = example, aes(x = x, y = y)) +
  geom_point() +
  theme_bw() +
  geom_smooth(method = "lm", se = FALSE, formula = y~poly(x, 9)) +
  labs(
    title = expression(paste(R^2, "= 1.000")),
    subtitle = expression(paste("y ~ 1 + x + ", x^2, " + ", x^3, " + ", x^4, " + ", x^5, " + ", x^6, " + ", x^7, " + ", x^8, " + ", x^9))
  )

grid.arrange(p1, p2, p3, nrow = 1)
```

By *saturating* the model with polynomial terms we can obtain perfect fit to the data. Unfortunately, this 8th-degree polynomial model is unlikely to generalize to new data sets. We have overfitted the model to the data. Also, interpolated predictions (e.g., at $x = 50$) are suspect, at best. Even models with lower-order polynomials tend to be overfitted to the data. Fortunately, there are several methods we can use to avoid overfitting.

## Bluegill Example

The data in *bluegills.csv* include measurements taken on 78 bluegills from [Lake Mary](https://www.dnr.state.mn.us/lakefind/search.html?name=Lake+Mary&county=21) in Minnesota. On each fish, a key scale was removed. The age of a fish is determined by [counting the number of annular rings on the scale](https://fishbio.com/field-notes/inside-fishbio/reading-scales). The goal of the analysis is to relate length of the fish at capture to the age of the fish. These data were collected by Richard Frie, and discussed in [@Weisberg:1986; @Weisberg:2005].

```{r out.width="50%"}
# Load libraries
library(broom)
library(car)
library(corrr)
library(dplyr)
library(ggplot2)
library(gridExtra)
library(readr)

# Import data
bluegills = read_csv("~/Dropbox/epsy-8264/data/bluegills.csv")
head(bluegills)

# Plot of length versus age
ggplot(data = bluegills, aes(x = age, y = length)) +
  geom_point() +
  theme_bw()
```

The scatterplot suggests that there may be a nonlinear relationship between the age and length of a fish. To examine this, we will fit and evaluate three potential models: a first-degree polynomial (linear); a second-degree polynomial (quadratic); and a third-degree polynomial (cubic). There are many methods for fitting polynomial models.

<aside>
The residual plot based on fitting the linear model also suggests potential nonlinearity.

```{r echo=FALSE}
augment(lm(length ~ 1 + age, data = bluegills)) %>% 
  ggplot(aes(x = .fitted, y = .std.resid)) +
    geom_point() +
    geom_hline(yintercept = 0) +
    xlab("Fitted values") +
    ylab("Standardized residuals") +
    theme_bw()
```
</aside>

- Create the polyomial terms in the data and use them in the `lm()` function.
- Create the polynomial terms directly in the `lm()` function without creating them in the data.
- Use the `poly()` function to create the polynomial terms in the `lm()` function.

There are benefits and costs to each of these methods, and each may be useful depending on the situation.

#### Method 1: Create Polynomial Terms in the Data

The first method for fitting a polynomial regression model is to create the polyomial terms in the data and then use these terms in the `lm()` function. To do this, we will first create the $age^2$ and $age^3$ terms in the data set.

```{r}
# Create the quadratic and cubic terms
bluegills = bluegills %>%
  mutate(
    age_quad = age ^ 2,
    age_cubic = age ^ 3
  )

head(bluegills)
```

Then we can include them in the models. For example, to fit the cubic (third-degree) polynomial model:

```{r}
lm.3 = lm(length ~ 1 + age + age_quad + age_cubic, data = bluegills)
tidy(lm.3)
glance(lm.3)
```

#### Method 2: Create Polynomial Terms in the lm() Function

The second method for fitting a polynomial regression model creates the polynomial terms directly in the `lm()` function. If you use this method you will not have to `mutate()` new terms into the data. To create the terms directly in the `lm()` we use the `I()` function:

```{r}
lm.3 = lm(length ~ 1 + age + I(age^2) + I(age^3), data = bluegills)
tidy(lm.3)
glance(lm.3)
```

#### Method 3: Use poly() Function in the lm() Function

The third method is to use the `poly()` function. This function will create a set of polynomial terms for a provided variable and degree. We also include the argument `raw=TRUE`. For example, the cubic polynomial can be fitted as:

```{r}
lm.3 = lm(length ~ 1 + poly(age, 3, raw = TRUE), data = bluegills)
tidy(lm.3)
glance(lm.3)
```

All three methods produce the same coefficient- and model-level output. Using the `poly()` function is the most efficient, but there are functions that won't work with this method (e.g., `augment()` to create the regression diagnostics). I typically use the `I()` method in my own work.

## Evaluating the Polynomial Terms: Statistical Inference

We will begin our analysis by fitting five polynomial models; 1st-degree through 5th-degree polynomials.

```{r}
lm.1 = lm(length ~ 1 + age,                                             data = bluegills)
lm.2 = lm(length ~ 1 + age + I(age^2),                                  data = bluegills)
lm.3 = lm(length ~ 1 + age + I(age^2) + I(age^3),                       data = bluegills)
lm.4 = lm(length ~ 1 + age + I(age^2) + I(age^3) + I(age^4),            data = bluegills)
lm.5 = lm(length ~ 1 + age + I(age^2) + I(age^3) + I(age^4) + I(age^5), data = bluegills)
```

One method of evaluating the polynomial models, since the models are nested, is to use an $F$-test to test the change in $R^2$. The hypothesis for this is that:

$$
H_0: R^2_{\mathrm{Simple~Model}} = R^2_{\mathrm{Complex~Model}}
$$

For example to compare the linear to the quadratic polynomial model,

$$
H_0: R^2_{\mathrm{Linear~Model}} = R^2_{\mathrm{Quadratic~Model}}
$$

We can evaluate this by submitting both models to the `anova()` function.

```{r}
anova(lm.1, lm.2)
```

Here we would reject the null hypothesis; $F(1, 8921) = 25$, $p < .001$. This suggests it is likely that the quadratic age term explains additional variation in fish length above and beyond the linear term.

To evaluate the necessity of the cubic term, we compare the quadratic model to the cubic model, etc. We can include a sequence of nested models in the `anova()` function to carry out all of these tests simultaneously.

```{r}
anova(lm.1, lm.2, lm.3, lm.4, lm.5)
```

Based on these results, we would adopt the quadratic polynomial model. Here we fail to reject the null hypothesis that the cubic term explains additional variation over and above the quadratic model; $F(1, 8916) = 0.04$, $p = 0.84$. It is unlikely that the cubic age term explains additional variation in fish length above and beyond the linear and quadratic terms. This would lead us to adopt the quadratic model.

## Reporting the Results

When we report the results of a polynomial regression model evaluated using statistical inference, we typically report the model-level ($R^2$, $F$, $p$), and coefficent-level ($B$, $SE$, $p$) summaries for the adopted model. Interpretations of the model are also provided. However, since a polynomial model is essentially an interaction model (interaction of a predictor with itself) the constituent main-effects are not typically interpreted. Providing a plot of the fitted model is generally the easiest way to help facilitate these interpretations.

At the model level:

```{r}
glance(lm.2)
```

The quadratic polynomial model using age to predict variation in fish length is statsitically significant; $F(2, 75)=151$, $p<.001$. The model explains 79.6\% of the variation in fish length.

At the coefficient-level:

```{r}
tidy(lm.2)
```


The fitted equation would be:

$$
\hat{\mathrm{Fish~Length}}_i = 13.62 + 54.05(\mathrm{Age}_i) - 4.72(\mathrm{Age}^2_i)
$$

Since this model is an interaction model, we would only interpret the coefficient associated with the highest order interaction term (quadratic term) and not the constituent lower-order effects (linear term). Here, the significant quadratic term implies that the effect of age on length depends on age. For a better interpretation, we can plot the fitted model and interpret the findings from the plot.

```{r}
data.frame(
  age = seq(from = 1, to = 6, by = 0.1) #Set up sequence of x-values
  ) %>%
  mutate(
    yhat = predict(lm.2, newdata = .) #Get y-hat values based on model
  ) %>%
  ggplot(aes(x = age, y = yhat)) +
    geom_line() +
    theme_bw() +
    xlab("Age") +
    ylab("Predicted length")
```

This shows the negative quadratic relationship (upside-down U-shape) between age and length. For younger fish, there is a large positive relationship between age and length. This relationship diminishes as fish age and may even be negative for older fish (although in our data that is extrapolation).

For simple relationships, you can include polynomial regression smoothers of the data by using the `formula=` argument in the `geom_smooth()` layer of a ggplot. For example, the ggplot below includes a linear, quadratic, and cubic polynomial regression smoother. 


```{r fig.cap='Plot of the bluegill data and three fitted polynomial models: linear (blue), quadratic (red), and cubic (yellow).'}
ggplot(data = bluegills, aes(x = age, y = length)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) + 
  geom_smooth(method = "lm", se = FALSE, formula = y~poly(x, 2, raw = TRUE), color = "red") +
  geom_smooth(method = "lm", se = FALSE, formula = y~poly(x, 3, raw = TRUE), color = "yellow") +
  theme_bw() 
```
  
Note that within the bounds of our $X$-values, the quadratic and cubic polynomials are all but indistinguishable.

## Evaluating the Polynomial Terms: Cross-Validation

While hypothesis testing and $p$-values are an important tool, they can lead to several problems:

- Increased Type I error rates: We carried out two tests to adopt the quadratic model. We probably should have adjusted the $p$-values obtained from the $F$-tests to accommodate this. 
- Overfit/Lack of generalization: The adopted model still has the potential to not perform well on future data.

A better method for model selection, that does not use testing, is *cross-validation*. To perform cross-validation, you randomly split the data set into two parts: a *training* set, and a *validation* set. Any candidate models are then fitted on the training data and evaluated using the validation data. Because we use a different data set to evaluate the models, the resulting evaluation is not biased toward the data we used to fit the model (i.e., less overfit). Because of this, we get better indications about the generalizability of the models to new/future data.

In our polynomial regression example, the steps for performing the cross-validation are:

1. Randomly divide the bluegill data into two sets of observations; training and validation data sets.
2. Fit all five candidate models (1st--5th degree polynomials) to the training observations. 
3. Use the estimated coefficients from those fits to estimate fitted values and residuals for the observations in the validation data. 
4. Based on the residuals from the validation observations, compute measures of model performance (e.g., $R^2$, MSE).

### Divide the Data into a Training and Validation Set

There are many ways to do this, but I will use the `sample()` function to randomly sample 39 case numbers, from 1 to 78 (the number of rows in the bluegills data), to make up the training data. Then I use the `filter()` function to select those cases. The remaining cases (those 39 observations not sampled) are put into a validation set.

```{r}
# Make the random sampling replicable
set.seed(42)

# Select the cases to be in the training set
training_cases = sample(1:nrow(bluegills), size = 39, replace = FALSE)

# Create training data
train = bluegills %>%
  filter(row_number() %in% training_cases)

# Create validation data
validate = bluegills %>%
  filter(!row_number() %in% training_cases)
```

### Fit the Candidate Models to the Training Data

We now fit the five candidate models to the observations in the training data.

```{r}
lm.1 = lm(length ~ 1 + age,                                             data = train)
lm.2 = lm(length ~ 1 + age + I(age^2),                                  data = train)
lm.3 = lm(length ~ 1 + age + I(age^2) + I(age^3),                       data = train)
lm.4 = lm(length ~ 1 + age + I(age^2) + I(age^3) + I(age^4),            data = train)
lm.5 = lm(length ~ 1 + age + I(age^2) + I(age^3) + I(age^4) + I(age^5), data = train)
```

### Fit the Models Obtained to the Validation Observations

Now we can obtain the predicted lengths for the observations in the validation data based on the coefficients from the models fitted to the training data.

```{r}
# Get the predicted values for the validation data
yhat_1 = predict(lm.1, newdata = validate)
yhat_2 = predict(lm.2, newdata = validate)
yhat_3 = predict(lm.3, newdata = validate)
yhat_4 = predict(lm.4, newdata = validate)
yhat_5 = predict(lm.5, newdata = validate)
```


These vectors of $\hat{Y}$ values can be used to compute the residuals for the validation observations, which in turn can be used to compute the Cross-Validated Mean Square Error (CV-MSE). The CV-MSE is computed as:

$$
\mathrm{CV\mbox{-}MSE} = \frac{1}{n} \sum_{i=1}^n e_i^2
$$

where $n$ is the number of observations in the validation set. We can compute the CV-MSE for each of the candidate models.

```{r}
sum((validate$length - yhat_1) ^ 2) / nrow(validate)
sum((validate$length - yhat_2) ^ 2) / nrow(validate)
sum((validate$length - yhat_3) ^ 2) / nrow(validate)
sum((validate$length - yhat_4) ^ 2) / nrow(validate)
sum((validate$length - yhat_5) ^ 2) / nrow(validate)
```

The candidate model having the smallest CV-MSE is the model that should be adopted. In this case, the values for the CV-MSE suggest adopting the quadratic or the cubic polynomial models.

## Leave-One-Out Cross-Validation

There are two major problems with the cross-validation we just performed.

1. The estimate of the CV-MSE is highly dependent on the observations chosen to be in the training and validation sets. This is illustrated in the figure below which shows the CV-MSE values for the three models for 10 different random splits of the data. Some of these splits support adopting the quadratic  polynomial model, other splits support adopting the cubic polynomial model, and one supports the linear model!


```{r fig.width = 8, fig.height = 6}
my_list = list()

set.seed(42)

my_seeds = sample(1:500, size = 9)

choose_seed = c(42, my_seeds)

for(i in 1:10){
  
  set.seed(choose_seed[i])  
  
  # Select the cases to be in the training set
  training_cases = sample(1:nrow(bluegills), size = 39, replace = FALSE)

  # Create training data
  train = bluegills %>%
    filter(row_number() %in% training_cases)
  
  # Create validation data
  validate = bluegills %>%
    filter(!row_number() %in% training_cases)

  lm.1 = lm(length ~ 1 + age,                                             data = train)
  lm.2 = lm(length ~ 1 + age + I(age^2),                                  data = train)
  lm.3 = lm(length ~ 1 + age + I(age^2) + I(age^3),                       data = train)
  lm.4 = lm(length ~ 1 + age + I(age^2) + I(age^3) + I(age^4),            data = train)
  lm.5 = lm(length ~ 1 + age + I(age^2) + I(age^3) + I(age^4) + I(age^5), data = train)
  
  yhat_1 = predict(lm.1, newdata = validate)
  yhat_2 = predict(lm.2, newdata = validate)
  yhat_3 = predict(lm.3, newdata = validate)
  yhat_4 = predict(lm.4, newdata = validate)
  yhat_5 = predict(lm.5, newdata = validate)
  
  my_list[[i]] = data.frame(
    degree = c(1, 2, 3, 4, 5),
    cv_mse = c(
      sum((validate$length - yhat_1)^2) / nrow(validate), 
      sum((validate$length - yhat_2)^2) / nrow(validate), 
      sum((validate$length - yhat_3)^2) / nrow(validate),
      sum((validate$length - yhat_4)^2) / nrow(validate),
      sum((validate$length - yhat_5)^2) / nrow(validate)
      ),
    data_set = i
  )
}

my_results = do.call(rbind, my_list)

ggplot(data = my_results, aes(x = degree, y = cv_mse, group = factor(data_set), color = factor(data_set))) +
  geom_line() +
  theme_bw() +
  scale_x_continuous(name = "Degree Polynomial", breaks = 1:5) +
  scale_y_continuous(name = "CV-MSE", limits = c(0, 1000)) +
  ggsci::scale_color_d3(name = "") +
  guides(color = FALSE)
```

2. Only a subset of the observations (those in the training set) are used to initially fit the model. Since statistical methods tend to perform worse when trained on fewer observations, this suggests that the CV-MSE may tend to overestimate the error rate (model accuracy measure) for the model fit.

Leave-one-out cross-validation (LOOCV) is one method that can be used to overcome these issues. The algorithm for performing LOOCV is:

1. Hold out the $i$th observatin as your validation data (a single observation) and use the remaining $n-1$ observations as your training data.
3. Fit all candidate models to the training data. 
4. Use the estimated coefficients from those fits to compute $\mathrm{MSE}_i$ using the validation data.
5. Repeat Steps 2--4 for each observation.

We then have $n$ estimates of the MSE that can be averaged to get the overall CV-MSE.

$$
\mathrm{CV\mbox{-}MSE} = \frac{1}{n} \sum_{i=1}^n \mathrm{MSE}_i
$$
<aside>
Since the validation dataset is composed of a single observation ($n=1$), $\mathrm{MSE}_i$ is simply the squared residual value for the validation observation.

$$
\begin{split}
\mathrm{MSE} &= \frac{1}{n} \sum_{i=1}^n e_i^2 \\
&= \frac{1}{1} \sum_{i=1}^n e_i^2 \\
&= \sum_{i=1}^n e_i^2
\end{split}
$$
</aside>


We can carry out LOOCV by using a `for()` loop to iterate through the algorithm for each of the $n$ observations. Here is some example syntax:

```{r}
# Set up empty vector to store results
mse_1 = rep(NA, 78)
mse_2 = rep(NA, 78)
mse_3 = rep(NA, 78)
mse_4 = rep(NA, 78)
mse_5 = rep(NA, 78)

# Loop through the cross-validation
for(i in 1:78){
  train = bluegills %>% filter(row_number() != i)
  validate = bluegills %>% filter(row_number() == i)
  
  lm.1 = lm(length ~ 1 + age,                                             data = train)
  lm.2 = lm(length ~ 1 + age + I(age^2),                                  data = train)
  lm.3 = lm(length ~ 1 + age + I(age^2) + I(age^3),                       data = train)
  lm.4 = lm(length ~ 1 + age + I(age^2) + I(age^3) + I(age^4),            data = train)
  lm.5 = lm(length ~ 1 + age + I(age^2) + I(age^3) + I(age^4) + I(age^5), data = train)
  
  yhat_1 = predict(lm.1, newdata = validate)
  yhat_2 = predict(lm.2, newdata = validate)
  yhat_3 = predict(lm.3, newdata = validate)
  yhat_4 = predict(lm.4, newdata = validate)
  yhat_5 = predict(lm.5, newdata = validate)
  
  mse_1[i] = (validate$length - yhat_1) ^ 2
  mse_2[i] = (validate$length - yhat_2) ^ 2
  mse_3[i] = (validate$length - yhat_3) ^ 2
  mse_4[i] = (validate$length - yhat_4) ^ 2
  mse_5[i] = (validate$length - yhat_5) ^ 2
}


# Compute CV-MSE
mean(mse_1)
mean(mse_2)
mean(mse_3)
mean(mse_4)
mean(mse_5)
```

The LOOCV results point toward the linear model (smallest average CV-MSE), although the CV-MSE for the quadratic model is not that much larger. Since this method is less biased; it trains the models on a much larger set of observations, its results are more believable than the simple cross-validation. ANother advantage of LOOCV is that it will always produce the same results as opposed to simple cross-validation) since there is no randomness in producing the training and validation datasets.

LOOCV can be computationally expensive; we have to fit the candidate models $n$ times. It turns out, however, that models fit with least squares linear or polynomial regression, can give us the LOOCV results using the following formula:

$$
\mathrm{CV\mbox{-}MSE} = \frac{1}{n} \sum_{i=1}^n \bigg(\frac{e_i}{1-h_{ii}}\bigg)^2
$$

where $\hat{y}_i$ is the $i$th fitted value from the original least squares fit, and $h_{ii}$ is the leverage value. This is similar to the model (biased) MSE, except the $i$th residual is divided by $1 âˆ’ h_{ii}$. 

LOOCV is a very general method, and can be used with any kind of modeling. For example we could use it with logistic regression, or mixed-effects analysis, or any of the methods you have encountered in your statistics courses to date. That being said, the formula does not hold for all these methods, so outside of linear and polynomial regression, the candidate models will actually need to be fitted $n$ times.

## k-Fold Cross-Validation

One alternative to LOOCV is $k$-fold cross-validation. The algorithm for $k$-fold cross-validation is:

1. Randomly divide the bluegill data into $k$ groups or folds.
2. Hold out the $i$th fold as your validation data and use the remaining $k-1$ folds as your training data.
3. Fit all candidate models to the training data. 
4. Use the estimated coefficients from those fits to compute $\mathrm{MSE}_i$ using the validation data.
5. Repeat Steps 2--4 for each fold.

We then have $k$ estimates of the MSE that can be averaged to get the overall CV-MSE.

$$
\mathrm{CV\mbox{-}MSE} = \frac{1}{k} \sum_{i=1}^k \mathrm{MSE}_i
$$

From the ideas presented in the algorithm, it is clear that LOOCV is a special case of $k$-fold cross-validation in which $k$ is set to equal $n$. In practice we typically use $k=5$ or $k=10$.


We can carry out $k$-fold cross-validation by using a `for()` loop to iterate through the algorithm for each of the $k$ folds. Here is some example syntax to carry out a 10-fold cross-validation:

```{r}
# Set up empty vector to store results
mse_1 = rep(NA, 10)
mse_2 = rep(NA, 10)
mse_3 = rep(NA, 10)
mse_4 = rep(NA, 10)
mse_5 = rep(NA, 10)

# Randomize the order of the bluegill data and assign to folds
set.seed(100)
random_order = sample(1:nrow(bluegills), replace = FALSE)

rand_bluegills = bluegills[random_order, ] %>%
  mutate(
    # Create 10 equally size folds
    fold = cut(seq(1, nrow(bluegills)), breaks = 10, labels = FALSE)
  )

# Loop through the cross-validation
for(i in 1:10){
  
  validate_data = rand_bluegills %>% filter(fold == i)
  train_data    = rand_bluegills %>% filter(fold != i)
  
  lm.1 = lm(length ~ 1 + age,                                             data = train)
  lm.2 = lm(length ~ 1 + age + I(age^2),                                  data = train)
  lm.3 = lm(length ~ 1 + age + I(age^2) + I(age^3),                       data = train)
  lm.4 = lm(length ~ 1 + age + I(age^2) + I(age^3) + I(age^4),            data = train)
  lm.5 = lm(length ~ 1 + age + I(age^2) + I(age^3) + I(age^4) + I(age^5), data = train)
  
  yhat_1 = predict(lm.1, newdata = validate)
  yhat_2 = predict(lm.2, newdata = validate)
  yhat_3 = predict(lm.3, newdata = validate)
  yhat_4 = predict(lm.4, newdata = validate)
  yhat_4 = predict(lm.5, newdata = validate)
  
  mse_1[i] = (validate$length - yhat_1) ^ 2
  mse_2[i] = (validate$length - yhat_2) ^ 2
  mse_3[i] = (validate$length - yhat_3) ^ 2
  mse_4[i] = (validate$length - yhat_4) ^ 2
  mse_5[i] = (validate$length - yhat_5) ^ 2
}

# Compute CV-MSE
mean(mse_1)
mean(mse_2)
mean(mse_3)
mean(mse_4)
mean(mse_5)
```

The results of carrying out the 10-fold cross-validation suggest that we adopt the quadratic model.

Using $k$-fold cross-validation is computationally less expensive so long as $k<n$. But this computational gain has a cost in that the results are again dependent on the $k$ random splits. This variation, however, is less than that in the single split simple cross-validation. We can also alleviate some of this by fitting the $k$-fold cross-validation several times and averaging across the results to get the CV-MSE estimate.

<aside>
There are several R packages that will fit a $k$-fold cross-validation (e.g., **DAAG**). 
</aside>

## Model Selection

The different methods of selecting variables suggest we adopt the following models:

```{r echo=FALSE}
tab_01 = data.frame(
  Method = c("Nested F-Tests", "Simple CV", "LOOCV", "10-Fold CV"),
  Model = c("Quadratic", "Quadratic/Cubic", "Linear", "Quadratic")
)

kable(tab_01)
```

<aside>
While the different CV methods can suggest different models, this is usually less problematic when the sample size is larger; we only have $n=78$ in the bluegill data.
</aside>

The LOOCV and $k$-fold CV are better suited for considering how the model will perform on future data sets, so I would adopt the linear or the quadratic models at this point. To get the coefficient estimates, we fit whichever model we adopt to the FULL data set (with all the observations) as this will give us the "best" estimates.

## Reporting Results from a Cross-Validation

When we report the results of a polynomial regression model evaluated using cross-validation, there are some subtle differences in what is reported. At the model-level, we report the $R^2$ from the adopted model fitted to the full-data. We also report the CV-MSE as a measure of the model error for future observations (generalized error).

At the coefficent-level we typically report the coefficient estimates and standard errors based on fitting he adopted model to the full data. However, we DO NOT REPORT NOR INTERPRET P-VALUES. The $p$-values do not take into account that the model was selected using cross-validation. Because of this they are incredibly misleading. As with any model, interpretations should also be offered, again typically by way of providing a plot of the fitted model to help facilitate these interpretations.

The quadratic polynomial model using age to predict variation in fish length is statsitically significant; $F(2, 75)=151$, $p<.001$. The model explains 79.6\% of the variation in fish length. The fitted equation is:

$$
\hat{\mathrm{Fish~Length}}_i = 13.62 + 54.05(\mathrm{Age}_i) - 4.72(\mathrm{Age}^2_i)
$$

This model suggests a negative quadratic relationship (upside-down U-shape) between age and fish length. For younger fish, there is a large positive relationship between age and length. This relationship diminishes as fish age and may even be negative for older fish (although in our data that is extrapolation).

<aside>

```{r echo=FALSE}
data.frame(
  age = seq(from = 1, to = 6, by = 0.1) #Set up sequence of x-values
  ) %>%
  mutate(
    yhat = predict(lm.2, newdata = .) #Get y-hat values based on model
  ) %>%
  ggplot(aes(x = age, y = yhat)) +
    geom_line() +
    theme_bw() +
    xlab("Age") +
    ylab("Predicted length")
```
</aside>

## Information Criteria

In EPsy 8252, we learned about using information criteria for model selection. In particular, we used AIC, BIC, and AICc for model selection. It turns out the AIC-based model selection and cross-validation are asymptotically equivalent @Stone:1977. For linear models, using BIC for model selection is symptotically equivalent to leave-$v$-out cross-validation when $v = n\bigg(1 - \frac{1}{\ln (n) - 1}\bigg)$ @Shao:1997.

As a practical note, computer simulations have suggested that the results from using AICc for model selection will, on average, be quite similar to those from cross-validation techniques. As such, AICc can be a useful alternative to the computationally expensive cross-validation methods. However, using AICc does not provide a measure of the model performance (MSE) in new datasets like cross-validation does.

```{r message=FALSE}
library(AICcmodavg)
aictab(
  cand.set = list(lm.1, lm.2, lm.3),
  modnames = c("Linear", "Quadratic", "Cubic")
  )
```




## Structural Collinearity

Let's imagine that we adopted the cubic polynomial model. This model includes two predictors: the linear and quadratic effects of age. The correlation matrix of these predictors suggests that the two predictors are highly correlated. (This is not surprising given that we created the quadratic and cubic terms from the linear term.)

<aside>
Note that we need to create the polynomial terms in the data to examine the correlation matrix.
</aside>

```{r}
bluegills %>%
  select(length, age, age_quad, age_cubic) %>%
  correlate()
```

If we look at the VIF indices associated with the coefficients from the quadratic model, we find that the sampling variances for the coefficients are grossly inflated. When we have collinearity that is a result of the structural form of the model (e.g., polynomials, interactions), we refer to it as *structural multicollinearity*.

```{r}
lm.3 = lm(length ~ 1 + age + I(age^2) + I(age^3), data = bluegills)
vif(lm.3)

# Effect on SEs
sqrt(vif(lm.3))
```

To alleviate this, we can center the age predictor and use the centered age to create the polynomial terms.

```{r}
bluegills = bluegills %>%
  mutate(
    c_age = as.numeric(scale(age, center = TRUE, scale = FALSE)),
    c_age2 = c_age ^ 2,
    c_age3 = c_age ^ 3
  )
```

The new correlation matrix using the predictors based on centered age show much smaller correlations between the predictors.

```{r}
bluegills %>%
  select(length, c_age, c_age2, c_age3) %>%
  correlate()
```


The model based on the polynomials created from the centered age shows much better VIF indices.

```{r}
lm.3_c = lm(length ~ 1 + c_age + I(c_age^2) + I(c_age^3), data = bluegills)
vif(lm.3_c)

# Effect on SEs
sqrt(vif(lm.3_c))
```


While the coefficients for the polynomials based on the centered age are different (they should be), the fitted model produces the same plot (just shifted to the left; centered).

```{r}
coef(lm.3_c)
```


```{r eval = FALSE}
data.frame(
  c_age = seq(from = -2.63, to = 2.37, by = 0.01) #Set up sequence of x-values
  ) %>%
  mutate(
    yhat = predict(lm.3_c, newdata = .) #Get y-hat values based on model
  ) %>%
  ggplot(aes(x = c_age, y = yhat)) +
    geom_line() +
    theme_bw() +
    xlab("Centered Age") +
    ylab("Predicted length")
```

```{r echo=FALSE, fig.width = 8, fig.height = 4}
p1 = data.frame(
  c_age = seq(from = -2.63, to = 2.37, by = 0.01) #Set up sequence of x-values
  ) %>%
  mutate(
    yhat = predict(lm.3_c, newdata = .) #Get y-hat values based on model
  ) %>%
  ggplot(aes(x = c_age, y = yhat)) +
    geom_point() +
    geom_line() +
    theme_bw() +
    xlab("Centered Age") +
    ylab("Predicted length") +
    ggtitle("Centered Age")

p2 = data.frame(
  age = seq(from = 1, to = 6, by = 0.1) #Set up sequence of x-values
  ) %>%
  mutate(
    yhat = predict(lm.3, newdata = .) #Get y-hat values based on model
  ) %>%
  ggplot(aes(x = age, y = yhat)) +
    geom_point() +
    geom_line() +
    theme_bw() +
    xlab("Age") +
    ylab("Predicted length") +
    ggtitle("Raw Age")

grid.arrange(p1, p2, nrow = 1)
```

### Orthogonal Polynomials

Another way to deal with the structural multicollinearity is to use *orthogonal polynomial contrasts*. In the design matrix for the polynomials based on the raw age values, each row is composed of $[1,x,x^2]$, where $x$ is age. Othogonal contrasts are a particular way of generating the design matrix so that the columns are linearly independent. To create these we use the `poly()` function with the argument `raw=FALSE` (or omit the `raw=` argument all together). Below are the first several rows of the design matrix for the: (1) polynomials based on raw age; (2) polynomials based on centered age; and (3) polynomials based on orthogonal contrasts. 

```{r}
# Polynomials based on raw age
lm.2 = lm(length ~ 1 + poly(age, 2, raw = TRUE), data = bluegills)
head(model.matrix(lm.2))

# Polynomials based on centered age
lm.2_center = lm(length ~ 1 + poly(c_age, 2, raw = TRUE), data = bluegills)
head(model.matrix(lm.2_center))

# Polynomials based on orthogonal contrasts
lm.2_ortho = lm(length ~ 1 + poly(age, 2), data = bluegills)
head(model.matrix(lm.2_ortho))

```

Similar to the centered values, the orthogonal polynomials produce different estimates of the coefficients and SEs, but the $p$-values for the highest order polynomial term and the interpretations from the plot of the resulting fitted model will be identical to that of the model created from the raw ages.

<aside>
Because of the unchanged inferences for the highest order term and the equivalent model interpretations, while dealing with structural multicollinearity can be convenient in some cases, it is not essential.
</aside>


