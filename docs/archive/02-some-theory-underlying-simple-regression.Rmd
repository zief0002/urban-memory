---
title: "Some Theory Underlying Simple Linear Regression"
date: "`r Sys.Date()`"
header-includes:
   - \usepackage{amsthm}
   - \usepackage{xcolor}
   - \usepackage{xfrac}
   - \usepackage[framemethod=tikz]{mdframed}
   - \usepackage{graphicx}
   - \usepackage{rotating}
   - \usepackage{booktabs}
   - \usepackage{caption}
   - \definecolor{umn}{RGB}{153, 0, 85}
   - \definecolor{umn2}{rgb}{0.1843137, 0.4509804, 0.5372549}
   - \definecolor{myorange}{HTML}{EA6153}
output: 
  pdf_document:
    includes:
      before_body: notes.tex
    highlight: tango
    latex_engine: xelatex
    fig_width: 6
    fig_height: 6
mainfont: "Minion Pro"
sansfont: "ITC Slimbach Std Book"
monofont: "Source Code Pro"
urlcolor: "umn2"
always_allow_html: yes
bibliography: epsy8264.bib
csl: apa-single-spaced.csl
nocite: | 
  @Fox:2016,
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

In these notes, we will examine the computations needed to derive the OLS simple regression coefficients, which can be computed using the following formulas:

$$
\begin{split}
B_1 &= \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2} \\[1em]
B_0 &= \bar{y} - B_1(\bar{x}) \\[1em]
\end{split}
$$


We will also mathematically develop several useful properties of the OLS simple regression model. These properties include:

- The OLS regression line passes through the means of both $X$ and $Y$;
- The average value of the residual is zero;
- The residual errors around the least-squares regression are uncorrelated with the
predictor variable $X$; and 
- The residual errors around the least-squares regression are also uncorrelated with the fitted values, $\hat{Y}$.

## Simple Regression Model

To begin, we will define the simple linear regression model, along with the concept of fitted values and residuals. The *population model* for simple linear regression, which uses Greek letters for the parameters, is

$$
y_i = \beta_0 + \beta_1(x_i) + \epsilon_i.
$$

We use Roman letters to denote the parameter estimates in the *sample model*, namely,

$$
y_i = B_0 + B_1(x_i) + e_i.
$$



### Fitted Values and Residuals

The fitted value, $\hat{y}_i$, is

$$
\hat{y}_i = B_0 + B_1(x_i),
$$

which implies that,

$$
y_i = \hat{y}_i + e_i.
$$

Solving this for $e_i$, we get

$$
\begin{split}
e_i &= y_i - \hat{y}_i \\
&= y_i - \bigg(B_0 + B_1(x_i)\bigg) \\
&= y_i - B_0 - B_1(x_i)
\end{split}
$$

### Empirical Example: Occupational Prestige

Here we will use the data in *duncan.csv* to illustrate the computation of the OLS simple regression coefficients and also verify several of the properties listed above. To do this, we will regress `prestige` on `income`.

```{r message=FALSE}
# Load libraries
library(broom)
library(dplyr)
library(readr)

# Read in Duncan data
duncan = read_csv("../data/duncan.csv")
head(duncan)

# Fit regression model
lm_1 = lm(prestige ~ 1 + income, data = duncan)
coef(lm_1) # Obtain coefficient estimates
```

Let's first verify the computational formulas for the OLS simple regression coefficients.

```{r message=FALSE}
x = duncan$income
x_bar = mean(x)

y = duncan$prestige
y_bar = mean(y)


# Compute B_1
B_1 = sum( (x - x_bar) * (y - y_bar) ) / sum( (x - x_bar)^2 )
B_1

# Compute B_0
B_0 = y_bar - B_1 * x_bar
B_0
```

Now we can also verify some of the properties of the OLS model.


```{r}
y_hat = fitted(lm_1) #Obtain fitted values
e = resid(lm_1)      #Obtain residuals

# Regression lines passes through (x_bar, y_bar)
B_0 + B_1 * x_bar
y_bar


# Average residual is zero
mean(e) #Zero within rounding

# Residuals are uncorrelated with X
cor(e, x) #Zero within rounding

# Residuals are uncorrelated with y_hat
cor(e, y_hat) #Zero within rounding
```

We can always verify certain properties and results using empirical data, but that often unsatisfactory. For example, does the property that the residuals have an average value of zero hold for all data sets? Or only for the data we verified the result on? Does the result of $-4.33 \times 10^{-17}$ really imply that the residuals are *uncorrelated* with $X$ ($r=0$)? Or do they just have a really weak relationship? 

Although empirically verifying results are a good first step, we need to do more than empirical verification on a single data set. This is where mathematics, especially the ideas of mathematical proof will help. The goal in the remainder of the notes is to derive these results more generally using mathematics.

\newpage

## Quantifying Model-Data Fit

In general, we have good fit when the residuals are generally small. This might lead you to define a good model as having minimal residuals,

$$
\sum e_i = 0,
$$

After all, the sum of the residuals quantifies the total model-data misfit, and the smallest we can make this sum is zero.

One problem with this is that any line that passes through the observation $(\bar{x},\bar{y})$ will have a sum of residuals that is equal to zero; $\sum e_i = 0$. To prove this, we begin with the regression model,

$$
y_i = B_0 + B_1(x_i) + e_i
$$

Then we make use of the fact that if a line passes through $(\bar{x},\bar{y})$, it will satisfy the following fitted equation, $\bar{y}=B_0 + B_1(\bar{x})$. We can subtract one of these quantities from both sides of the equation to maintain the equality in the regression model.

$$
\begin{split}
y_i - \bar{y} &= B_0 + B_1(x_i) + e_i - \bigg[B_0 + B_1(\bar{x})\bigg] \\
y_i - \bar{y} &= B_1\bigg[x_i - \bar{x}\bigg] + e_i \\
e_i &= y_i - \bar{y} - B_1\bigg[x_i - \bar{x}\bigg]
\end{split}
$$

Since we are interested in the sum of the residuals, we want to sum the left-hand side. However, to maintain the equality, we sum both sides.

$$
\begin{split}
\sum e_i &= \sum \bigg(y_i - \bar{y} - B_1\bigg[x_i - \bar{x}\bigg]\bigg) \\
&= \sum y_i - \sum \bar{y} - \sum B_1\bigg[x_i - \bar{x}\bigg] \\
&= \sum y_i - n\bar{y} - B_1 \bigg[\sum x_i - \sum \bar{x}\bigg] \\
&= \sum y_i - n\bar{y} - B_1 \bigg[\sum x_i - n\bar{x}\bigg]
\end{split}
$$

But, $\bar{x} = \frac{\sum x_i}{n}$, which means $n\bar{x} = \sum x_i$. Thus,

$$
\begin{split}
\sum e_i &= \underbrace{\sum y_i - n\bar{y}}_0 - B_1 \bigg[\underbrace{\sum x_i - n\bar{x}}_0\bigg] \\
&= 0
\end{split}
$$


So, if we use the criteria, $\sum e_i = 0$, this leads to an infinite number of solutions, since any line passing through $(\bar{x},\bar{y})$ will satisfy this criteria. Also, large negative residuals are just as bad as a large positive residuals when measuring misfit. Because of this, we need to be more specific about how we combine (sum) the residuals. There are two solutions that would seem to fix this problem.

- Minimize $\sum |e_i|$; Least absolute value regression
- Minimize $\sum e_i^2$; Least squares regression


## Least Squares Optimization

Using the sample regression model,

$$
\sum e_i^2 = \sum \bigg[y_i - B_0 - B_1(x_i)\bigg]^2
$$

We can write the sum of squared residuals as a function of the parameter estimates,

$$
f(B_0, B_1) = \sum \bigg[y_i - B_0 - B_1(x_i)\bigg]^2
$$

The problem of least squares regression is to find the parameter estimates, $B_0$ and $B_1$, which minimize the sum of squared residuals. Mathematically, we can take the partial derivatives of $f(B_0, B_1)$ with respect to $B_0$ and $B_1$; set those partial derivatives equal to zero, and solve for $B_0$ and $B_1$. After taking the partial derivatives, we get the following:

$$
\begin{split}
\frac{\partial f(B_0, B_1)}{\partial B_0} &= \sum (-1)(2)\bigg(y_i - B_0 - B_1(x_i)\bigg) \\
\frac{\partial f(B_0, B_1)}{\partial B_1} &= \sum (-x_i)(2)\bigg(y_i - B_0 - B_1(x_i)\bigg) \\
\end{split}
$$


(If you have previously taken a calculus course, verify this. If not, take my word for it.) These equations are then set equal to zero.

$$
\begin{split}
0 &= \sum (-1)(2)\bigg(y_i - B_0 - B_1(x_i)\bigg) \\
0 &= \sum (-x_i)(2)\bigg(y_i - B_0 - B_1(x_i)\bigg) \\
\end{split}
$$

Now we can use our summation rules to simplify these equations. Starting with the first equation:

$$
\begin{split}
0 &= \sum (-1)(2)\bigg(y_i - B_0 - B_1(x_i)\bigg) \\
&= -2 \sum \bigg(y_i - B_0 - B_1(x_i)\bigg) \\
&= -2 \bigg(\sum y_i - \sum B_0 - \sum B_1(x_i)\bigg) \\
&= -2 \bigg(\sum y_i - \sum B_0 - B_1 \sum (x_i)\bigg) \\
&= \sum y_i - \sum B_0 - B_1 \sum (x_i) \\
&= \sum y_i - n B_0 - B_1 \sum (x_i)
\end{split}
$$
And, re-arranging this, we get:

$$
\sum y_i = n B_0 + B_1 \sum (x_i)
$$

\newpage

Then, using summation rules on the second equation,

$$
\begin{split}
0 &= \sum (-x_i)(2)\bigg(y_i - B_0 - B_1(x_i)\bigg) \\
&= -2 \sum x_i \bigg(y_i - B_0 - B_1(x_i)\bigg) \\
&= -2 \sum \bigg(x_iy_i - B_0x_i - B_1x_i^2\bigg) \\
&= -2 \bigg(\sum x_iy_i - \sum B_0x_i - \sum B_1x_i^2\bigg) \\
&= -2 \bigg(\sum x_iy_i - B_0\sum x_i - B_1\sum x_i^2\bigg) \\
&= \sum x_iy_i - B_0\sum x_i - B_1\sum x_i^2
\end{split}
$$

Re-arranging this, we get:

$$
\sum x_iy_i = B_0\sum x_i + B_1\sum x_i^2
$$


## Normal Equations and Derivation of the Coefficient Estimators

Now we have a system of two equations, referred to as the *normal equations*, with two unknowns, $B_0$ and $B_1$. 

$$
\begin{split}
\sum y_i &= n B_0 + B_1 \sum (x_i) \\[1em]
\sum x_iy_i &= B_0\sum x_i + B_1\sum x_i^2
\end{split}
$$


We can use algebra to solve for $B_0$ and $B_1$. Here we will use the substitution method (see http://www.sosmath.com/soe/SE211105/SE211105.html) to solve the first normal equation for $B_0$ and then substitute this into the second normal equation.

$$
\begin{split}
\sum y_i &= n B_0 + B_1 \sum (x_i) \\[1em]
\frac{\sum y_i}{n} &= \frac{n B_0 + B_1 \sum (x_i)}{n} \\[1em]
\bar{y} &= B_0 + B_1(\bar{x}) \qquad \mathrm{which~means} \\[1em]
B_0 &= \bar{y} - B_1(\bar{x})
\end{split}
$$

\newpage

Now we substitute this into the second normal equation.

$$
\begin{split}
\sum x_iy_i &= B_0\sum x_i + B_1\sum x_i^2 \\[1em]
&= \bigg(\bar{y} - B_1(\bar{x})\bigg)\sum x_i + B_1\sum x_i^2 \\[1em]
&= \bar{y}\sum x_i - B_1(\bar{x})\sum x_i + B_1\sum x_i^2 \\[1em]
\sum x_iy_i - \bar{y}\sum x_i &= B_1\bigg(\sum x_i^2 - \bar{x}\sum x_i \bigg) \\[1em]
&= B_1\bigg(\sum x_i^2 - \bar{x}n\bar{x}  \bigg) \\[1em]
&= B_1\bigg(\sum x_i^2 - n\bar{x}^2 \bigg) 
\end{split}
$$

\rule{0.5\linewidth}{\linethickness} 

ASIDE: Here are two results which are useful.

$$
\begin{split}
\sum (x_i - \bar{x})^2 &= \sum \bigg(x_i^2 - 2(x_i)\bar{x} + \bar{x}^2\bigg) \\
&= \sum x_i^2 - \sum 2(x_i)\bar{x} + \sum \bar{x}^2 \\
&= \sum x_i^2 - 2\bar{x}\sum (x_i) + \sum \bar{x}^2 \\
&= \sum x_i^2 - 2\bar{x}(n\bar{x}) + n \bar{x}^2 \\
&= \sum x_i^2 - 2n\bar{x}^2 + n \bar{x}^2 \\
&= \sum x_i^2 -  n \bar{x}^2 \\
\end{split}
$$

Also,

$$
\begin{split}
\sum (x_i - \bar{x})(y_i - \bar{y}) &= \sum \bigg(x_iy_i - x_i\bar{y} - y_i\bar{x} + \bar{x}\bar{y}\bigg) \\
&= \sum  x_iy_i - \sum x_i\bar{y} - \sum y_i\bar{x} + \sum \bar{x}\bar{y} \\
&= \sum  x_iy_i - \bar{y}\sum x_i - \bar{x} \sum y_i + n\bar{x}\bar{y} \\
&= \sum  x_iy_i - \bar{y}n\bar{x} - \bar{x} n\bar{y} + n\bar{x}\bar{y} \\
&= \sum  x_iy_i - n\bar{x}\bar{y} \\
&= \sum  x_iy_i - \bar{y}\sum x_i
\end{split}
$$

\vspace{2em}

\rule{0.5\linewidth}{\linethickness} 

\newpage

We can use these results in our substitution:

$$
\begin{split}
\sum x_iy_i - \bar{y}\sum x_i &= B_1\bigg(\sum x_i^2 - n\bar{x}^2 \bigg) \\[1em]
&= B_1\sum (x_i - \bar{x})^2 \\[1em]
\sum (x_i - \bar{x})(y_i - \bar{y}) &= B_1\sum (x_i - \bar{x})^2 \\[1em]
B_1 &= \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2}
\end{split}
$$

The OLS coefficient estimators are then:

$$
\begin{split}
B_0 &= \bar{y} - B_1(\bar{x}) \\[1em]
B_1 &= \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2}
\end{split}
$$

These coefficients are uniquely defined, so long as $\sum (x_i - \bar{x})^2 \neq 0$. The only time this value is zero is if all the $x_i=\bar{x}$; there is no variation in the predictor. If there is no variation in $X$, then there are infinite solutions; any line that passes through the point $(\bar{x},\bar{y})$ would be a potential solution.



## Implication: OLS Line Passes through the Point $(\bar{x},\bar{y})$

Through solving the first normal equation we found that $\bar{y} = B_0 + B_1(\bar{x})$. This directly implies that the least squares regression line will always pass through $(\bar{x},\bar{y})$ since $\bar{y}$ is the predicted value for an $x$-value of $\bar{x}$.


## Implications: Sum and Average of the Residuals is Zero

Subsequently, the fact that the line passes through the observation $(\bar{x},\bar{y})$ also implies that the sum of the residuals from the least squares equation will be zero; $\sum e_i = 0$ (shown earlier).

Once we recognize that the sum of the residuals is zero, the average residual must be zero.

$$
\begin{split}
\bar{e} &= \frac{\sum e_i}{n} \\[1em]
&= \frac{0}{n} \\[1em]
&= 0
\end{split}
$$

\newpage

## Residual Standard Error

Although the OLS line has the property that its estimates minimize the sum of squared errors, that does not mean that the line fits the data well. It is worth quantifying how well the OLS line actually fits the data.

One answer to this question is to compute the *standard error of the regression*, or the *residual standard error* (aka: *root mean square error*; RMSE). 

$$
\mathrm{RMSE} = s_e = \sqrt{\frac{\sum e_i^2}{n-2}}
$$

This represents the "average" size of the residual in the metric of the $Y$-variable. For example, in the corrected Duncan data, RMSE = 17.4. 

```{r}
# Obtain RMSE
n = length(duncan$prestige)

rmse = sqrt( sum(e^2) / (n-2) )
rmse
```


On average, when using income to predict occupational prestige, we will have an error of roughly 17.4 prestige units. (If we can believe that the residuals are normally distributed, about 70% of the errors will be in the range of $\pm17.4$.)

## Simple Correlation

Social scientists also use the correlation coefficient as a measure of fit. It is important to realize that although the RMSE is an absolute measure of the regression fit, the correlation coefficient provides a relative measure of the regression fit. This means, we can only interpret it as a measure of regression fit in relation to another model.

The baseline we typically use is the model with no predictor; $X$ is not explanatory. This reduces our fitted model to:

$$
y_i = B_0 + e_i 
$$

If we minimize the sum of squared errors for this model, we find that the OLS estimate for $B_0$ is $\bar{y}$. Thus,

$$
\mathrm{TSS} = \sum e_i^2 = \sum(y_i - \bar{y})^2
$$

We refer to this value as the *sum of squares total*; SST. In contrast, the sum of squared residuals from the regression model that includes $X$ as a predictor is referred to as the *residual sum of squares* or *sum of squares error*; SSE.

$$
\mathrm{RSS} = \sum e_i^2 = \sum(y_i - \hat{y}_i)^2
$$

The difference between these two values is what we refer to as the *regression sum of squares*; SSReg.

$$
\mathrm{SSReg} = \mathrm{SST} - \mathrm{SSE}
$$

The SSReg indicates the reduction in the squared error of the residuals from the model with no predictors to the model that includes $X$ as a predictor. 

\newpage

The ratio of SSReg to the SST is the square of the correlation coefficient.

$$
r^2 = \frac{\mathrm{SSReg}}{\mathrm{SST}}
$$

To compute the correlation coefficient, we take the positive square root of this ratio when $B_1>0$ and the negative square root when $B_1 < 0$. Let's examine the relationship between the sums of squares in more detail. To start we will begin with an individual observation, $i$.

$$
\begin{split}
y_i &= y_i - \hat{y}_i + \hat{y}_i  \\
y_i - \bar{y} &= y_i - \hat{y}_i + \hat{y}_i -\bar{y} \\
y_i - \bar{y} &= (y_i - \hat{y}_i) + (\hat{y}_i -\bar{y}) \\
\end{split}
$$

Now we square both sides of the equation

$$
\begin{split}
(y_i - \bar{y})^2 &= \bigg[(y_i - \hat{y}_i) + (\hat{y}_i -\bar{y})\bigg]^2 \\
&= (y_i - \hat{y}_i)^2 + (\hat{y}_i -\bar{y})^2 + 2 (y_i - \hat{y}_i)(\hat{y}_i -\bar{y})
\end{split}
$$

Now we can sum both sides, distributing this over the quantitites on the right-side.

$$
\begin{split}
\sum (y_i - \bar{y})^2 &= \sum (y_i - \hat{y}_i)^2 + \sum (\hat{y}_i -\bar{y})^2 + 2 \sum (y_i - \hat{y}_i)(\hat{y}_i -\bar{y})
\end{split}
$$

Since this last term is zero, we have

$$
\begin{split}
\sum (y_i - \bar{y})^2 &= \sum (y_i - \hat{y}_i)^2 + \sum (\hat{y}_i -\bar{y})^2 \\
\mathrm{SST} &= \mathrm{SSE} + \sum (\hat{y}_i -\bar{y})^2 \\
\mathrm{SST} - \mathrm{SSE} &= \sum (\hat{y}_i -\bar{y})^2 \\
\mathrm{SSReg} &= \sum (\hat{y}_i -\bar{y})^2
\end{split}
$$

This process of decomposing the variation into "explained" and "unexplained" variation is referred to as *ANOVA decomposition*, or simply *ANOVA*. The `anova()` function is used to carry out an ANOVA decomposition in practice.

```{r}
anova(lm_1)

r2 = 30665 / (30665 + 13023)
r2

r = sqrt(r2)
r
```



## Correlation Coefficient: Take 2

The correlation can also be defined as the ratio of the covariance between two random variables and the product of their standard deviations:

$$
\rho = \frac{\mathrm{Cov}(X,Y)}{\mathrm{SD}(X)\mathrm{SD}(Y)} = \frac{\sigma_{XY}}{\sigma_X\sigma_Y}
$$

We define the sample covariance as

$$
s_{XY} = \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{n-1}
$$

Thus

$$
\begin{split}
r &= \frac{\frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{n-1}}{\sqrt{\frac{\sum (x_i - \bar{x})^2}{n-1} \times \frac{\sum (y_i - \bar{y})^2}{n-1}}} \\[1em]
&= \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum (x_i - \bar{x})^2\sum (y_i - \bar{y})^2}} \\[1em]
&= \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})\sum (y_i - \bar{y})} 
\end{split}
$$


```{r}
# Alternate computation of r
cov(x, y) / ( sd(x) * sd(y) )
```

\newpage

Notice if we multiply the correlation by the ratio $\frac{\mathrm{SD}(Y)}{\mathrm{SD}(x)}$ we get,

$$
\begin{split}
r \times \frac{\mathrm{SD}(Y)}{\mathrm{SD}(x)} &= \frac{\frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{n-1}}{\sqrt{\frac{\sum (x_i - \bar{x})^2}{n-1} \times \frac{\sum (y_i - \bar{y})^2}{n-1}}} \times \frac{\sqrt{\frac{\sum (y_i - \bar{y})^2}{n-1}}}{\sqrt{\frac{\sum (x_i - \bar{x})^2}{n-1}}}\\[1em]
&= \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum (x_i - \bar{x})^2}\sqrt{\sum (y_i - \bar{y})^2}} \times \frac{\sqrt{\sum (y_i - \bar{y})^2}}{\sqrt{\sum (x_i - \bar{x})^2}} \\[1em]
&= \frac{\sum (x_i - \bar{x})(y_i - \bar{y})}{\sum (x_i - \bar{x})^2} \\[1em]
&= B_1
\end{split}
$$

Drawing on this result, we can also compute $B_1$ using:

$$
\begin{split}
B_1 &= r \times \frac{\mathrm{SD}(Y)}{\mathrm{SD}(x)} \\[1em]
&= \frac{\mathrm{Cov}(X,Y)}{\mathrm{SD}(X)\mathrm{SD}(Y)} \times \frac{\mathrm{SD}(Y)}{\mathrm{SD}(x)} \\[1em]
&= \frac{\mathrm{Cov}(X,Y)}{\mathrm{SD}(X)} \times \frac{1}{\mathrm{SD}(X)} \\[1em]
&= \frac{\mathrm{Cov}(X,Y)}{\mathrm{Var}(X)}
\end{split}
$$

```{r}
# Compute B_1
cov(x, y) / var(x)
```

## Implications: Residuals are Uncorrelated with X and the Fitted Values

Now we can use the covariance rules to show that:

- The residuals from the least squares equation are *uncorrelated* with the $x$-values; and
- The residuals from the least squares equation are *uncorrelated* with the fitted-values.

First we will show that the residuals from the least squares equation are *uncorrelated* with the $x$-values. To do this, we need to recognize that if the covariance between $e_i$ and $x_i$ is zero, then the correlation between them will also be zero.


$$
\begin{split}
\mathrm{Cov}(e_i,x_i) &= \mathrm{Cov}\bigg(y_i - B_0 - B_1(x_i), x_i\bigg) \\[1em]
&= \mathrm{Cov}(y_i,x_i) - \mathrm{Cov}(B_0,x_i) - \mathrm{Cov}\bigg(B_1(x_i),x_i\bigg) \\[1em]
&= \mathrm{Cov}(y_i,x_i) - 0 - B_1\mathrm{Cov}(x_i,x_i) \\[1em]
&= \mathrm{Cov}(y_i,x_i) - B_1\mathrm{Var}(x_i) \\[1em]
\end{split}
$$

Since $B_1 = \frac{\mathrm{Cov}(x_i,y_i)}{\mathrm{Var}(x_i)}$, we can substitute this into the last expression.

$$
\begin{split}
\mathrm{Cov}(e_i,x_i) &= \mathrm{Cov}(y_i,x_i) - B_1\mathrm{Var}(x_i) \\[1em]
&= \mathrm{Cov}(y_i,x_i) - \frac{\mathrm{Cov}(x_i,y_i)}{\mathrm{Var}(x_i)} \times \mathrm{Var}(x_i) \\[1em]
&= \mathrm{Cov}(y_i,x_i) - \mathrm{Cov}(x_i,y_i) \\[1em]
&= 0
\end{split}
$$



We can similarly show that the residuals from the least squares equation are *uncorrelated* with the fitted-values using the covariance rules.

## References



