{
  "articles": [
    {
      "path": "about.html",
      "title": "Instructor and TA",
      "description": "This blog includes content for the Fall 2021 section of EPsy 8264.",
      "author": [],
      "contents": "\n\n\n\nInstructor: Andrew Zieffler Email: zief0002@umn.edu Physical Office: Education Sciences Building 178 Office Hours: Wednesday 9:00 AM‚Äì10:00 AM; and by appointment Virtual Office: If you want to meet virtually, send me a Google calendar invite and include a Zoom link.\n\n\n\n\n\n\n\n\n\n",
      "last_modified": "2021-09-10T09:41:53-05:00"
    },
    {
      "path": "assignments.html",
      "title": "Assignments",
      "description": "Assignment due dates.",
      "author": [],
      "contents": "\nBelow are the due dates for the assignments, as well as links to each assignment. The due dates may change at the instructor‚Äôs discretion. Any revised due dates will be announced in class and posted to the website.\n\n\nAssignment\n\n\nDue Date\n\n\nHTML\n\n\nüêß Assignment 01: Matrix Algebra for Linear Regression \n\n\nSept.¬†28\n\n\n\n\n\nAssignment 02: Simulating from the Regression Model\n\n\nOct.¬†07\n\n\n\n\n\nAssignment 03: Regression Diagnostics\n\n\nOct.¬†14\n\n\n\n\n\nAssignment 04: Using WLS to Model Data with Outliers\n\n\nOct.¬†21\n\n\n\n\n\nAssignment 05: Variable Reduction\n\n\nNov.¬†02\n\n\n\n\n\nAssignment 06: Shrinkage Methods\n\n\nNov.¬†09\n\n\n\n\n\nAssignment 07: Cross-Validation\n\n\nNov.¬†23\n\n\n\n\n\nAssignment 08: Bootstrapping and Prediction\n\n\nDec.¬†16\n\n\n\n\n\n\n\n",
      "last_modified": "2021-09-10T09:41:54-05:00"
    },
    {
      "path": "data.html",
      "title": "Data",
      "description": "Datasets and codebooks used in the course.",
      "author": [],
      "contents": "\n\n\nName\n\n\nData\n\n\nCodebook\n\n\nbluegills.csv\n\n\n  \n\n\n\n\n\n\n\ncanadian-prestige.csv\n\n\n  \n\n\n\n\n\n\n\ncontraception.csv\n\n\n  \n\n\n  \n\n\ncredit.csv\n\n\n  \n\n\n  \n\n\ndavis.csv\n\n\n  \n\n\n\n\n\n\n\ndavis-corrected.csv\n\n\n  \n\n\n\n\n\n\n\nduff.csv\n\n\n  \n\n\n\n\n\n\n\nduncan.csv\n\n\n  \n\n\n\n\n\n\n\neducation-expenditures.csv\n\n\n  \n\n\n\n\n\n\n\nequal-education-opportunity.csv\n\n\n  \n\n\n\n\n\n\n\nevaluations.csv\n\n\n  \n\n\n  \n\n\nfake-piecewise-data.csv\n\n\n  \n\n\n\n\n\n\n\ngalton.csv\n\n\n  \n\n\n\n\n\n\n\ngrade_data.csv\n\n\n  \n\n\n\n\n\n\n\nhouston.csv\n\n\n  \n\n\n\n\n\n\n\niowa-judges.csv\n\n\n  \n\n\n  \n\n\nloess.csv\n\n\n  \n\n\n\n\n\n\n\nmcycle.csv\n\n\n  \n\n\n  \n\n\nmpls-violent-crime.csv\n\n\n  \n\n\n  \n\n\npolynomial-example.csv\n\n\n  \n\n\n\n\n\n\n\nrelate.csv\n\n\n  \n\n\n\n\n\n\n\nslid.csv\n\n\n  \n\n\n\n\n\n\n\nstack-1979.csv\n\n\n  \n\n\n  \n\n\nstates-2019.csv\n\n\n  \n\n\n  \n\n\ntokyo-water-use.csv\n\n\n  \n\n\n\n\n\n\n\n\n\n",
      "last_modified": "2021-09-10T09:41:55-05:00"
    },
    {
      "path": "index.html",
      "title": "EPsy 8264",
      "author": [],
      "contents": "\nWelcome\nWelcome to EPsy 8264: Advanced Multiple Regression Analysis. This is an advanced seminar for doctoral students in education covering a diverse set of regression methodologies. We will begin with a brief review of the General Linear Model and establishment of a mathematical foundation for the estimation of regression coefficients and standard errors in these models through the use of matrix algebra. The course will also cover more advanced modeling techniques, such as regression diagnostics, WLS and sandwich estimation, PCA, shrinkage methods, model selection and local models.\n\n\n\nClassroom\nTuesday/Thursday (9:45‚Äì11:00): Appleby Hall 102\n\nSyllabus\nThe course syllabus is available here.\nMessage from Snoop Dogg about the syllabus\n\nTextbooks\nThe course textbook is available via the University of Minnesota library.\nFox, J. (2013). A mathematical primer for social statistics. Sage.\n\n\n\n\n",
      "last_modified": "2021-09-10T09:41:59-05:00"
    },
    {
      "path": "notes-01.html",
      "title": "Regression Using Matrices",
      "description": "An introduction to using matrix algebra to compute regression estimates (e.g., coefficients, standard errors, fitted values).\n",
      "author": [
        {
          "name": "Andrew Zieffler",
          "url": "http://zief0002.github.io/epsy-8264"
        }
      ],
      "date": "`r Sys.Date()`",
      "contents": "\nRecall the model equation we use in simple linear regression\n\\[\nY_i = \\beta_0 + \\beta_1(X_i) + \\epsilon_i\n\\]\nwhere the response variable (Y) is represented as a linear function of the predictor (X) and a residual (\\(\\epsilon\\)). Equation terms with an i subscript vary across subjects. Terms without an i subscript are the same (fixed) across subjects.\n\nRepresenting the Model Equation Using Matrix Algebra\nUsing the subject-specific subscripts (\\(1, 2, 3, \\ldots, n\\)), we can write out each subject‚Äôs equation:\n\\[\n\\begin{split}\nY_1 &= \\beta_0 + \\beta_1(X_1) + \\epsilon_1 \\\\\nY_2 &= \\beta_0 + \\beta_1(X_2) + \\epsilon_2 \\\\\nY_3 &= \\beta_0 + \\beta_1(X_3) + \\epsilon_3 \\\\\n\\vdots &~ ~~~~~\\vdots ~~~~~~~~~~\\vdots~~~~~~~~~~~~~\\vdots  \\\\\nY_n &= \\beta_0 + \\beta_1(X_n) + \\epsilon_n\n\\end{split}\n\\]\nThese can be arranged into a set of vectors and matrices, namely,\n\\[\n\\begin{split}\n\\begin{bmatrix}Y_1 \\\\ Y_2 \\\\ Y_3 \\\\ \\vdots \\\\ Y_n\\end{bmatrix} &= \\begin{bmatrix}1 & X_1 \\\\ 1 & X_2 \\\\ 1 & X_3 \\\\ \\vdots \\\\ 1 & X_n\\end{bmatrix} \\begin{bmatrix}\\beta_0 \\\\ \\beta_1\\end{bmatrix}+ \\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\vdots \\\\ \\epsilon_n\\end{bmatrix} \\\\[2ex]\n\\mathbf{y} &= \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\epsilon}\n\\end{split}\n\\]\nwhere,\ny is an \\(n \\times 1\\) vector of observations on the outcome variable.\nX is an \\(n \\times k\\) matrix (called the design matrix) consisting of a column of ones and the observations for k independent predictors. In the simple regression example, \\(k=2\\), and the design matrix has two columns‚Äîa column of ones and a column of observations for the predictor X.\n\\(\\boldsymbol\\beta\\) is a \\(k \\times 1\\) vector of unknown population parameters that we want to estimate. In the simpke regression model, b is a \\(2 \\times 1\\) vector consisting of \\(\\beta_0\\) and \\(\\beta_1\\).\n\\(\\boldsymbol\\epsilon\\) is a \\(n \\times 1\\) vector of residuals.\nNote that this model has a systematic (or deterministic) part (\\(\\mathbf{X}\\boldsymbol{\\beta}\\)) and a stochastic (or random) part (\\(\\boldsymbol\\epsilon\\)). In a regression analysis, one goal is often to estimate the values of the parameters in the \\(\\boldsymbol\\beta\\) vector using sample data (i.e., the Y and X values.\n\nEstimating the Regression Coefficients\nIn ordinary least squares (OLS) estimation, the estimated coefficients minimize the sum of the squared sample residuals (i.e., the SSE), which in scalar form is \\(\\mathrm{SSE}=\\sum e_i^2\\). Here we are using the notation \\(e_i\\) to denote the sample residuals rather than \\(\\epsilon_1\\), which denotes the population residuals. The SSE can be expressed in matrix notation as:\n\\[\n\\begin{split}\n\\mathrm{SSE} &= \\mathbf{e}^{\\intercal}\\mathbf{e} \\\\[2ex]\n&= \\begin{bmatrix}\\epsilon_1 & \\epsilon_2 & \\epsilon_3 & \\ldots & \\epsilon_n\\end{bmatrix}\\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\vdots \\\\ \\epsilon_n\\end{bmatrix}\n\\end{split}\n\\]\nSince we can also express the residual vector as \\(\\mathbf{e} = \\mathbf{y}-\\mathbf{Xb}\\), the SSE can also be expressed as:\n\\[\n\\mathrm{SSE} = (\\mathbf{y}-\\mathbf{Xb})^{\\intercal}(\\mathbf{y}-\\mathbf{Xb})\n\\]\nHere again, we are using b to denote that this denotes the coefficient estimates rather than the actual parameters. This can be re-written as:\n\\[\n\\begin{split}\n\\mathrm{SSE} &= \\mathbf{y}^{\\intercal}\\mathbf{y} - \\mathbf{b}^{\\intercal}\\mathbf{X}^{\\intercal}\\mathbf{y} - \\mathbf{y}^{\\intercal}\\mathbf{X}\\mathbf{b} + \\mathbf{b}^{\\intercal}\\mathbf{X}^{\\intercal}\\mathbf{X}\\mathbf{b} \\\\[2ex]\n&= \\mathbf{y}^{\\intercal}\\mathbf{y} - 2\\mathbf{b}^{\\intercal}\\mathbf{X}^{\\intercal}\\mathbf{y} + \\mathbf{b}^{\\intercal}\\mathbf{X}^{\\intercal}\\mathbf{X}\\mathbf{b}\n\\end{split}\n\\]\nTo find the values for the elements in b that minimize the equation, we use calculus to differentiate this expression with respect to b\n\nAlthough calculus, especially calculus on matrices, is beyond the scope of this course, Fox (2009) gives the interested reader some mathematical background on optimization (i.e., minimizing). For now you just need to understand we can optimize a function by computing its derivative, setting the derivative equal to 0, and solve for any remaining unknowns.\n\nThis gives the expression:\n\\[\n\\mathbf{X}^{\\intercal}\\mathbf{Xb} = \\mathbf{X}^{\\intercal}\\mathbf{y}\n\\]\nThis expression is referred to as the Normal Equations. To solve for the elements in b, we pre-multiply both sides of the equation by \\((\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\).\n\\[\n\\begin{split}\n(\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}(\\mathbf{X}^{\\prime}\\mathbf{X})\\mathbf{b} &= (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}(\\mathbf{X}^{\\intercal}\\mathbf{y}) \\\\[2ex]\n\\mathbf{I}\\mathbf{b} &= (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}\\mathbf{y} \\\\[2ex]\n\\mathbf{b} &= (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}\\mathbf{y}\n\\end{split}\n\\]\nThe vector of regression coefficients is given as:\n\\[\n\\mathbf{b} = (\\mathbf{X}^{\\intercal}\\mathbf{X})^{-1}\\mathbf{X}^{\\intercal}\\mathbf{y}\n\\]\nThis implies that the vector of regression coefficients can be obtained directly through manipulation of the design matrix and the vector of outcomes. In other words, the OLS coefficients is a direct function of the data. Note that as of yet, we have made no assumptions about the residuals. The coefficients can be estimated making no assumptions about the distributions of the residuals.\n\nProperties of the OLS Estimators\nOne property of the OLS estimators is that they minimize the sum of squared residuals. There are also several other properties that the OLS estimators have. Remember these estimators are based on the normal equations:\n\\[\n\\mathbf{X}^{\\intercal}\\mathbf{Xb} = \\mathbf{X}^{\\intercal}\\mathbf{y}\n\\] If we substitute \\(\\mathbf{Xb}+\\mathbf{e}\\) in for y in this expression, we get:\n\\[\n\\begin{split}\n\\mathbf{X}^{\\intercal}\\mathbf{Xb} &= \\mathbf{X}^{\\intercal}(\\mathbf{Xb}+\\mathbf{e}) \\\\[2ex]\n&= \\mathbf{X}^{\\intercal}\\mathbf{Xb} + \\mathbf{X}^{\\intercal}\\mathbf{e}\n\\end{split}\n\\]\nTo make this equality work, implies that:\n\\[\n\\mathbf{X}^{\\intercal}\\mathbf{e} = \\mathbf{0}\n\\] Let‚Äôs examine \\(\\mathbf{X}^{\\intercal}\\mathbf{e}\\):\n\\[\n\\begin{split}\n\\mathbf{X}^{\\intercal}\\mathbf{e} &= \\mathbf{0} \\\\[2ex]\n\\begin{bmatrix}1 & 1 & 1 & \\ldots & 1\\\\X_1 & X_2 & X_3 & \\ldots & X_n\\end{bmatrix}\\begin{bmatrix}\\epsilon_1 \\\\ \\epsilon_2 \\\\ \\epsilon_3 \\\\ \\vdots \\\\ \\epsilon_n\\end{bmatrix} &= \\mathbf{0} \\\\[2ex]\n\\begin{bmatrix}\\epsilon_1 + \\epsilon_2 + \\epsilon_3 + \\ldots + \\epsilon_n \\\\ X_1\\epsilon_1 + X_2\\epsilon_2 + X_3\\epsilon_3 + \\ldots + X_n\\epsilon_n\\end{bmatrix} &= \\begin{bmatrix}0 \\\\ 0 \\end{bmatrix}\n\\end{split}\n\\]\nThis implies that for every column in the design matrix, \\(\\mathbf{X}_k\\), that \\(\\mathbf{X}_k^{\\intercal}\\mathbf{e}=0\\). In other words, the dot product between \\(\\mathbf{X}_k\\) and \\(\\mathbf{e}\\) is zero indicating that the two vectors are independent.\nPROPERTY: The observed values of the predictor(s) are uncorrelated with the sample residuals.\nNote that this does not mean that the predictor(s) are uncorrelated with the residuals in the population; that is an assumption we will have to make later on.\n\nProperties of the OLS Regressors\nIf the regression model includes an intercept (the first column of the design matrix is a ones vector) then the following properties also hold.\nPROPERTY: The sum of the residuals is 0.\nIf the first column of the design matrix is a ones vector, then the first element of the \\(\\mathbf{X}^{\\intercal}\\mathbf{e}\\) matrix is \\(\\epsilon_1 + \\epsilon_2 + \\epsilon_3 + \\ldots + \\epsilon_n = \\sum \\epsilon_i\\), which is equal to zero since \\(\\mathbf{X}^{\\intercal}\\mathbf{e}=\\mathbf{0}\\).\nPROPERTY: The sample mean of the residuals is zero.\n\n\n\nFox, J. (2009). A mathematical primer for social statistics. Sage.\n\n\n\n\n",
      "last_modified": "2021-09-10T09:42:02-05:00"
    },
    {
      "path": "schedule.html",
      "title": "Calendar",
      "description": "Tentative calendar of topics.",
      "author": [],
      "contents": "\nThe calendar below lists the tentative course topics. The dates listed are subject to change at the instructor‚Äôs discretion.\n\n\n\n\nDate\n\n\nReading\n\n\nTopic\n\n\nNotes\n\n\n¬†\n\n\nSept.¬†07\n\n\n\n\n\nWelcome to EPsy 8264\n\n\n¬†\n\n\nUnit 01: Mathematical and Computational Foundations\n\n\n¬†\n\n\nSept.¬†09\n\n\n\n\n\nIntroduction to Matrix Algebra\n\n\n\n\n\n¬†\n\n\nSept.¬†14\n\n\n¬†\n\n\nSept.¬†16\n\n\n\n\n\nOLS Regression using Matrices and its Properties\n\n\n\n\n\n¬†\n\n\nSept.¬†21\n\n\n¬†\n\n\nSept.¬†23\n\n\n\n\nSimulating from the Regression Model\n\n\n\n\n¬†\n\n\nSept.¬†28\n\n\n¬†\n\n\nSept.¬†30\n\n\n\n\n",
      "last_modified": "2021-09-10T09:42:03-05:00"
    }
  ],
  "collections": ["posts/posts.json"]
}
