---
title: "Biased Estimation and Shrinkage"
description: |
  A brief introduction to biased estimation and shrinkage methods for dealing with collinearity.
author:
  - name: Andrew Zieffler 
    url: http://www.datadreaming.org/
date: "`r Sys.Date()`"
output: radix::radix_article
bibliography: epsy8264.bib
csl: apa-single-spaced.csl
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(knitr)
library(kableExtra)
```

The data in *equal-educational-opportunity.csv* consist of data taken from a random sample of 70 schools in 1965 [@Coleman:1966; @Chatterjee:2012; @Mosteller:1972]. The variables, which have all been mean-centered and standardized, include:

- `achievement`: Measurement indicating the student achievement level
- `faculty`: Measurement indicating the faculty's credentials
- `peer`: Measurement indicating the influence of peer groups in the school
- `school`: Measurement indicating the school facilities (e.g., building, teaching materials)

We will use these data to mimic one of the original regression analyses performed; examining whether the level of school facilities was an important predictor of student achievement after accounting for the variation in faculty credientials and peer influence.

```{r}
# Load libraries
library(MASS)
library(broom)
library(car)
library(corrr)
library(dplyr)
library(ggplot2)
library(readr)
library(sm)

# Read in data
eeo = read_csv("~/Dropbox/epsy-8264/data/equal-education-opportunity.csv")
head(eeo)
```

To examine the RQ, the following model was posited:

$$
\mathrm{Achievement}_i = \beta_0 + \beta_1(\mathrm{Faculty}_i) + \beta_2(\mathrm{Peer}_i) + \beta_3(\mathrm{School}_i) + \epsilon_i
$$

Unfortunately, the model sufferred from strong collinearity resulting in poor estimates of the coefficients and very large SEs which didn't allow us to answer the RQ.

## Biased Estimation

Recall that the Gauss-Markov Theorem posited many attractive features of the OLS regression model, two of which were that it produced unbiased coefficients and that the sampling variances of the coefficients were minimized. In our example, the fitted model showed strong evidence of collinearity; which makes the SEs super large even if they are the minimum of all possible linear, unbiased estimates.  

One method of dealing with collinearity is to use a biased estimation method. These methods forfeit unbiasedness to decrease the size of the sampling variances; it is bias--variance tradeoff. By accepting a less than perfect estimate (higher bias), we gain a model with less overall error (less variation).

<aside>
The bias--variance tradeoff is a commonplace, especially in prediction models. You can explore it in more detail at [http://scott.fortmann-roe.com/docs/BiasVariance.html](http://scott.fortmann-roe.com/docs/BiasVariance.html).
</aside>


## Ridge Regression

The most common biased estimation method is ridge regression. Instead of finding the coefficients that minimize the sum of squared errors, ridge regression finds the coefficients that minimize a penalized sum of squares, namely:

$$
\mathrm{SSE}_{\mathrm{Penalized}} = \sum_{i=1}^n \bigg(y_i - \hat y_i\bigg)^2 + \lambda \sum_{j=1}^p \beta_j^2
$$

Another way to think about this is

$$
\mathrm{SSE}_{\mathrm{Penalized}} = \mathrm{SSE} + \mathrm{Penalty}
$$

Minimizing over this penalized sum of squared error has the effect of "shrinking" the mean square error and coefficient estimates toward zero. As such, ridge regression is part of a larger class of methods known as *shrinkage methods*. 

The $\lambda$ value in the penalty term controls the amount of shrinkage. A $\lambda = 0$ will make the penalty term 0, and the $\mathrm{SSE}_{\mathrm{Penalized}} = \mathrm{SSE}$; this will result in coefficients that are the OLS estimates. The bigger the $\lambda$ value, the more the shrinkage toward zero. At the extreme end, $\lambda = \infty$ will shrink every coefficients to zero.

### Ridge Regression in Practice

Because the penalty is based on the size of the coefficients, large coefficients will impact this penalty more than small coefficients. For example, if we were using high school GPA and household income to predict undergraduate GPA, the coefficient associated with houshold income will be larger than the coefficient associated with high school GPA simply because of the different metrics. Because of this we tend to center and scale (standardize) all variables prior to fitting a ridge regression. 




### Matrix Formulation of Ridge Regression

Recall that the OLS estimates are given by:

$$
\boldsymbol{\beta} = (\mathbf{X}^{\intercal}\mathbf{X})^{-1}\mathbf{X}^{\intercal}\mathbf{Y}
$$

For standardized variables, the model coefficient vector for the ridge regression can be calculated as:

$$
\widetilde{\boldsymbol{\beta}} = (\mathbf{X}^{\intercal}\mathbf{X} + \lambda \mathbf{I})^{-1}\mathbf{X}^{\intercal}\mathbf{Y}
$$

Prior to finding the inverse of the $\mathbf{X}^{\intercal}\mathbf{X}$ matrix, we add a value of $\lambda$ to all of the diagonal elements in $\mathbf{X}^{\intercal}\mathbf{X}$. By inflating these diagonal elements, we can condition the matrix, hopefully leading to more stable coefficient estimates. 



### An Example

Let's fit a ridge regression model to our EEO data. FOr now, let's choose $\lambda = 0.1$. Prior to any matrix calculations, we first standardize all potential variables in the model.

```{r}
# Standardize the variables
eeo = eeo %>% 
  mutate(
    z_achievement = as.numeric(scale(achievement)),
    z_faculty     = as.numeric(scale(faculty)),
    z_peer        = as.numeric(scale(peer)),
    z_school      = as.numeric(scale(school))
  )
```


The design matrix, **X**, is a $70\times 3$ matrix (since the variables are standardized, the intercept in any regression will be set to zero and is not estimated). Here we use the `data.matrix()` function to create a design matrix based on the standardized predictors.

```{r}
x = eeo %>% 
  dplyr::select(z_faculty, z_peer, z_school) %>% 
  data.matrix()
```

The **Y** matrix is a $70\times 1$ column matrix of the standardized outcome values, namely:

```{r}
y = eeo$z_achievement
```


Since $\mathbf{X}^{\intercal}\mathbf{X}$ is a $3\times 3$ matrix, $\lambda \mathbf{I}$ must also be a $3\times 3$ matrix (to be able to sum them). Here we use $\lambda=0.1$ just to illustrate the concept of ridge regression:

```{r}
lambda_I = 0.1 * diag(3)
lambda_I
```


Finally we can perform the matrix algebra to obtain the ridge regression coefficients:

```{r}
# Compute ridge regression coefficients
solve(t(x) %*% x + lambda_I) %*% t(x) %*% y
```

The fitted ridge regression model is then,

$$
\hat{\mathrm{Achievement}}^{\star}_i = 0.435(\mathrm{Faculty}^{\star}_i) + 0.855(\mathrm{Peer}^{\star}_i) - 0.849(\mathrm{School}^{\star}_i)
$$

where the star-superscript denotes a standardized (*z*-scored) variable.

#### Using a Built-In Function

We can also use the `glmnet()` function from the **glmnet** package to fit a ridge regression. To fit a ridge regression we provide the following arguments to `glmnet()`:

- `x=`: A matrix of the predictors
- `y=`: A vector of the outcome
- `alpha=0`: This fits a ridge regression
- `lambda=`: This sets the $\lambda$ value; however, because `glmnet()` uses the MSE rather than the SSE to optimize over, to keep results consistent with the those from the matrix algebra we provide this argument a modified value, namely $\frac{\lambda}{n}$
- `intercept=FALSE`: We will include this argument to drop the intercept term; appropriate for standardized regression.



```{r}
# Fit ridge regression
ridge.1 = glmnet(
  x = x, 
  y = y, 
  alpha = 0, 
  lambda = 0.1/70, 
  intercept = FALSE
  )

# Show coefficients
tidy(ridge.1)
```

#### Comparison to the OLS Coefficients

Fitting a standardized OLS model to the data, we find:

```{r}
# OLS model
lm.1 = lm(z_achievement ~ z_faculty + z_peer + z_school - 1, data = eeo)
coef(lm.1)
```

Comparing these coefficients to the coefficients from the ridge regression:

```{r echo=FALSE}
tab_01 = data.frame(
  B = c("Faculty", "Peer", "School"),
  OLS = coef(lm.1),
  Ridge = coef(ridge.1)
)

kable(tab_01, digits = 3, row.names = FALSE)
```

Based on this comparison, the ridge regression has "shrunk" the estimate of each coefficient toward zero. Remember, the larger the $lambda$ value, the more the coefficient estimates will shrink toward 0. The table below shows the coefficient estimates for four different values of $\lambda$.

```{r echo=FALSE}
tab_02 = data.frame(
  B = c("Faculty", "Peer", "School"),
  x1 = coef(lm.1),
  x4 = coef(lm.ridge(z_achievement ~ z_faculty + z_peer + z_school - 1, data = eeo, lambda = 0.01)),
  x2 = coef(ridge.1),
  x3 = coef(lm.ridge(z_achievement ~ z_faculty + z_peer + z_school - 1, data = eeo, lambda = 0.5))
)

kable(tab_02, digits = 3, row.names = FALSE, col.names = c("B", "$\\lambda=0$", "$\\lambda=0.01$", "$\\lambda=0.1$", "$\\lambda=0.5$"))
```


## Choosing $\lambda$

In practice you need to specify the $\lambda$ value to use in the ridge regression. Ideally you want to choose a value for $\lambda$ that:

- Introduces the least amount of bias possible, while also 
- Obtaining better sampling variances. 

This is an impossible task without knowing the true values of the coefficients (i.e., the $\beta$ values). There are, however, some empirical methods to help us in this endeavor.

### Ridge Trace

The plot of the *ridge trace* displays the ridge regression coefficients as a function of $\lambda$. Examining this plot, the analyst can use it to pick a $\lambda$ value. To do this, pick the smallest value for $\lambda$ that produces stable regression coefficients.

```{r}
# Fit ridge model across several lambda values
ridge_models = glmnet(
  x = x, 
  y = y, 
  alpha = 0, 
  lambda = seq(from = 0, to = 10, by = 0.001), 
  intercept = FALSE
  )

# Ridge trace
plot(ridge_models, xvar = "lambda")
```

Based on this plot the trace for each of the three coefficients starts to flatten out around a $\ln(\lambda_{\mathrm{Modified}})\approx -3$. 

```{r}
# Find lambda
exp(-3) * 70
```

This suggests that a $\lambda$ value around 3.5 would be a good choice.


### Cross-Validation

Unfortunately, the trace plot is built from using the data to both fit the model and evaluate the model. This tends to overfit to the data we used. A better method of selecting $\lambda$ is through cross-validation. Cross-validation takes the data and randomly splits it into a *training set* and a *test set*. Models are built using the training set and evaluated using the test set. With this method, you can see how the model will perform on a "new" set of data.

<aside>
Read more about cross-validation at https://machinelearningmastery.com/k-fold-cross-validation/
</aside>

The cross-validation method used in the **glmnet** package to determine the $\lambda$ value proceeds using the following algorithm:

- Split the dataset randomly into 10 groups
- For each unique group:
    - Hold the group out as test data set
    - Take the remaining 9 groups and use them as a training data set
    - Fit a ridge model on the training set with a specific value of $\lambda$
    - Evaluate the model  using the test set by computing the error (deviance)
    - Repeat for different values of $\lambda$
- Summarize the performance for the various $\lambda$ values by averaging across the 10 different tests sets
- Choose the $\lambda$ value that gives the lowest average amount of error

To use cross-validation for $\lambda$ selection, we perform the cross-validation using the `cv.glmnet()` function. Then, we can plot the results and extract the "best" $\lambda$ value.

```{r}
# Carry out the cross-validation
set.seed(42) #only needed for reproducability
ridge_cv = cv.glmnet(x = x, y = y, alpha = 0, intercept = FALSE)

# Plot of the results
plot(ridge_cv)

# Extract the lambda value with the lowest mean error
ridge_cv$lambda.min
```


Here the $\lambda_{\mathrm{Modified}}$ value with the lowest mean error from the cross-validation results is `r ridge_cv$lambda.min`. This value is also associated with the $\ln \lambda$ value that corresponds to the lowest MSE from the plot. This modified $\lambda$ value could be used directly in the `lambda=` argument of the `glmnet()` function, or could be transformed (by multiplying it by *n*) for use in the matrix algebra equation.

```{r}
# Fit ridge regression
ridge_best = glmnet(x = x, y = y, alpha = 0, lambda = ridge_cv$lambda.min, intercept = FALSE)

# Show coefficients
tidy(ridge_best)
```


## Sampling Variances, Standard Errors, and Confidence Intervals

In theory it is possible to obtain the sampling variances for the ridge regression coefficients using matrix algebra:

$$
\mathrm{Var}(\widetilde{\boldsymbol{\beta}}) = \sigma^2\mathbf{W}\mathbf{X}^\intercal \mathbf{X} \mathbf{W}
$$

where $\mathbf{W} = (\mathbf{X}^\intercal \mathbf{X} + \lambda\mathbf{I})^{-1}$. Note that the $\lambda$ value here is the unmodifed version.

```{r}
# Unmodified lambda
L = 0.3080785 * 70
```


To obtain the estimate of $\sigma^2$, the MSE, we access the results of the cross-validated model, specifically the `cvm` for the "best" $\lambda$.

```{r}
mse = ridge_cv$cvm[ridge_cv$lambda == ridge_cv$lambda.min]
mse
```


```{r}
# Compute W
W = solve( t(x) %*% x + L * diag(3) )

# Compute variance-covariance matrix of the ridge coefficients
mse * W %*% t(x) %*% x %*% W
```

The estimated sampling variances, could in theory, be used to compute the SEs and confidence intervals for the ridge coefficients.

There is a reason, however, that software does not typically report SEs for the coefficents. This is summarized best by Jelle Goeman who wrote in the documentation for the **penalized** package [@Goeman:2018],

> It is a very natural question to ask for standard errors of regression coefficients or other estimated quantities. In principle such standard errors can easily be calculated, e.g. using the bootstrap.
>
Still, this package deliberately does not provide them. The reason for this is that standard errors are not very meaningful for strongly biased estimates such as arise from penalized estimation methods. Penalized estimation is a procedure that reduces the variance of estimators by introducing substantial bias. The bias of each estimator is therefore a major component of its mean squared error, whereas its variance may contribute only a small part.
>
Unfortunately, in most applications of penalized regression it is impossible to obtain a sufficiently precise estimate of the bias. Any bootstrap-based calculations can only give an assessment of the variance of the estimates. Reliable estimates of the bias are only available if reliable unbiased estimates are available, which is typically not the case in situations in which penalized estimates are used.
>
Reporting a standard error of a penalized estimate therefore tells only part of the story. It can give a mistaken impression of great precision, completely ignoring the inaccuracy caused by the bias. It is certainly a mistake to make confidence statements that are only based on an assessment of the variance of the estimates, such as bootstrap-based confidence intervals do.



## Shrinkage Methods as a Solution for Small N Large P Problems

Shrinkage methods are not only useful for analyzing collinear data, but are also useful for addressing problems in which there is a small sample size and many predictors. Typically we would be limited to fitting models in which $p < n$. However, shrinkage methods can help address the ill-conditioned matrix obtained from fitting a model in which $p > n$.  

A different shrinkage model is the *lasso*. This model finds the coefficients that minimize a diffeent penalized sum of squares, namely:

$$
\mathrm{SSE}_{\mathrm{Penalized}} = \sum_{i=1}^n \bigg(y_i - \hat y_i\bigg)^2 + \lambda \sum_{j=1}^p \mid \beta_j \mid
$$

The difference between the ridge regression model and the lasso is in the penalty term. The lasso model leads has similar behavior to ridge regression; as $\lambda$ increases, the sampling variance decreases and the bias increases.

To fit the lasso, we again use the `glmnet()` function, but use the argument `alpha=1` rather than `alpha=0`. As with ridge regression, we need to find an appropriate $\lambda$ value.

```{r}
# Carry out the cross-validation
set.seed(42) #only needed for reproducibility
lasso_cv = cv.glmnet(x = x, y = y, alpha = 1, intercept = FALSE)
lasso_cv$lambda.min

# Fit lasso model with "best" lambda
lasso_1 = glmnet(x = x, y = y, alpha = 1, intercept = FALSE, lambda = 0.02026951)
tidy(lasso_1)
```

The lasso model has shrunk the faculty and school coefficients to zero! Examining the MSE,

```{r}
lasso_cv$cvm[lasso_cv$lambda == lasso_cv$lambda.min]
```

The MSE from the lasso (0.823) is smaller than the MSE from the ridge regression (0.831). This implies that the lasso model will produce smaller average errors when predicting new data sets than the ridge regression. 



In general, one might expect the lasso to perform better in a setting where a relatively small number of predictors have substantial coefficients, and the remaining predictors have coefficients that are very small or that equal zero. Ridge regression will perform better when the response is a function of many predictors, all with coefficients of roughly equal size. However, the number of predictors that is related to the response is never known a priori for real data sets. A technique such as cross-validation can be used in order to determine which approach is better on a particular data set.

<!-- ## Lasso -->

<!-- The ridge regression model uses a penalty term that is based on $\lambda \sum \beta_j^2$. We can express the distance of the $\boldsymbol{\widetilde\beta}$ from 0 by computing: -->

<!-- $$ -->
<!-- \mathrm{Distance} = \sqrt{\widetilde\beta_1^2 + \widetilde\beta_2^2 +\ldots+\widetilde\beta_p^2} -->
<!-- $$ -->

<!-- This is called the *L2 norm* and is denoted ${\lVert \boldsymbol{\widetilde\beta} \rVert}_{2}$.  -->
<!-- In matrix algebra, we canthe sum of squared vWe can also  -->

<!-- Euclidean distance (L2 norm): $$ -->
<!-- Manhattan distance (L1 norm): $\mid\beta_1 + \beta_2 +\ldots+\beta_p\mid$ -->


